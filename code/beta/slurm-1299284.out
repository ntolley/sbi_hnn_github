distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:36211'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:39593'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:36107'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:46963'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:46183'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:38253'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:32773'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:37491'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:40991'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:44975'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:42259'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:38591'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:36291'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:46709'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:37077'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:37621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:42025'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:35039'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:41921'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:34263'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:39153'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:44595'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:43847'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:43133'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:34527'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:38713'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:36425'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:33827'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:36573'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:39867'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:38179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:43553'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:43653'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:46757'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:41501'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:32941'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:35083'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:34159'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:36867'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:38115'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:41781'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:41233'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:46777'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:39029'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:34675'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:43583'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:46175'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:38577'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:37299'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.126:41311'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:41041
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:39721
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:41041
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:39721
distributed.worker - INFO -          dashboard at:      198.202.101.126:41327
distributed.worker - INFO -          dashboard at:      198.202.101.126:33327
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pv6_tzrd
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4edc9vuf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:42077
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:42077
distributed.worker - INFO -          dashboard at:      198.202.101.126:37387
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-spzhhgl0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:37205
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:37205
distributed.worker - INFO -          dashboard at:      198.202.101.126:46077
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3syc6rqh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:44273
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:44273
distributed.worker - INFO -          dashboard at:      198.202.101.126:47033
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-aad6vwor
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:46053
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:46053
distributed.worker - INFO -          dashboard at:      198.202.101.126:41815
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-210ou51p
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:42685
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:42685
distributed.worker - INFO -          dashboard at:      198.202.101.126:36469
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5rms4ewn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:43991
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:43991
distributed.worker - INFO -          dashboard at:      198.202.101.126:45135
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vnspr6jx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:46189
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:46189
distributed.worker - INFO -          dashboard at:      198.202.101.126:43021
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-km0gfgxu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:38963
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:38963
distributed.worker - INFO -          dashboard at:      198.202.101.126:43295
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zj7zfabz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:41261
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:41261
distributed.worker - INFO -          dashboard at:      198.202.101.126:44067
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uym_knb1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:46881
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:46881
distributed.worker - INFO -          dashboard at:      198.202.101.126:41785
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-__q79z0v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:36513
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:36513
distributed.worker - INFO -          dashboard at:      198.202.101.126:36629
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-x3haycgf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:41913
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:41913
distributed.worker - INFO -          dashboard at:      198.202.101.126:38837
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-e2ivbwm7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:34063
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:34063
distributed.worker - INFO -          dashboard at:      198.202.101.126:46651
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tymb8m54
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:44947
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:44947
distributed.worker - INFO -          dashboard at:      198.202.101.126:38627
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0c99z91q
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:37985
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:37985
distributed.worker - INFO -          dashboard at:      198.202.101.126:35317
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1cusb3mi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:36015
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:36015
distributed.worker - INFO -          dashboard at:      198.202.101.126:43163
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5isbnlx5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 3.01 GB -- Worker memory limit: 4.00 GB
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:34191
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:34191
distributed.worker - INFO -          dashboard at:      198.202.101.126:44969
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_lpl023o
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.32 GB -- Worker memory limit: 4.00 GB
distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 3.32 GB -- Worker memory limit: 4.00 GB
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:38361
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:38361
distributed.worker - INFO -          dashboard at:      198.202.101.126:33933
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ialmgop2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 3.70 GB -- Worker memory limit: 4.00 GB
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:46895
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:46895
distributed.worker - INFO -          dashboard at:      198.202.101.126:33361
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0q27vhok
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:39811
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:39811
distributed.worker - INFO -          dashboard at:      198.202.101.126:45425
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bpt9blqc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.nanny - INFO - Worker process 80798 was killed by signal 15
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:46819
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:46819
distributed.worker - INFO -          dashboard at:      198.202.101.126:44131
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yk6aokj0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:33315
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:33315
distributed.worker - INFO -          dashboard at:      198.202.101.126:46883
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-740whcro
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:40857
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:40857
distributed.worker - INFO -          dashboard at:      198.202.101.126:42269
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-85q3antu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:33405
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:33405
distributed.worker - INFO -          dashboard at:      198.202.101.126:43829
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-b195lms2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.nanny - WARNING - Restarting worker
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:42853
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:42853
distributed.worker - INFO -          dashboard at:      198.202.101.126:35367
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-j1qjppig
distributed.worker - INFO - -------------------------------------------------
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:37577
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:37577
distributed.worker - INFO -          dashboard at:      198.202.101.126:42453
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ahxfnegc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:47071
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:47071
distributed.worker - INFO -          dashboard at:      198.202.101.126:42693
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7ak3_mlt
distributed.worker - INFO - -------------------------------------------------
Warning: no DISPLAY environment variable.
--No graphics will be displayed.
distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c4862790>, tensor([1.0830e+05, 2.5357e+02, 4.7901e+01, 1.4634e+01, 1.6390e-04, 6.9012e-05,
        2.4128e+02, 2.9991e+01, 3.7840e+00, 4.6258e-04, 1.8758e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 332))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

Warning: no DISPLAY environment variable.
--No graphics will be displayed.
distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x155458c9ac10>, tensor([1.6128e+05, 2.5067e+02, 2.7663e+01, 1.7261e+01, 2.3104e-04, 4.1266e-04,
        2.4327e+02, 2.3646e+01, 4.8817e+00, 3.7816e-04, 4.1672e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 315))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c4245f70>, tensor([1.8743e+05, 2.5094e+02, 3.5740e+01, 1.1558e+01, 3.6191e-04, 2.9712e-04,
        2.5010e+02, 2.4134e+01, 1.7387e+01, 3.9621e-04, 1.2296e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 257))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

Warning: no DISPLAY environment variable.
--No graphics will be displayed.
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:41353
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:41353
distributed.worker - INFO -          dashboard at:      198.202.101.126:43269
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5kd0sf34
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c46a7040>, tensor([1.6105e+05, 2.4545e+02, 4.4865e+01, 1.7757e+01, 3.7298e-04, 4.2636e-04,
        2.4267e+02, 9.2856e+00, 1.1539e+01, 3.6668e-04, 1.1367e-05]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 185))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x155458c57820>, tensor([8.4821e+04, 2.3514e+02, 2.9415e+01, 8.8146e+00, 4.6500e-04, 1.1450e-04,
        2.5427e+02, 8.0449e+00, 1.6130e+01, 1.5339e-04, 2.2111e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 4))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x155458c4b550>, tensor([1.7817e+05, 2.5345e+02, 1.8289e+01, 9.6577e+00, 4.8899e-04, 2.8868e-04,
        2.5167e+02, 5.7167e+00, 7.3830e+00, 3.8123e-04, 4.3612e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 341))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x155458c35dc0>, tensor([7.2721e+04, 2.3638e+02, 4.1116e+01, 1.2816e+01, 4.7144e-04, 4.0370e-04,
        2.3773e+02, 2.1496e+01, 4.3066e+00, 8.6662e-06, 3.9888e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 404))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x155458b4a5e0>, tensor([1.4670e+05, 2.2687e+02, 4.3762e+01, 1.9359e+01, 1.6808e-04, 3.0557e-04,
        2.4769e+02, 2.5750e+01, 6.8487e+00, 1.1695e-04, 8.1290e-05]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 88))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.core - INFO - Starting established connection
distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c4827af0>, tensor([1.0135e+05, 2.5497e+02, 3.7177e+01, 1.8687e+01, 4.7010e-04, 3.1135e-04,
        2.4377e+02, 7.6743e+00, 1.4970e+01, 4.9208e-04, 1.7319e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 334))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c44c6b80>, tensor([1.1586e+05, 2.3566e+02, 2.0471e+01, 1.3218e+01, 1.7763e-04, 1.2617e-04,
        2.4393e+02, 1.1335e+01, 2.1458e+00, 5.8200e-06, 2.3016e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 370))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x155458d02790>, tensor([1.1278e+05, 2.4452e+02, 1.7343e+01, 1.1065e+01, 2.6874e-04, 2.0500e-04,
        2.4511e+02, 2.7212e+01, 1.6758e+01, 4.6196e-04, 2.8518e-05]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 149))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c4667430>, tensor([1.5031e+05, 2.2658e+02, 3.2844e+01, 4.1557e+00, 4.9784e-04, 4.3699e-04,
        2.4658e+02, 1.6173e+01, 9.6458e+00, 2.8289e-04, 1.7614e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 347))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c45da1f0>, tensor([1.0242e+05, 2.3775e+02, 1.5556e+01, 1.1674e+01, 4.4627e-04, 4.0356e-04,
        2.4567e+02, 1.7402e+01, 1.9132e+01, 1.1789e-05, 2.4836e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 353))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x155458da0dc0>, tensor([7.2416e+04, 2.3200e+02, 1.6114e+01, 4.1430e+00, 1.0794e-04, 1.6167e-04,
        2.4678e+02, 7.1291e+00, 5.6975e+00, 4.3696e-04, 3.0253e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 203))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c47fc280>, tensor([9.0713e+04, 2.5335e+02, 2.8712e+01, 5.3810e+00, 2.7633e-04, 1.0903e-04,
        2.4623e+02, 9.1292e+00, 1.9178e+01, 4.1989e-04, 7.2504e-05]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 260))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c428c820>, tensor([1.5917e+05, 2.4068e+02, 3.9622e+01, 2.0360e+00, 4.3472e-04, 2.3355e-04,
        2.5056e+02, 2.7468e+01, 1.5448e+01, 4.7483e-04, 3.1444e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 407))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c48a2940>, tensor([7.7948e+04, 2.4531e+02, 4.4391e+01, 8.7211e+00, 4.6423e-04, 1.3454e-04,
        2.3835e+02, 5.4036e+00, 1.1969e+01, 8.6236e-05, 4.2168e-06]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 119))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c427cdc0>, tensor([1.4615e+05, 2.5301e+02, 3.5837e+01, 4.8697e+00, 2.8398e-04, 1.6262e-05,
        2.4793e+02, 1.6355e+01, 4.9615e+00, 4.9346e-04, 2.1313e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 7))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c48abe50>, tensor([7.1021e+04, 2.4350e+02, 1.2838e+01, 1.6338e+01, 3.8103e-04, 1.4084e-04,
        2.3933e+02, 2.2755e+01, 1.4669e+01, 2.9104e-04, 4.9175e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 83))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x155458b61af0>, tensor([6.2882e+04, 2.2803e+02, 2.2160e+01, 1.5866e+01, 3.5639e-04, 5.6828e-05,
        2.4218e+02, 1.0874e+01, 4.7563e+00, 1.1040e-04, 2.9163e-06]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 173))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x155458d1a160>, tensor([6.5487e+04, 2.2969e+02, 2.9710e+01, 1.5861e+01, 3.6254e-04, 3.7686e-04,
        2.5177e+02, 2.9849e+01, 1.1802e+01, 3.3238e-04, 2.2182e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 320))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c4198670>, tensor([1.0879e+05, 2.4622e+02, 4.9040e+01, 2.3760e+00, 4.6948e-04, 2.5748e-04,
        2.4983e+02, 1.7049e+01, 1.6052e+01, 4.7842e-04, 9.6465e-05]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 219))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c462b670>, tensor([1.3272e+05, 2.2655e+02, 1.0532e+01, 7.9036e+00, 2.7628e-04, 1.8083e-04,
        2.4672e+02, 2.8353e+01, 3.3075e+00, 1.3855e-04, 3.3216e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 255))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x155458bfa820>, tensor([1.5099e+05, 2.5276e+02, 4.9566e+01, 1.2627e+01, 9.5772e-05, 1.3933e-04,
        2.4962e+02, 1.2200e+01, 1.0088e+01, 3.5190e-04, 9.5619e-05]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 395))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c455b550>, tensor([1.0003e+05, 2.4104e+02, 4.9460e+01, 2.7639e+00, 5.7777e-05, 4.1577e-04,
        2.5482e+02, 1.0667e+01, 1.6683e+01, 2.9260e-04, 1.7348e-06]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 371))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x155458be4ca0>, tensor([1.8167e+05, 2.4656e+02, 2.2106e+01, 8.0797e+00, 1.5806e-04, 4.0431e-04,
        2.3845e+02, 2.3130e+01, 4.8492e+00, 2.3946e-04, 1.9413e-05]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 350))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c44c6940>, tensor([1.5333e+05, 2.4960e+02, 4.1985e+01, 1.1654e+01, 4.8285e-04, 2.0123e-04,
        2.4536e+02, 7.2932e+00, 1.8613e+01, 4.0360e-04, 6.5646e-05]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 459))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c499a9d0>, tensor([1.0897e+05, 2.3129e+02, 2.9143e+01, 3.9865e+00, 3.0240e-04, 2.0797e-04,
        2.5071e+02, 2.1189e+01, 1.9278e+01, 2.0879e-05, 2.6916e-05]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 420))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c428c5e0>, tensor([7.8358e+04, 2.5406e+02, 2.5993e+01, 3.5052e+00, 4.9346e-04, 4.8631e-04,
        2.3652e+02, 2.2415e+01, 1.7110e+00, 1.1867e-04, 9.4575e-05]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 207))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c4a449d0>, tensor([6.1362e+04, 2.4627e+02, 2.5463e+01, 1.4189e+01, 5.5241e-05, 3.6050e-04,
        2.4909e+02, 2.8881e+01, 8.0639e+00, 1.6556e-04, 1.9803e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 279))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c49daca0>, tensor([1.8502e+05, 2.4204e+02, 3.6162e+01, 8.5377e+00, 1.5810e-04, 1.4254e-04,
        2.5037e+02, 1.4284e+01, 7.5426e+00, 4.2801e-04, 2.7119e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 186))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c414f430>, tensor([1.7319e+05, 2.5252e+02, 2.4890e+01, 7.6127e+00, 2.1970e-04, 2.9433e-04,
        2.3656e+02, 5.7155e+00, 1.1836e+01, 2.8438e-04, 2.5906e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 440))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c45dc940>, tensor([1.7172e+05, 2.4714e+02, 2.2754e+01, 4.3098e+00, 7.7430e-05, 4.3865e-04,
        2.4598e+02, 1.8353e+01, 1.1821e+00, 3.0931e-06, 3.6974e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 13))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

distributed.worker - WARNING -  Compute Failed
Function:  execute_task
args:      ((<function run_simulator at 0x1554c414f670>, tensor([1.4826e+05, 2.3081e+02, 3.7753e+01, 1.2008e+01, 4.3980e-04, 1.5842e-04,
        2.5286e+02, 1.8157e+01, 1.6161e+01, 2.0221e-04, 1.0478e-04]), '../../data/beta/params/beta_param.param', (<class 'dict'>, [['dipole_scalefctr', (<class 'tuple'>, [60000, 200000])], ['t_evprox_1', (<class 'tuple'>, [225, 255])], ['sigma_t_evprox_1', (<class 'tuple'>, [10, 50])], ['numspikes_evprox_1', (<class 'tuple'>, [1, 20])], ['gbar_evprox_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evprox_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['t_evdist_1', (<class 'tuple'>, [235, 255])], ['sigma_t_evdist_1', (<class 'tuple'>, [5, 30])], ['numspikes_evdist_1', (<class 'tuple'>, [1, 20])], ['gbar_evdist_1_L2Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])], ['gbar_evdist_1_L5Pyr_ampa', (<class 'tuple'>, [1e-06, 0.0005])]]), 384))
kwargs:    {}
Exception: TypeError("'numpy.float32' object cannot be interpreted as an integer")

/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:43587
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:43587
distributed.worker - INFO -          dashboard at:      198.202.101.126:44227
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cbiuovoo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:40905
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:40905
distributed.worker - INFO -          dashboard at:      198.202.101.126:44145
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7ij30tts
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:35953
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:35953
distributed.worker - INFO -          dashboard at:      198.202.101.126:38795
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wc9xtpix
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:46059
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:46059
distributed.worker - INFO -          dashboard at:      198.202.101.126:33791
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wzyvukgg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:33519
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:33519
distributed.worker - INFO -          dashboard at:      198.202.101.126:37657
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rylntvz7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:41811
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:41811
distributed.worker - INFO -          dashboard at:      198.202.101.126:43953
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-r2fy53dc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:35885
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:35885
distributed.worker - INFO -          dashboard at:      198.202.101.126:37659
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-k3ibl1jb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:37795
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:37795
distributed.worker - INFO -          dashboard at:      198.202.101.126:34565
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wbedh4ba
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:35997
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:35997
distributed.worker - INFO -          dashboard at:      198.202.101.126:41897
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-al08z8vc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:44965
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:44965
distributed.worker - INFO -          dashboard at:      198.202.101.126:39329
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-r9uzvyra
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:35595
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:35595
distributed.worker - INFO -          dashboard at:      198.202.101.126:44503
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zry78szv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:35837
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:35837
distributed.worker - INFO -          dashboard at:      198.202.101.126:38005
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-y4nuqejm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:42695
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:42695
distributed.worker - INFO -          dashboard at:      198.202.101.126:40629
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pwvy_g1v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:43015
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:43015
distributed.worker - INFO -          dashboard at:      198.202.101.126:40767
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wmgd2yz8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:33199
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:33199
distributed.worker - INFO -          dashboard at:      198.202.101.126:35471
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jximr0th
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:43601
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:43601
distributed.worker - INFO -          dashboard at:      198.202.101.126:33801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ypm9nmbv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:42433
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:42433
distributed.worker - INFO -          dashboard at:      198.202.101.126:41303
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-157vw84g
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:39805
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:39805
distributed.worker - INFO -          dashboard at:      198.202.101.126:43199
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ei37usu7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:44123
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:44123
distributed.worker - INFO -          dashboard at:      198.202.101.126:35531
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6hjgkyfb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:45457
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:45457
distributed.worker - INFO -          dashboard at:      198.202.101.126:32871
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fm1c_aww
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.126:45281
distributed.worker - INFO -          Listening to: tcp://198.202.101.126:45281
distributed.worker - INFO -          dashboard at:      198.202.101.126:33829
distributed.worker - INFO - Waiting to connect to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n8qfism8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.102.90:39767
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd: error: *** JOB 1299284 ON exp-12-37 CANCELLED AT 2021-02-14T18:25:19 ***
