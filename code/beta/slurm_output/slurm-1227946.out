distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:33403'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:42007'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41359'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:40685'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:35525'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:42025'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:35659'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41989'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:32847'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:39887'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:44499'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:43773'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:43253'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41761'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:42955'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:40979'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41451'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:45083'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:46411'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:34337'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:37675'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:32991'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:36863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:38083'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:43107'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41629'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:44317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41573'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:42271'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:42725'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:37613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:40811'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:43039'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:38297'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:42105'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:45203'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:36247'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:46203'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:39001'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:43711'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:36645'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:33923'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:33971'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:33523'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:33731'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41699'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:39891'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:36205'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:40441'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:40123'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41591'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:45249'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41075'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:34027'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:34489'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:34417'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41583'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41277'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:43535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:36811'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:36093'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:38791'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:42171'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:32783'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:35009'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:38041'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:46397'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:37681'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:38241'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:46775'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:36551'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:43147'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:40341'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:44371'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41113'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41503'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:37355'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:44467'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:36133'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:45819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:42527'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:37205'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:42927'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:35143'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:45465'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41721'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:35567'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:33535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:35241'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:41427'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:33415'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:34611'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:45265'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:43311'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:40569'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:43289'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:36265'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:44631'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:32933'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.46:40829'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:40555
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:46167
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:44263
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:39863
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:40555
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:46167
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:44263
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:39863
distributed.worker - INFO -          dashboard at:       198.202.102.46:41563
distributed.worker - INFO -          dashboard at:       198.202.102.46:35669
distributed.worker - INFO -          dashboard at:       198.202.102.46:41921
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO -          dashboard at:       198.202.102.46:46145
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0i2h8oh8
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cnxn6wwb
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_0cxlnui
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zxswc7tu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:34861
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:34861
distributed.worker - INFO -          dashboard at:       198.202.102.46:35367
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n7au6ta9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:41349
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:41349
distributed.worker - INFO -          dashboard at:       198.202.102.46:33359
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2r3qka59
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:39833
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:39833
distributed.worker - INFO -          dashboard at:       198.202.102.46:32837
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yq6gpjsm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:46765
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:46765
distributed.worker - INFO -          dashboard at:       198.202.102.46:38551
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hjgf0ymp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:39789
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:39789
distributed.worker - INFO -          dashboard at:       198.202.102.46:36623
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zwpxgu0t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:39639
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:39639
distributed.worker - INFO -          dashboard at:       198.202.102.46:42741
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ns76q7s7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:42361
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:42361
distributed.worker - INFO -          dashboard at:       198.202.102.46:42515
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-r4944bwi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:43105
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:43105
distributed.worker - INFO -          dashboard at:       198.202.102.46:43375
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w9kq5ssl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:35341
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:35341
distributed.worker - INFO -          dashboard at:       198.202.102.46:37133
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-z4o9yjad
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:43219
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:43219
distributed.worker - INFO -          dashboard at:       198.202.102.46:36235
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:35553
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p2m34wyb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:35553
distributed.worker - INFO -          dashboard at:       198.202.102.46:39123
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4biy9j9t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:44835
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:44835
distributed.worker - INFO -          dashboard at:       198.202.102.46:34807
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-z07_hsm1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:40801
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:40801
distributed.worker - INFO -          dashboard at:       198.202.102.46:38623
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-04s44qmu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:42915
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:42915
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:33267
distributed.worker - INFO -          dashboard at:       198.202.102.46:39783
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:33267
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:       198.202.102.46:45277
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9ktkqnyd
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xhg8r1w7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:45685
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:45685
distributed.worker - INFO -          dashboard at:       198.202.102.46:46365
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8e1dfybn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:41819
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:41819
distributed.worker - INFO -          dashboard at:       198.202.102.46:36515
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-h4vyvwsl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:42131
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:42131
distributed.worker - INFO -          dashboard at:       198.202.102.46:39259
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kgn7u16x
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:41865
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:41865
distributed.worker - INFO -          dashboard at:       198.202.102.46:44523
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-f2mrqu1u
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:39921
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:39921
distributed.worker - INFO -          dashboard at:       198.202.102.46:41873
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1fq9j7in
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:35119
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:35119
distributed.worker - INFO -          dashboard at:       198.202.102.46:42213
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fr7c3wka
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:43185
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:43185
distributed.worker - INFO -          dashboard at:       198.202.102.46:41061
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-873gyifc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:35055
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:35055
distributed.worker - INFO -          dashboard at:       198.202.102.46:37489
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pr3x3sws
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:39753
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:39753
distributed.worker - INFO -          dashboard at:       198.202.102.46:44753
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2wf_t2l5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:41175
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:41175
distributed.worker - INFO -          dashboard at:       198.202.102.46:44355
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ouiw_nrr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:43011
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:43011
distributed.worker - INFO -          dashboard at:       198.202.102.46:38191
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pqpwh_hf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:39447
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:39447
distributed.worker - INFO -          dashboard at:       198.202.102.46:44625
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xew7jgcd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:34787
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:34787
distributed.worker - INFO -          dashboard at:       198.202.102.46:45531
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3lehxfns
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:43277
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:43277
distributed.worker - INFO -          dashboard at:       198.202.102.46:36549
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p3idkqel
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:43271
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:43271
distributed.worker - INFO -          dashboard at:       198.202.102.46:33675
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rammrff8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:37823
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:37823
distributed.worker - INFO -          dashboard at:       198.202.102.46:35073
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ffw8ur7_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:46741
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:46741
distributed.worker - INFO -          dashboard at:       198.202.102.46:36627
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-58mnsu8x
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:33305
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:33305
distributed.worker - INFO -          dashboard at:       198.202.102.46:42273
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ha50me4n
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:36369
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:36369
distributed.worker - INFO -          dashboard at:       198.202.102.46:40159
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_ts76zkq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:35943
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:35943
distributed.worker - INFO -          dashboard at:       198.202.102.46:37477
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0sai0b31
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:40493
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:40493
distributed.worker - INFO -          dashboard at:       198.202.102.46:46683
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-b3xsqh_y
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:39825
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:39825
distributed.worker - INFO -          dashboard at:       198.202.102.46:36997
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tus949gv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:42485
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:42485
distributed.worker - INFO -          dashboard at:       198.202.102.46:44335
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6mjwm5fe
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:36691
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:36691
distributed.worker - INFO -          dashboard at:       198.202.102.46:45793
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-usj3c6k2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:46481
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:46481
distributed.worker - INFO -          dashboard at:       198.202.102.46:37207
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u1mouk4u
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:40695
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:40695
distributed.worker - INFO -          dashboard at:       198.202.102.46:41465
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ho04xaxz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:34903
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:34903
distributed.worker - INFO -          dashboard at:       198.202.102.46:45851
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4aw5zy27
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:46053
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:46053
distributed.worker - INFO -          dashboard at:       198.202.102.46:42491
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cnp0g20b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:34883
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:34883
distributed.worker - INFO -          dashboard at:       198.202.102.46:34361
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wuqft8yi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:37303
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:37303
distributed.worker - INFO -          dashboard at:       198.202.102.46:36553
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-l14royo0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:44203
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:33557
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:33557
distributed.worker - INFO -          dashboard at:       198.202.102.46:44407
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:44203
distributed.worker - INFO -          dashboard at:       198.202.102.46:41331
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3awgu83g
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sdfil11e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:37123
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:37123
distributed.worker - INFO -          dashboard at:       198.202.102.46:46021
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-do8_2lk9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:35971
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:35971
distributed.worker - INFO -          dashboard at:       198.202.102.46:38753
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fojvho7o
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:34303
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:34303
distributed.worker - INFO -          dashboard at:       198.202.102.46:34867
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-niu7c37n
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:36395
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:36395
distributed.worker - INFO -          dashboard at:       198.202.102.46:45771
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6p0ozyzb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:43985
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:43985
distributed.worker - INFO -          dashboard at:       198.202.102.46:46821
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_49_bol3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:43611
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:43611
distributed.worker - INFO -          dashboard at:       198.202.102.46:35969
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vgpx6naf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:44491
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:44491
distributed.worker - INFO -          dashboard at:       198.202.102.46:45489
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kygpay1s
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:35571
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:35571
distributed.worker - INFO -          dashboard at:       198.202.102.46:41077
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7yborli4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:41419
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:41419
distributed.worker - INFO -          dashboard at:       198.202.102.46:43757
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rt9w76uu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:42159
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:42159
distributed.worker - INFO -          dashboard at:       198.202.102.46:39427
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-a4lk2f_b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:35063
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:35063
distributed.worker - INFO -          dashboard at:       198.202.102.46:42487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cd8mjaet
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:45437
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:45437
distributed.worker - INFO -          dashboard at:       198.202.102.46:37375
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ww3id4bp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:37313
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:37313
distributed.worker - INFO -          dashboard at:       198.202.102.46:36041
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-thlwr_ly
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:40969
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:40969
distributed.worker - INFO -          dashboard at:       198.202.102.46:46565
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-dc5xnxk_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:42803
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:42803
distributed.worker - INFO -          dashboard at:       198.202.102.46:41483
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-h9ybqy0y
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:42669
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:42669
distributed.worker - INFO -          dashboard at:       198.202.102.46:41671
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zjct5_ic
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:43991
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:43991
distributed.worker - INFO -          dashboard at:       198.202.102.46:43439
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9_2u7hml
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:46253
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:46253
distributed.worker - INFO -          dashboard at:       198.202.102.46:44507
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:35153
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:35153
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -          dashboard at:       198.202.102.46:33307
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jk70w8o7
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qiw2mi0w
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:35069
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:35069
distributed.worker - INFO -          dashboard at:       198.202.102.46:45197
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-oyf_42yj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:38625
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:38625
distributed.worker - INFO -          dashboard at:       198.202.102.46:36807
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-byfcl_p8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:41527
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:41527
distributed.worker - INFO -          dashboard at:       198.202.102.46:42059
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-k18yd1yu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:35473
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:35473
distributed.worker - INFO -          dashboard at:       198.202.102.46:37495
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wy06s6je
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:34425
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:34425
distributed.worker - INFO -          dashboard at:       198.202.102.46:41773
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-be77w19t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:34427
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:34427
distributed.worker - INFO -          dashboard at:       198.202.102.46:39893
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qufmlofl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:41661
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:41661
distributed.worker - INFO -          dashboard at:       198.202.102.46:42247
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-393vp6h3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:33867
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:33867
distributed.worker - INFO -          dashboard at:       198.202.102.46:35561
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-brwmcilb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:32771
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:32771
distributed.worker - INFO -          dashboard at:       198.202.102.46:36919
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lf3q4iqt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:34131
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:34131
distributed.worker - INFO -          dashboard at:       198.202.102.46:39973
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bcxg_hvf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:37217
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:37217
distributed.worker - INFO -          dashboard at:       198.202.102.46:43501
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7c30_fgq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:46955
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:46955
distributed.worker - INFO -          dashboard at:       198.202.102.46:38959
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-aki_83xs
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:43845
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:43845
distributed.worker - INFO -          dashboard at:       198.202.102.46:38515
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zyh0fijd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:34715
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:34715
distributed.worker - INFO -          dashboard at:       198.202.102.46:34875
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7dcsf6m0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:46639
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:46639
distributed.worker - INFO -          dashboard at:       198.202.102.46:40307
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-aua2x5jo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:43389
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:43389
distributed.worker - INFO -          dashboard at:       198.202.102.46:44425
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-l6phz8bj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:35175
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:35175
distributed.worker - INFO -          dashboard at:       198.202.102.46:41353
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-glzc9yh4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:39943
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:39943
distributed.worker - INFO -          dashboard at:       198.202.102.46:38227
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8jum83cd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:40203
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:40203
distributed.worker - INFO -          dashboard at:       198.202.102.46:42197
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ng_2u1jn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:33407
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:33407
distributed.worker - INFO -          dashboard at:       198.202.102.46:37829
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xq_v2bi1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:45233
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:45233
distributed.worker - INFO -          dashboard at:       198.202.102.46:37027
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uxdvd6r6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:41585
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:41585
distributed.worker - INFO -          dashboard at:       198.202.102.46:41601
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zi56avre
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:44273
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:44273
distributed.worker - INFO -          dashboard at:       198.202.102.46:37049
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-719lgyyu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:36745
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:36745
distributed.worker - INFO -          dashboard at:       198.202.102.46:33309
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qz5x_720
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:43425
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:43425
distributed.worker - INFO -          dashboard at:       198.202.102.46:36069
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tn89iws9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:45559
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:45559
distributed.worker - INFO -          dashboard at:       198.202.102.46:37389
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-h5ah9988
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:37125
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:37125
distributed.worker - INFO -          dashboard at:       198.202.102.46:40291
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:37233
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:37233
distributed.worker - INFO -          dashboard at:       198.202.102.46:38317
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rnya_t20
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hb23u08r
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:44241
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:44241
distributed.worker - INFO -          dashboard at:       198.202.102.46:32895
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-62a7lu24
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.46:44711
distributed.worker - INFO -          Listening to: tcp://198.202.102.46:44711
distributed.worker - INFO -          dashboard at:       198.202.102.46:44735
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tjsoc288
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:33403'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:42007'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:39863
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41359'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:44263
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:40685'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:35525'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:46167
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:42025'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:40555
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:35659'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:39833
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41989'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:34861
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:32847'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:46765
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:39887'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:41349
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:44499'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:39789
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:43773'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:39639
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:43253'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:43219
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41761'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:43105
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:42955'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:35341
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:42361
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:40979'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41451'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:45083'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:35553
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:40801
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:46411'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:44835
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:34337'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:33267
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:37675'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:42915
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:32991'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:41819
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:36863'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:45685
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:38083'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:41865
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:43107'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:42131
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41629'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:44317'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:39921
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:35119
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:43185
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41573'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:42271'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:42725'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:35055
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:39753
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:34787
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:43011
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:37613'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:40811'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:43039'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:41175
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:39447
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:38297'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:42105'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:37823
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:45203'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:43277
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:36247'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:43271
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:46203'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:46741
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:39001'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:33305
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:43711'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:35943
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:36645'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:40493
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:33923'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:36369
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:33971'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:42485
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:33523'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:34903
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:33731'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:36691
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41699'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:40695
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:39891'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:37303
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:36205'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:46053
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:44203
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:40441'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:40123'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:37123
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41591'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:34883
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:45249'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:33557
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41075'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:35971
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:34027'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:36395
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:34489'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:34303
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:43611
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:34417'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41583'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:44491
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41277'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:43985
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:43535'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:35571
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:36811'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:35063
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:36093'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:41419
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:42159
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:37313
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:38791'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:42171'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:32783'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:40969
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:35009'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:45437
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:38041'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:42803
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:46397'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:42669
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:37681'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:35069
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:38241'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:46775'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:43991
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:35153
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:36551'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:35473
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:43147'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:40341'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:34427
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:46253
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:44371'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:38625
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41113'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:34425
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41503'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:41661
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:37355'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:33867
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:44467'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:41527
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:36133'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:46639
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:45819'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:43845
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:42527'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:34131
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:37205'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:39825
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:42927'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:43389
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:35143'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:34715
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:45465'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:37217
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41721'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:46955
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:35567'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:45233
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:33535'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:39943
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:35241'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:35175
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:41427'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:46481
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:33415'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:32771
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:34611'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:43425
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:45265'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:37233
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:43311'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:37125
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:40569'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:33407
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:43289'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:36745
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:36265'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:40203
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:44631'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:44273
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:32933'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:41585
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.46:40829'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:45559
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:44711
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.46:44241
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85864 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85861 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85858 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85855 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85852 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85849 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85846 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85718 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85714 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85702 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85644 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85578 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85575 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85572 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85569 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85567 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85562 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85558 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85557 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85564 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85550 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85554 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85531 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85479 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85459 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85482 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85412 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85407 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85400 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85397 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85402 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85394 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85381 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85385 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85388 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85375 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85372 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85366 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85363 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85370 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85357 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85349 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85353 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85344 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85340 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85337 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85333 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85327 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85325 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85321 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85319 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85315 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85308 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85311 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85303 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85304 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85296 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85293 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85290 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85286 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85282 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85277 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85280 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85271 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85268 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85264 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85260 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85256 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85252 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85247 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85242 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85245 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85236 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85233 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85230 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85224 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85221 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85214 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85213 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85208 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85207 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85204 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85201 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85198 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85195 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85191 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85189 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85186 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85183 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85179 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85177 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85175 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85171 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85168 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85164 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85161 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85159 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85156 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85152 parent=85079 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85153 parent=85079 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
