distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:42243'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:32841'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:35553'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:37615'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:38177'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:46595'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:33517'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:44721'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:44283'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:35287'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:45121'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:36267'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:40783'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:43765'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:41597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:35833'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:39817'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:46361'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:46687'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:40149'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:40713'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:44355'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:40203'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:42733'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:39275'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:40387'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:43579'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:45617'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:45829'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:42349'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:43917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:34107'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:45445'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:34563'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:46953'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:33633'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:38925'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:44755'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:39927'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:39595'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:40645'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:35459'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:39063'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:43613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:45803'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:39029'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:38339'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:35225'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:44967'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:33385'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:41135'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:42109'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:36263'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:36127'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:46549'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:42283'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:35441'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:41513'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:41925'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:42393'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:45981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:45935'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:38787'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:33423'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:40757'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:38137'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:42355'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:35643'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:34819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:46223'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:41025'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:42229'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:34889'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:43961'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:42129'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:40621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:40875'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:42465'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:33319'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:43291'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:39863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:44079'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:32885'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:41715'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:45587'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:39519'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:43407'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:37023'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:40205'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:45799'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:45289'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:39923'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:37457'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:38937'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:40091'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:43179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:37307'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:33257'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:33929'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.244:45485'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:39265
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:39099
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:39099
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:39265
distributed.worker - INFO -          dashboard at:      198.202.102.244:45909
distributed.worker - INFO -          dashboard at:      198.202.102.244:41119
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6aic_9q8
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2ez9v6yj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:38405
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:38405
distributed.worker - INFO -          dashboard at:      198.202.102.244:44045
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-alzsvrab
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:34685
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:34685
distributed.worker - INFO -          dashboard at:      198.202.102.244:47005
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wpfdan6s
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:36601
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:36601
distributed.worker - INFO -          dashboard at:      198.202.102.244:44703
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8hqrv03e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:46029
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:46029
distributed.worker - INFO -          dashboard at:      198.202.102.244:39455
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-e9u9aofk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:35403
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:35403
distributed.worker - INFO -          dashboard at:      198.202.102.244:34613
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-adh_p2qh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:35983
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:35983
distributed.worker - INFO -          dashboard at:      198.202.102.244:45329
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hdwm4zfp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:44813
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:44813
distributed.worker - INFO -          dashboard at:      198.202.102.244:42475
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n9mi0yxd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:34987
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:34987
distributed.worker - INFO -          dashboard at:      198.202.102.244:41173
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-x4qsyqfb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:39527
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:39527
distributed.worker - INFO -          dashboard at:      198.202.102.244:39507
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-y94w4fjx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:41789
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:41789
distributed.worker - INFO -          dashboard at:      198.202.102.244:34247
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-upsth115
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:33441
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:33441
distributed.worker - INFO -          dashboard at:      198.202.102.244:35811
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ij2d821s
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:40589
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:40589
distributed.worker - INFO -          dashboard at:      198.202.102.244:36701
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9k5npqec
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:36773
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:36773
distributed.worker - INFO -          dashboard at:      198.202.102.244:37907
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3xpz6zou
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:35483
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:35483
distributed.worker - INFO -          dashboard at:      198.202.102.244:33605
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xcg_et2k
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:32893
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:32893
distributed.worker - INFO -          dashboard at:      198.202.102.244:39621
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ykstpyto
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:41081
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:41081
distributed.worker - INFO -          dashboard at:      198.202.102.244:33915
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-iwsk6exn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:46671
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:46671
distributed.worker - INFO -          dashboard at:      198.202.102.244:42189
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-q6nugzzi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:39395
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:39395
distributed.worker - INFO -          dashboard at:      198.202.102.244:32939
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-c_91i3x3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:44655
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:44655
distributed.worker - INFO -          dashboard at:      198.202.102.244:44881
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-knd40jzs
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:37025
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:37025
distributed.worker - INFO -          dashboard at:      198.202.102.244:38011
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p3ctagak
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:38169
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:38169
distributed.worker - INFO -          dashboard at:      198.202.102.244:36303
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qmtib53m
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:39601
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:39601
distributed.worker - INFO -          dashboard at:      198.202.102.244:46795
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4a96u5m5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:46705
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:46705
distributed.worker - INFO -          dashboard at:      198.202.102.244:40909
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w3k10ew2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:43885
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:43885
distributed.worker - INFO -          dashboard at:      198.202.102.244:45637
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-eb6arqre
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:41377
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:41377
distributed.worker - INFO -          dashboard at:      198.202.102.244:47051
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hi6ytzks
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:33739
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:33739
distributed.worker - INFO -          dashboard at:      198.202.102.244:42749
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:40405
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:40405
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:      198.202.102.244:36531
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wwkicxwq
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:42609
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mnf92tmw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:42609
distributed.worker - INFO -          dashboard at:      198.202.102.244:42273
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2758b5ru
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:36269
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:36269
distributed.worker - INFO -          dashboard at:      198.202.102.244:46781
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hwlxtvo_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:45487
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:45487
distributed.worker - INFO -          dashboard at:      198.202.102.244:40643
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uhulebcb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:45769
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:45769
distributed.worker - INFO -          dashboard at:      198.202.102.244:33431
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bw9e9vnx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:37537
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:37537
distributed.worker - INFO -          dashboard at:      198.202.102.244:32979
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wlxweqgx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:43865
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:43865
distributed.worker - INFO -          dashboard at:      198.202.102.244:38819
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-m892w6_y
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:46069
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:46069
distributed.worker - INFO -          dashboard at:      198.202.102.244:35599
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-efb88a2l
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:43127
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:43127
distributed.worker - INFO -          dashboard at:      198.202.102.244:43607
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mbji_4vw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:35913
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:35913
distributed.worker - INFO -          dashboard at:      198.202.102.244:41651
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-l54i36g2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:39065
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:39065
distributed.worker - INFO -          dashboard at:      198.202.102.244:36223
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ueyvqz90
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:44255
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:44255
distributed.worker - INFO -          dashboard at:      198.202.102.244:35995
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1suwj00g
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:36855
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:36855
distributed.worker - INFO -          dashboard at:      198.202.102.244:36275
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ea1n8y2a
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:44835
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:44835
distributed.worker - INFO -          dashboard at:      198.202.102.244:45207
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-slwu54kp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:44489
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:44489
distributed.worker - INFO -          dashboard at:      198.202.102.244:39627
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ghl1ts6l
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:38535
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:38535
distributed.worker - INFO -          dashboard at:      198.202.102.244:34035
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ds_o2p1h
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:40173
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:40173
distributed.worker - INFO -          dashboard at:      198.202.102.244:34555
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-g_9dbaqv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:34585
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:34585
distributed.worker - INFO -          dashboard at:      198.202.102.244:41673
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w5f226mz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:40207
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:40207
distributed.worker - INFO -          dashboard at:      198.202.102.244:38443
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tujeaeo4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:40623
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:40623
distributed.worker - INFO -          dashboard at:      198.202.102.244:33499
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sg85xbae
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:43363
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:43363
distributed.worker - INFO -          dashboard at:      198.202.102.244:43145
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uqi0hzpj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:46779
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:46779
distributed.worker - INFO -          dashboard at:      198.202.102.244:35891
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zz0piz9c
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:45987
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:45987
distributed.worker - INFO -          dashboard at:      198.202.102.244:36487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hsdxx4_4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:39635
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:39635
distributed.worker - INFO -          dashboard at:      198.202.102.244:38785
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rt6ypvuz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:36347
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:36347
distributed.worker - INFO -          dashboard at:      198.202.102.244:34271
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sl5bsvdn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:34213
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:34213
distributed.worker - INFO -          dashboard at:      198.202.102.244:44229
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-s9i3p8av
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:33041
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:33041
distributed.worker - INFO -          dashboard at:      198.202.102.244:40025
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_6ie8414
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:46453
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:46453
distributed.worker - INFO -          dashboard at:      198.202.102.244:44823
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6_o6ufsu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:38689
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:38689
distributed.worker - INFO -          dashboard at:      198.202.102.244:36817
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-r993l1x9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:42937
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:42937
distributed.worker - INFO -          dashboard at:      198.202.102.244:43673
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-dg39huwd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:42347
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:42347
distributed.worker - INFO -          dashboard at:      198.202.102.244:40017
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-nmhkyoyd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:43177
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:43177
distributed.worker - INFO -          dashboard at:      198.202.102.244:42719
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0ln0zehh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:42773
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:42773
distributed.worker - INFO -          dashboard at:      198.202.102.244:46237
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p3p70ep7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:44133
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:44133
distributed.worker - INFO -          dashboard at:      198.202.102.244:36049
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-elkzl1a7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:43933
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:43933
distributed.worker - INFO -          dashboard at:      198.202.102.244:46617
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8p5c9pqg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:41875
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:41875
distributed.worker - INFO -          dashboard at:      198.202.102.244:43511
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-knygmj6c
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:33481
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:33481
distributed.worker - INFO -          dashboard at:      198.202.102.244:41057
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-m_kq53dh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:36501
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:36501
distributed.worker - INFO -          dashboard at:      198.202.102.244:44333
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-33bx6req
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:34361
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:34361
distributed.worker - INFO -          dashboard at:      198.202.102.244:38517
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-dus2z45a
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:38253
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:38253
distributed.worker - INFO -          dashboard at:      198.202.102.244:34751
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hpbauz4q
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:32877
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:32877
distributed.worker - INFO -          dashboard at:      198.202.102.244:33691
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kh259exk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:36005
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:36005
distributed.worker - INFO -          dashboard at:      198.202.102.244:46409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-auqmh8fu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:43775
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:43775
distributed.worker - INFO -          dashboard at:      198.202.102.244:41061
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vamdwxv3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:36439
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:36439
distributed.worker - INFO -          dashboard at:      198.202.102.244:41185
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3w2rocyw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:43403
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:43403
distributed.worker - INFO -          dashboard at:      198.202.102.244:34403
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-memohq3a
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:38623
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:38623
distributed.worker - INFO -          dashboard at:      198.202.102.244:33419
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_g1bjcqr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:42689
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:42689
distributed.worker - INFO -          dashboard at:      198.202.102.244:38817
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zinfjhrt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:34935
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:34935
distributed.worker - INFO -          dashboard at:      198.202.102.244:43261
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-azohdkh2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:43717
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:43717
distributed.worker - INFO -          dashboard at:      198.202.102.244:46521
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-g8ui9hpd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:39335
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:39335
distributed.worker - INFO -          dashboard at:      198.202.102.244:41687
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-axs7k87q
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:39017
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:39017
distributed.worker - INFO -          dashboard at:      198.202.102.244:46615
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5ai7qa_5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:44589
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:44589
distributed.worker - INFO -          dashboard at:      198.202.102.244:42923
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2o8kr5ee
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:35683
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:35683
distributed.worker - INFO -          dashboard at:      198.202.102.244:36059
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sa8a6bkl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:34963
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:34963
distributed.worker - INFO -          dashboard at:      198.202.102.244:38603
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-l103s09b
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:44139
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:44139
distributed.worker - INFO -          dashboard at:      198.202.102.244:35101
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6k57e5li
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:37107
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:37107
distributed.worker - INFO -          dashboard at:      198.202.102.244:44159
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kmh6lrcc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:44549
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:44549
distributed.worker - INFO -          dashboard at:      198.202.102.244:45365
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-in7xaiji
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:40011
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:40011
distributed.worker - INFO -          dashboard at:      198.202.102.244:46741
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-a9bd5bbu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:40237
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:40237
distributed.worker - INFO -          dashboard at:      198.202.102.244:39859
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-by4ixlk0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:42673
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:42673
distributed.worker - INFO -          dashboard at:      198.202.102.244:34463
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-iv81d7uc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:41905
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:41905
distributed.worker - INFO -          dashboard at:      198.202.102.244:39599
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kdc1h98s
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:40109
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:40109
distributed.worker - INFO -          dashboard at:      198.202.102.244:34599
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ztp_3v4l
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:40383
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:40383
distributed.worker - INFO -          dashboard at:      198.202.102.244:42763
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1sk73jt2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:46973
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:46973
distributed.worker - INFO -          dashboard at:      198.202.102.244:44101
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9sz7igre
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:43019
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:43019
distributed.worker - INFO -          dashboard at:      198.202.102.244:34129
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-52_dqftv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:42191
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:42191
distributed.worker - INFO -          dashboard at:      198.202.102.244:39041
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ov_ymf64
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:42517
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:42517
distributed.worker - INFO -          dashboard at:      198.202.102.244:42583
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p7wdnbkt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:39191
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:39191
distributed.worker - INFO -          dashboard at:      198.202.102.244:35259
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3tk76q6q
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:40735
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:40735
distributed.worker - INFO -          dashboard at:      198.202.102.244:36021
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kbkodtcv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:42357
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:42357
distributed.worker - INFO -          dashboard at:      198.202.102.244:40815
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-r6skx8xq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:42285
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:42285
distributed.worker - INFO -          dashboard at:      198.202.102.244:37281
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lt8c3eo2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.244:43121
distributed.worker - INFO -          Listening to: tcp://198.202.102.244:43121
distributed.worker - INFO -          dashboard at:      198.202.102.244:36699
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9cz3z_zh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:42243'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:32841'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:34685
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:35553'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:39099
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:37615'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:39265
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:38177'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:46595'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:38405
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:33517'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:36601
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:44721'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:35403
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:44283'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:46029
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:35287'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:35983
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:45121'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:44813
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:36267'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:39527
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:40783'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:34987
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:43765'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:41789
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:33441
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:40589
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:41597'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:35833'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:39817'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:36773
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:46361'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:35483
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:46687'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:46671
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:40149'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:32893
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:40713'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:41081
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:44355'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:37025
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:40203'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:44655
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:42733'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:39395
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:39275'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:38169
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:40387'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:39601
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:43579'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:43885
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:45617'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:41377
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:33739
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:45829'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:42609
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:42349'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:43917'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:40405
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:34107'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:46705
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:45445'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:45769
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:36269
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:45487
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:34563'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:46953'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:33633'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:37537
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:38925'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:43865
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:44755'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:46069
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:39927'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:43127
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:39595'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:40645'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:35913
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:44255
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:35459'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:39063'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:44835
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:36855
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:43613'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:39065
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:45803'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:38535
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:39029'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:40173
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:38339'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:34585
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:35225'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:40623
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:44967'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:43363
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:33385'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:45987
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:41135'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:42109'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:40207
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:36347
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:36263'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:46779
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:36127'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:46453
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:46549'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:34213
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:42283'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:33041
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:39635
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:35441'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:38689
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:41513'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:41925'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:42937
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:42393'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:43177
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:45981'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:42347
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:45935'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:38787'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:42773
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:41875
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:33423'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:33481
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:40757'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:34361
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:38137'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:44133
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:42355'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:36439
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:35643'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:36501
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:34819'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:38253
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:43933
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:36005
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:46223'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:41025'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:42229'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:34889'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:43403
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:38623
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:43961'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:39017
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:42129'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:32877
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:40621'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:42689
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:40875'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:39335
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:42465'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:43717
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:33319'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:44489
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:43291'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:44139
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:39863'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:44079'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:43775
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:34935
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:32885'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:37107
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:41715'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:44549
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:45587'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:44589
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:39519'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:40237
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:43407'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:40109
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:37023'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:35683
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:40205'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:41905
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:45799'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:40011
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:45289'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:34963
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:39923'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:39191
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:37457'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:40735
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:38937'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:42191
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:40091'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:40383
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:43179'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:42357
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:37307'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:46973
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:33257'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:43121
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:33929'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:42517
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.244:45485'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:43019
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:42285
distributed.worker - INFO - Stopping worker at tcp://198.202.102.244:42673
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54725 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54722 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54719 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54718 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54651 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54634 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54582 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54579 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54576 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54573 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54568 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54566 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54561 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54563 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54489 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54494 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54477 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54402 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54421 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54347 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54351 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54345 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54341 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54338 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54333 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54334 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54330 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54323 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54328 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54325 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54299 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54254 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54248 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54245 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54238 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54251 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54167 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54174 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54165 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54161 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54156 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54154 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54151 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54143 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54140 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54131 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54133 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54126 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54128 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54122 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54119 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54112 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54115 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54109 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54106 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54103 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54099 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54094 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54088 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54089 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54083 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54080 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54076 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54072 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54069 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54066 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54063 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54058 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54052 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54051 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54045 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54044 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54040 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54037 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54025 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54022 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54019 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54009 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54013 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54005 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54002 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53999 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53993 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53990 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53983 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53981 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53975 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53971 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53967 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53969 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53963 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53958 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53956 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53953 parent=53880 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53955 parent=53880 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
