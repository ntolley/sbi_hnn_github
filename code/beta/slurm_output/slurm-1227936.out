distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:35713'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:35445'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:44781'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:36987'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:40299'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:34119'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:47023'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:38165'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:37531'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:43853'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:42801'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:36481'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:44357'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:37915'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:36723'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:33037'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:45589'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:36485'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:36227'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:33781'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:41961'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:38813'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:44203'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:39277'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:45257'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:36049'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:44235'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:46961'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:35861'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:41931'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:36211'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:41401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:46839'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:38791'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:42581'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:39795'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:44075'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:46893'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:43521'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:36487'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:38691'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:44439'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:44355'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:40673'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:38509'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:38969'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:36017'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:45399'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:33729'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:40919'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:37747'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:39757'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:44651'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:44643'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:36367'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:40815'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:44607'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:32957'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:45805'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:41955'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:35577'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:34155'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:36069'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:45195'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:39537'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:40709'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:44441'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:40903'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:39323'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:44933'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:41881'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:37833'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:40571'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:41375'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:46201'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:33075'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:36473'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:33833'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:43311'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:37961'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:35961'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:46385'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:43053'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:46419'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:46727'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:35659'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:37951'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:33573'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:46225'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:39951'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:35009'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:41691'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:34001'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:42789'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:33825'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:38535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:43645'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:39283'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:46783'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.143:45769'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:40607
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:46213
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:36133
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:40607
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:46213
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:36133
distributed.worker - INFO -          dashboard at:      198.202.101.143:34669
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -          dashboard at:      198.202.101.143:44945
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -          dashboard at:      198.202.101.143:34991
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-dntkvgup
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-k3im4hpo
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p29lirri
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:36667
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:36667
distributed.worker - INFO -          dashboard at:      198.202.101.143:35131
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1dsd8ju_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:33849
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:33849
distributed.worker - INFO -          dashboard at:      198.202.101.143:45955
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sgl2_b53
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:34779
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:34779
distributed.worker - INFO -          dashboard at:      198.202.101.143:43753
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-v74y9mhj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:38989
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:38989
distributed.worker - INFO -          dashboard at:      198.202.101.143:38773
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xh3rwcvf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:39645
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:39645
distributed.worker - INFO -          dashboard at:      198.202.101.143:35595
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2mcwoxc2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:36561
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:36561
distributed.worker - INFO -          dashboard at:      198.202.101.143:37939
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ho0_3cjo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:36071
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:36071
distributed.worker - INFO -          dashboard at:      198.202.101.143:33853
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lj0xwyut
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:38457
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:38457
distributed.worker - INFO -          dashboard at:      198.202.101.143:43135
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0ruqtgev
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:38323
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:38323
distributed.worker - INFO -          dashboard at:      198.202.101.143:39437
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sfkrrtwu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:42979
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:42979
distributed.worker - INFO -          dashboard at:      198.202.101.143:39005
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-33u7z6i3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:37383
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:37383
distributed.worker - INFO -          dashboard at:      198.202.101.143:34451
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2lbgt_v5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:37839
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:37839
distributed.worker - INFO -          dashboard at:      198.202.101.143:42463
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uzrszkfi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:36599
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:36599
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:34097
distributed.worker - INFO -          dashboard at:      198.202.101.143:42817
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:34097
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -          dashboard at:      198.202.101.143:45753
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-anxfbpqq
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_0p2kww8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:39717
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:39717
distributed.worker - INFO -          dashboard at:      198.202.101.143:42289
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3j9gfgre
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:33907
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:33907
distributed.worker - INFO -          dashboard at:      198.202.101.143:33133
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:34977
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:34977
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -          dashboard at:      198.202.101.143:45925
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kt85m122
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0s_sxrjg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:38209
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:38209
distributed.worker - INFO -          dashboard at:      198.202.101.143:46555
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-g495ycy0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:33641
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:33641
distributed.worker - INFO -          dashboard at:      198.202.101.143:37743
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sfea7vhz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:46871
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:46871
distributed.worker - INFO -          dashboard at:      198.202.101.143:44117
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-e0n0sdvk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:44465
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:44465
distributed.worker - INFO -          dashboard at:      198.202.101.143:35599
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-91fk6ddk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:34333
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:34333
distributed.worker - INFO -          dashboard at:      198.202.101.143:45649
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-326v6u93
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:35227
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:35227
distributed.worker - INFO -          dashboard at:      198.202.101.143:32785
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bhx7xzzw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:45579
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:45579
distributed.worker - INFO -          dashboard at:      198.202.101.143:41869
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1wzo7fv4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:38329
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:38329
distributed.worker - INFO -          dashboard at:      198.202.101.143:43967
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_ndxvey5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:39709
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:39709
distributed.worker - INFO -          dashboard at:      198.202.101.143:45103
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5ukdr3xu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:35881
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:35881
distributed.worker - INFO -          dashboard at:      198.202.101.143:40619
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gdmgdlvu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:40689
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:40689
distributed.worker - INFO -          dashboard at:      198.202.101.143:42493
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-v4yjm99t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:34639
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:34639
distributed.worker - INFO -          dashboard at:      198.202.101.143:41333
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qrrash7p
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:38685
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:38685
distributed.worker - INFO -          dashboard at:      198.202.101.143:33579
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jcgic5kv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:45543
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:45543
distributed.worker - INFO -          dashboard at:      198.202.101.143:33199
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4bukk6wu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:41577
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:41577
distributed.worker - INFO -          dashboard at:      198.202.101.143:42387
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cnf4ix6x
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:34763
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:34763
distributed.worker - INFO -          dashboard at:      198.202.101.143:42977
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qen1g68x
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:40621
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:40621
distributed.worker - INFO -          dashboard at:      198.202.101.143:45017
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_qykkmu8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:47063
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:47063
distributed.worker - INFO -          dashboard at:      198.202.101.143:39953
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7oibplow
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:33101
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:33101
distributed.worker - INFO -          dashboard at:      198.202.101.143:45233
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1pnio7gm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:43145
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:43145
distributed.worker - INFO -          dashboard at:      198.202.101.143:37181
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-11hjam8v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:34567
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:34567
distributed.worker - INFO -          dashboard at:      198.202.101.143:34377
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rw9a3td0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:36343
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:36343
distributed.worker - INFO -          dashboard at:      198.202.101.143:39877
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4re8_mgb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:43541
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:43541
distributed.worker - INFO -          dashboard at:      198.202.101.143:36591
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-noeqan57
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:45495
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:45495
distributed.worker - INFO -          dashboard at:      198.202.101.143:40441
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ujwl_i7q
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:42673
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:42673
distributed.worker - INFO -          dashboard at:      198.202.101.143:41599
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hszvhweb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:35739
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:35739
distributed.worker - INFO -          dashboard at:      198.202.101.143:46873
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-b3ciffdg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:33139
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:33139
distributed.worker - INFO -          dashboard at:      198.202.101.143:38313
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lcqx4ud9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:38519
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:38519
distributed.worker - INFO -          dashboard at:      198.202.101.143:38595
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-02fhhg3x
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:37877
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:37877
distributed.worker - INFO -          dashboard at:      198.202.101.143:39861
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-dc04sla0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:39635
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:39635
distributed.worker - INFO -          dashboard at:      198.202.101.143:40119
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-z0z3u_46
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:43791
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:43791
distributed.worker - INFO -          dashboard at:      198.202.101.143:36059
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sqkvkzh1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:43341
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:43341
distributed.worker - INFO -          dashboard at:      198.202.101.143:35383
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-b7c8vn3j
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:45621
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:45621
distributed.worker - INFO -          dashboard at:      198.202.101.143:44581
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8oj678yf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:39903
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:39903
distributed.worker - INFO -          dashboard at:      198.202.101.143:36443
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ymih2mlc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:36245
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:36245
distributed.worker - INFO -          dashboard at:      198.202.101.143:41509
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8skyuaqf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:34751
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:34751
distributed.worker - INFO -          dashboard at:      198.202.101.143:46677
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-iak899fc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:40595
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:40595
distributed.worker - INFO -          dashboard at:      198.202.101.143:45197
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ecuxdry8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:37533
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:33609
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:37533
distributed.worker - INFO -          dashboard at:      198.202.101.143:33457
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:33609
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:      198.202.101.143:44679
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3bql2kyl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wlokjri4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:34899
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:34899
distributed.worker - INFO -          dashboard at:      198.202.101.143:43813
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lgqyntmh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:46913
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:46913
distributed.worker - INFO -          dashboard at:      198.202.101.143:44489
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-01hr_k5m
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:32989
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:32989
distributed.worker - INFO -          dashboard at:      198.202.101.143:41519
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6_g_n3pw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:41421
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:41421
distributed.worker - INFO -          dashboard at:      198.202.101.143:45681
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9d71abqj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:40817
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:40817
distributed.worker - INFO -          dashboard at:      198.202.101.143:36239
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ecrhl8lg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:46205
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:46205
distributed.worker - INFO -          dashboard at:      198.202.101.143:35175
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u1tdw5d_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:41383
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:41383
distributed.worker - INFO -          dashboard at:      198.202.101.143:34135
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-m27h47q8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:37339
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:37339
distributed.worker - INFO -          dashboard at:      198.202.101.143:46797
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gnja7h85
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:33547
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:33547
distributed.worker - INFO -          dashboard at:      198.202.101.143:42185
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2r4947ny
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:37263
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:37263
distributed.worker - INFO -          dashboard at:      198.202.101.143:34929
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ymawhx5h
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:40661
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:40661
distributed.worker - INFO -          dashboard at:      198.202.101.143:43123
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vq8vz9he
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:38119
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:38119
distributed.worker - INFO -          dashboard at:      198.202.101.143:40831
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yvdibfav
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:42981
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:42981
distributed.worker - INFO -          dashboard at:      198.202.101.143:44213
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-44h6mxl5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:40287
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:40287
distributed.worker - INFO -          dashboard at:      198.202.101.143:42245
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_ffmi5_d
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:37633
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:37633
distributed.worker - INFO -          dashboard at:      198.202.101.143:38467
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9oc36k77
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:33791
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:33791
distributed.worker - INFO -          dashboard at:      198.202.101.143:45533
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-t6e8_lf3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:34831
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:34831
distributed.worker - INFO -          dashboard at:      198.202.101.143:40975
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mtgvvivc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:33919
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:33919
distributed.worker - INFO -          dashboard at:      198.202.101.143:45277
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6n5j6ge1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:33803
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:33803
distributed.worker - INFO -          dashboard at:      198.202.101.143:38725
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6jtmaphn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:40737
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:40737
distributed.worker - INFO -          dashboard at:      198.202.101.143:36169
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kjs907di
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:39679
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:39679
distributed.worker - INFO -          dashboard at:      198.202.101.143:40051
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2acvkyd2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:41825
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:41825
distributed.worker - INFO -          dashboard at:      198.202.101.143:39069
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-of1gd9jf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:38917
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:38917
distributed.worker - INFO -          dashboard at:      198.202.101.143:44467
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1tg97yta
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:46669
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:46669
distributed.worker - INFO -          dashboard at:      198.202.101.143:44009
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-r9jgyji3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:47079
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:47079
distributed.worker - INFO -          dashboard at:      198.202.101.143:34955
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-smmondrw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:43389
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:43389
distributed.worker - INFO -          dashboard at:      198.202.101.143:44729
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w3yzvky4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:46939
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:46939
distributed.worker - INFO -          dashboard at:      198.202.101.143:34827
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5nxt61od
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:36065
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:36065
distributed.worker - INFO -          dashboard at:      198.202.101.143:45839
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-z_usc6lk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:45819
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:45819
distributed.worker - INFO -          dashboard at:      198.202.101.143:34755
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2v0ak6x2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:44783
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:44783
distributed.worker - INFO -          dashboard at:      198.202.101.143:44559
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-peeq2uk4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:38043
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:38043
distributed.worker - INFO -          dashboard at:      198.202.101.143:43919
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1c4kcwd0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:44769
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:44769
distributed.worker - INFO -          dashboard at:      198.202.101.143:33301
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vqcblqxb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:45337
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:45337
distributed.worker - INFO -          dashboard at:      198.202.101.143:36081
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1ajk10he
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:46123
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:46123
distributed.worker - INFO -          dashboard at:      198.202.101.143:44139
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ucnmi42n
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:42211
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:42211
distributed.worker - INFO -          dashboard at:      198.202.101.143:40895
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-e8crzprl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:40987
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:40987
distributed.worker - INFO -          dashboard at:      198.202.101.143:42261
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7k673uj4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:41939
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:41939
distributed.worker - INFO -          dashboard at:      198.202.101.143:41963
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4dmklgiy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:34153
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:34153
distributed.worker - INFO -          dashboard at:      198.202.101.143:46937
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-skx2kwsw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:38625
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:38625
distributed.worker - INFO -          dashboard at:      198.202.101.143:46547
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mn32t8vz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:37923
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:37923
distributed.worker - INFO -          dashboard at:      198.202.101.143:34657
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yu1r1uu2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.143:43071
distributed.worker - INFO -          Listening to: tcp://198.202.101.143:43071
distributed.worker - INFO -          dashboard at:      198.202.101.143:45515
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1x75tu_x
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:35713'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:35445'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:44781'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:34779
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:36987'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:36133
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:40299'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:36667
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:34119'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:46213
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:47023'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:33849
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:38989
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:38165'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:37531'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:36561
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:36071
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:43853'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:42801'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:39645
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:36481'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:38323
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:44357'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:37383
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:37915'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:38457
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:36723'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:42979
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:40607
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:33037'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:45589'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:36599
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:36485'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:37839
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:36227'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:34097
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:39717
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:33781'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:41961'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:34977
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:46871
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:38813'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:44203'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:33641
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:34333
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:39277'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:45257'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:44465
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:36049'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:35227
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:44235'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:45579
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:46961'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:34639
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:35861'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:38329
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:41931'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:45543
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:36211'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:39709
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:41401'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:40621
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:46839'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:34763
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:38791'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:40689
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:42581'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:41577
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:39795'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:38685
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:44075'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:47063
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:36343
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:46893'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:43521'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:33101
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:36487'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:43145
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:38691'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:44439'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:34567
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:45495
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:44355'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:43541
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:40673'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:42673
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:38509'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:33139
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:38969'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:33907
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:35739
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:36017'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:45399'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:37877
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:33729'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:38519
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:40919'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:37747'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:38209
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:39757'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:36245
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:40595
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:44651'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:43791
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:44643'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:43341
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:36367'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:39903
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:40815'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:39635
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:44607'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:40817
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:32989
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:32957'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:45805'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:35881
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:41955'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:45621
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:46913
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:37533
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:35577'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:34155'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:36069'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:46205
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:45195'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:33609
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:39537'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:34751
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:40709'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:41383
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:44441'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:34899
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:40903'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:39323'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:37339
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:38119
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:44933'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:42981
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:41881'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:40287
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:37833'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:33919
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:40571'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:37633
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:41375'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:46201'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:41421
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:40737
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:33075'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:40661
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:36473'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:39679
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:33833'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:33803
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:43311'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:33547
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:37961'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:41825
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:35961'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:43389
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:46385'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:34831
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:43053'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:47079
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:46419'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:41939
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:46727'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:33791
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:35659'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:37263
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:37951'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:44769
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:33573'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:42211
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:46225'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:46123
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:39951'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:44783
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:35009'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:43071
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:41691'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:38917
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:34001'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:46669
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:42789'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:36065
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:33825'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:45819
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:38535'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:38625
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:43645'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:37923
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:34153
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:46939
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:39283'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:46783'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.143:45769'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:38043
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:40987
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.143:45337
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36726 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36724 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36718 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36715 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36713 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36711 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36708 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36701 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36699 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36690 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36694 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36684 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36678 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36674 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36671 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36668 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36664 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36649 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36659 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36640 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36644 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36636 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36638 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36571 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36552 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36543 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36540 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36536 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36533 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36530 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36528 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36522 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36521 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36515 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36512 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36510 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36507 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36496 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36492 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36494 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36482 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36474 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36458 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36456 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36452 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36449 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36443 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36431 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36416 parent=36338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36412 parent=36338 started daemon>
