distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:46143'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:33129'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:39469'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:33197'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:35925'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:40281'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:33901'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:45673'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:36869'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:44673'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:39051'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:34595'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:33741'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:41913'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:39593'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:39415'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:41283'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:44549'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:43951'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:40099'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:40005'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:44695'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:41375'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:45405'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:34749'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:44323'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:36071'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:33653'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:43307'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:39333'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:45417'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:42125'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:37079'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:36333'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:45783'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:42799'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:39145'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:46539'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:42821'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:46577'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:32775'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:46247'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:42033'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:41303'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:36455'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:38221'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:38865'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:33137'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:42723'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:34129'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:35509'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:34439'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:38409'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:36551'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:39093'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:34177'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:38311'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:33815'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:40343'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:37701'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:40625'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:33809'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:37433'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:41361'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:41253'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:40103'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:45185'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:36881'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:45747'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:41245'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:35985'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:34095'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:41479'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:33751'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:41195'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:42711'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:41471'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:45779'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:41643'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:41597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:33093'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:33633'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:39535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:40717'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:43649'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:34843'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:43773'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:37425'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:33345'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:45815'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:45043'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:34009'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:36207'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:45987'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:42159'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:37125'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:34759'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:45981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:33715'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.201:46283'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:43641
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:38309
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:35833
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:43641
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:35389
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:38309
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:35833
distributed.worker - INFO -          dashboard at:      198.202.103.201:40315
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:35389
distributed.worker - INFO -          dashboard at:      198.202.103.201:45107
distributed.worker - INFO -          dashboard at:      198.202.103.201:39771
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO -          dashboard at:      198.202.103.201:43117
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wsvv2zvz
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-o7vd9gyo
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-oup_i1u_
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sza_fonb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:42001
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:42001
distributed.worker - INFO -          dashboard at:      198.202.103.201:39355
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5qb_xlrg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:36961
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:36961
distributed.worker - INFO -          dashboard at:      198.202.103.201:32787
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-y67kphwz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:39939
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:39939
distributed.worker - INFO -          dashboard at:      198.202.103.201:38753
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0v9lyo7h
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:35807
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:35807
distributed.worker - INFO -          dashboard at:      198.202.103.201:41145
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rqm_l_u4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:40561
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:40561
distributed.worker - INFO -          dashboard at:      198.202.103.201:37591
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-eetcot_i
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:35451
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:35451
distributed.worker - INFO -          dashboard at:      198.202.103.201:41753
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ue75xduq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:43825
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:43825
distributed.worker - INFO -          dashboard at:      198.202.103.201:34841
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-t19juzwd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:44321
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:44321
distributed.worker - INFO -          dashboard at:      198.202.103.201:38573
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vvsvg6o3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:41691
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:41691
distributed.worker - INFO -          dashboard at:      198.202.103.201:44651
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-powuv5be
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:35239
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:35239
distributed.worker - INFO -          dashboard at:      198.202.103.201:41379
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rkiqdk4t
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:42725
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:42725
distributed.worker - INFO -          dashboard at:      198.202.103.201:37751
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ltgad5bg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:36179
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:36179
distributed.worker - INFO -          dashboard at:      198.202.103.201:37669
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8l5q0yyy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:44557
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:44557
distributed.worker - INFO -          dashboard at:      198.202.103.201:34973
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5xg07zaq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:47049
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:47049
distributed.worker - INFO -          dashboard at:      198.202.103.201:43297
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1y5kncvh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:36947
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:36947
distributed.worker - INFO -          dashboard at:      198.202.103.201:40269
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rt48c3q4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:44999
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:44999
distributed.worker - INFO -          dashboard at:      198.202.103.201:41977
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-y83k9qgw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:42029
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:42029
distributed.worker - INFO -          dashboard at:      198.202.103.201:38623
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kepd4abv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:37017
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:37017
distributed.worker - INFO -          dashboard at:      198.202.103.201:44025
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-54fqhq77
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:38721
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:38721
distributed.worker - INFO -          dashboard at:      198.202.103.201:44911
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-a1d1zde_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:46801
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:46801
distributed.worker - INFO -          dashboard at:      198.202.103.201:39633
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-l8vk1abz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:45391
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:45391
distributed.worker - INFO -          dashboard at:      198.202.103.201:32887
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yf0nbjqj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:39667
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:39667
distributed.worker - INFO -          dashboard at:      198.202.103.201:36719
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8mshhd22
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:42727
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:42727
distributed.worker - INFO -          dashboard at:      198.202.103.201:39387
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-z_195e4p
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:45057
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:45057
distributed.worker - INFO -          dashboard at:      198.202.103.201:42889
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8odt1xwi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:38927
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:38927
distributed.worker - INFO -          dashboard at:      198.202.103.201:35407
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rsl47yrg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:36581
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:36581
distributed.worker - INFO -          dashboard at:      198.202.103.201:45349
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sqs2srfk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:36649
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:36649
distributed.worker - INFO -          dashboard at:      198.202.103.201:32777
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mk1lhael
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:36253
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:36253
distributed.worker - INFO -          dashboard at:      198.202.103.201:42525
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xmom4_29
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:45959
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:45959
distributed.worker - INFO -          dashboard at:      198.202.103.201:41831
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1jd09o6b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:42709
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:42709
distributed.worker - INFO -          dashboard at:      198.202.103.201:37627
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1swhwza9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:38817
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:38817
distributed.worker - INFO -          dashboard at:      198.202.103.201:47031
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7cosbz6y
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:42447
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:42447
distributed.worker - INFO -          dashboard at:      198.202.103.201:39523
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8teyhfb6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:44621
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:44621
distributed.worker - INFO -          dashboard at:      198.202.103.201:37393
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fhyjn_h6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:39095
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:39095
distributed.worker - INFO -          dashboard at:      198.202.103.201:43667
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4qo_12yf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:33913
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:33913
distributed.worker - INFO -          dashboard at:      198.202.103.201:39147
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-62j29q9c
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:32829
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:32829
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:      198.202.103.201:34203
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hgcp2c7s
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:33807
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:33807
distributed.worker - INFO -          dashboard at:      198.202.103.201:34787
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mvnk3051
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:39583
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:39583
distributed.worker - INFO -          dashboard at:      198.202.103.201:36749
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p18uqt6o
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:35773
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:35773
distributed.worker - INFO -          dashboard at:      198.202.103.201:46603
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2mq64uli
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:44119
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:44119
distributed.worker - INFO -          dashboard at:      198.202.103.201:34501
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-eyhzpe5t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:40049
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:40049
distributed.worker - INFO -          dashboard at:      198.202.103.201:39385
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qiepwhpi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:39297
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:39297
distributed.worker - INFO -          dashboard at:      198.202.103.201:33229
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3i8r1tqi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:38903
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:38903
distributed.worker - INFO -          dashboard at:      198.202.103.201:38867
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tntq2qny
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:46397
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:46397
distributed.worker - INFO -          dashboard at:      198.202.103.201:42235
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gimctgnx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:35691
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:35691
distributed.worker - INFO -          dashboard at:      198.202.103.201:36945
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3utdf6in
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:33509
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:33509
distributed.worker - INFO -          dashboard at:      198.202.103.201:38765
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xcvcm56k
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:36379
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:36379
distributed.worker - INFO -          dashboard at:      198.202.103.201:37361
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7e1hi465
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:39569
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:39569
distributed.worker - INFO -          dashboard at:      198.202.103.201:34909
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jy_rpjso
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:32979
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:32979
distributed.worker - INFO -          dashboard at:      198.202.103.201:42761
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-nn87ino_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:45833
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:45833
distributed.worker - INFO -          dashboard at:      198.202.103.201:35017
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n7bugy3i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:41741
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:41741
distributed.worker - INFO -          dashboard at:      198.202.103.201:43985
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tfre5gc2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:34433
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:34433
distributed.worker - INFO -          dashboard at:      198.202.103.201:40965
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-51inpx22
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:35313
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:35313
distributed.worker - INFO -          dashboard at:      198.202.103.201:41221
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7upk1b8t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:34731
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:34731
distributed.worker - INFO -          dashboard at:      198.202.103.201:35349
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yy5wg6x5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:46569
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:46569
distributed.worker - INFO -          dashboard at:      198.202.103.201:41581
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ptsq5l1o
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:35011
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:35011
distributed.worker - INFO -          dashboard at:      198.202.103.201:39905
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5w312zoc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:33867
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:33867
distributed.worker - INFO -          dashboard at:      198.202.103.201:40923
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2e_g8dvj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:43793
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:43793
distributed.worker - INFO -          dashboard at:      198.202.103.201:34459
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4iws837j
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:45729
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:45729
distributed.worker - INFO -          dashboard at:      198.202.103.201:38647
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-twft2sr3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:35151
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:35151
distributed.worker - INFO -          dashboard at:      198.202.103.201:36851
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_q6k3qmm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:46121
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:46121
distributed.worker - INFO -          dashboard at:      198.202.103.201:37391
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-nerbsco7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:44145
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:44145
distributed.worker - INFO -          dashboard at:      198.202.103.201:44513
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yck17zuk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:38043
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:38043
distributed.worker - INFO -          dashboard at:      198.202.103.201:34285
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4trod8ud
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:37427
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:37427
distributed.worker - INFO -          dashboard at:      198.202.103.201:36743
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vc8hhm9z
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:41301
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:41301
distributed.worker - INFO -          dashboard at:      198.202.103.201:42841
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5vh5ss_3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:34261
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:34261
distributed.worker - INFO -          dashboard at:      198.202.103.201:41179
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:43281
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ei97nd7i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:43281
distributed.worker - INFO -          dashboard at:      198.202.103.201:36519
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-h7mwt_e5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:43913
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:43913
distributed.worker - INFO -          dashboard at:      198.202.103.201:46193
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uszjir3p
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:41717
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:41717
distributed.worker - INFO -          dashboard at:      198.202.103.201:46117
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ejca00zq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:37143
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:37143
distributed.worker - INFO -          dashboard at:      198.202.103.201:45909
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sqeptpet
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:46583
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:46583
distributed.worker - INFO -          dashboard at:      198.202.103.201:41343
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-i9a8nhv1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:35315
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:35315
distributed.worker - INFO -          dashboard at:      198.202.103.201:38367
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mrzvmjwu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:38811
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:38811
distributed.worker - INFO -          dashboard at:      198.202.103.201:46487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-txw5zeav
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:37395
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:37395
distributed.worker - INFO -          dashboard at:      198.202.103.201:42351
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pkibtrbk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:45773
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:45773
distributed.worker - INFO -          dashboard at:      198.202.103.201:38119
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xi6mpswm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:36963
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:36963
distributed.worker - INFO -          dashboard at:      198.202.103.201:43581
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kbhid7lr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:35273
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:35273
distributed.worker - INFO -          dashboard at:      198.202.103.201:44921
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-op8lz105
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:40473
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:40473
distributed.worker - INFO -          dashboard at:      198.202.103.201:39347
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-04z2stfy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:41589
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:41589
distributed.worker - INFO -          dashboard at:      198.202.103.201:37097
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u58nfb39
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:36495
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:36495
distributed.worker - INFO -          dashboard at:      198.202.103.201:39329
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-01sgb0_r
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:43177
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:43177
distributed.worker - INFO -          dashboard at:      198.202.103.201:33981
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kyx6ud89
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:45421
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:45421
distributed.worker - INFO -          dashboard at:      198.202.103.201:42975
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7kp9wmjf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:45323
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:45323
distributed.worker - INFO -          dashboard at:      198.202.103.201:34957
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bb8j03z5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:32925
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:32925
distributed.worker - INFO -          dashboard at:      198.202.103.201:36293
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tc2375p7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:40413
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:40413
distributed.worker - INFO -          dashboard at:      198.202.103.201:33517
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-br5bys7o
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:38801
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:38801
distributed.worker - INFO -          dashboard at:      198.202.103.201:45601
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p3jrns4q
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:46691
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:46691
distributed.worker - INFO -          dashboard at:      198.202.103.201:34337
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9q2wnmlb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:40531
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:40531
distributed.worker - INFO -          dashboard at:      198.202.103.201:45453
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vnspzwbk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:39293
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:39293
distributed.worker - INFO -          dashboard at:      198.202.103.201:40249
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ngqtsc3q
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:35477
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:35477
distributed.worker - INFO -          dashboard at:      198.202.103.201:43843
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u9u_nhyh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:39239
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:39239
distributed.worker - INFO -          dashboard at:      198.202.103.201:38411
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ds06prl4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:43445
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:43445
distributed.worker - INFO -          dashboard at:      198.202.103.201:38635
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uz5rkumt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:38683
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:38683
distributed.worker - INFO -          dashboard at:      198.202.103.201:43295
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-inx_uu1a
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:44235
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:44235
distributed.worker - INFO -          dashboard at:      198.202.103.201:36289
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jduaddww
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:45289
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:45289
distributed.worker - INFO -          dashboard at:      198.202.103.201:33511
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p5s0k9zy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.201:34741
distributed.worker - INFO -          Listening to: tcp://198.202.103.201:34741
distributed.worker - INFO -          dashboard at:      198.202.103.201:38313
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zztjgwdc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/iostream.py", line 867, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/iostream.py", line 1140, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 122, in convert_stream_closed_error
    raise CommClosedError(
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:36253
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:36649
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:36179
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:36961
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:42725
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:35239
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:36581
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:38309
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:43825
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:42001
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:40561
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:47049
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:36947
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:35833
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:35389
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:44557
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:44321
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:35807
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:41691
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:35451
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:43641
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:42727
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:39939
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:37017
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:45391
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:44999
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:38721
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:46801
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:42029
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:39667
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:45057
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:38927
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:38865'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:44549'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:45815'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:39535'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:39415'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:42723'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:45405'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:39051'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:34009'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:41375'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:43773'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:36207'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:46247'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:34439'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:45043'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:40103'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:41913'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:41195'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:46143'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:43307'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:39469'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:34095'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:34843'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:38221'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:44695'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:33715'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:33901'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:41471'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:41245'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:45783'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:33815'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:44673'
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:33129'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:33197'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:39239
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:35925'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:40281'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:38801
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:45673'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:36379
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:36869'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:45729
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:34595'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:33741'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:39583
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:41589
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:39593'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:35315
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:41283'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:41741
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:43951'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:32925
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:40099'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:46691
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:40005'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:35273
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:34749'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:36963
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:37143
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:44323'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:36071'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:45323
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:33653'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:35773
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:39333'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:41717
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:45417'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:46397
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:42125'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:43177
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:37079'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:46583
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:36333'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:45421
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:42799'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:43281
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:39145'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:35477
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:46539'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:38043
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:42821'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:42709
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:46577'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:39569
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:32775'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:40049
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:42033'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:32829
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:41303'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:35151
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:36455'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:34433
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:34261
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:34741
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:33137'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:34129'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:35509'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:44119
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:38409'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:44235
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:36551'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:45833
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:39093'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:38811
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:34177'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:45959
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:38311'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:43445
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:33509
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:41301
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:40343'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:37701'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:40625'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:33809'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:32979
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:42447
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:37433'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:38817
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:41361'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:37395
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:41253'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:40413
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:45185'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:46121
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:36881'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:36495
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:45747'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:46569
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:35985'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:39095
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:41479'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:38903
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:45289
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:33867
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:33751'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:42711'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:45779'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:39293
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:41643'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:37427
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:41597'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:33913
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:33093'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:43913
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:33633'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:35313
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:40717'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:34731
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:43649'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:39297
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:37425'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:33807
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:33345'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:44621
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:45987'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:40473
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:42159'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:43793
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:37125'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:35691
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:34759'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:45981'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:44145
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:35011
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.201:46283'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:40531
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:45773
distributed.worker - INFO - Stopping worker at tcp://198.202.103.201:38683
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - INFO - Worker process 67040 was killed by signal 15
distributed.nanny - INFO - Worker process 67052 was killed by signal 15
distributed.nanny - INFO - Worker process 67022 was killed by signal 15
distributed.nanny - INFO - Worker process 67014 was killed by signal 15
distributed.nanny - INFO - Worker process 67047 was killed by signal 15
distributed.nanny - INFO - Worker process 67070 was killed by signal 15
distributed.nanny - INFO - Worker process 67015 was killed by signal 15
distributed.nanny - INFO - Worker process 67034 was killed by signal 15
distributed.nanny - INFO - Worker process 67006 was killed by signal 15
distributed.nanny - INFO - Worker process 67030 was killed by signal 15
distributed.nanny - INFO - Worker process 67056 was killed by signal 15
distributed.nanny - INFO - Worker process 67075 was killed by signal 15
distributed.nanny - INFO - Worker process 67088 was killed by signal 15
distributed.nanny - INFO - Worker process 67079 was killed by signal 15
distributed.nanny - INFO - Worker process 67016 was killed by signal 15
distributed.nanny - INFO - Worker process 67010 was killed by signal 15
distributed.nanny - INFO - Worker process 67058 was killed by signal 15
distributed.nanny - INFO - Worker process 67091 was killed by signal 15
distributed.nanny - INFO - Worker process 67095 was killed by signal 15
distributed.nanny - INFO - Worker process 67074 was killed by signal 15
distributed.nanny - INFO - Worker process 67085 was killed by signal 15
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - INFO - Worker process 67146 was killed by signal 15
distributed.nanny - INFO - Worker process 67110 was killed by signal 15
distributed.nanny - INFO - Worker process 67094 was killed by signal 15
distributed.nanny - INFO - Worker process 67106 was killed by signal 15
distributed.nanny - INFO - Worker process 67115 was killed by signal 15
distributed.nanny - INFO - Worker process 67149 was killed by signal 15
distributed.nanny - INFO - Worker process 67133 was killed by signal 15
distributed.nanny - INFO - Worker process 67101 was killed by signal 15
distributed.nanny - INFO - Worker process 67157 was killed by signal 15
distributed.nanny - INFO - Worker process 67118 was killed by signal 15
distributed.nanny - INFO - Worker process 67128 was killed by signal 15
distributed.nanny - INFO - Worker process 67124 was killed by signal 15
distributed.nanny - INFO - Worker process 67140 was killed by signal 15
distributed.nanny - INFO - Worker process 67111 was killed by signal 15
distributed.nanny - INFO - Worker process 67160 was killed by signal 15
distributed.nanny - INFO - Worker process 67164 was killed by signal 15
distributed.nanny - INFO - Worker process 67159 was killed by signal 15
distributed.nanny - INFO - Worker process 67165 was killed by signal 15
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - INFO - Worker process 67181 was killed by signal 15
distributed.nanny - INFO - Worker process 67205 was killed by signal 15
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67294 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67292 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67288 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67286 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67284 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67276 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67274 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67257 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67264 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67245 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67252 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67240 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67230 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67232 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67224 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67220 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67210 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67208 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67214 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67189 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67188 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67186 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67176 parent=66927 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67179 parent=66927 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
