distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:44179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:44955'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:38139'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:38639'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:40891'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:38281'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:43167'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:44389'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:38005'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:33469'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:39323'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:35061'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:36287'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:45457'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:45299'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:35541'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:34377'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:36469'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:39883'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:36823'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:35975'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:46673'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:34281'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:42081'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:42665'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:46455'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:39327'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:36571'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:33809'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:35563'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:44957'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:35035'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:34937'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:33765'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:46493'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:33417'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:40935'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:39871'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:37159'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:45965'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:44069'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:36589'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:39169'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:42881'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:41087'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:42629'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:38401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:40331'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:45675'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:42351'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:39453'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:34521'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:33291'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:38161'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:37715'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:33605'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:45395'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:45679'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:41631'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:44103'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:43347'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:45991'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:34721'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:46945'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:33963'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:33753'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:40055'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:37193'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:33739'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:46183'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:39339'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:38233'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:42781'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:34687'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:43657'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:41109'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:43907'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:41895'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:40313'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:38689'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:42743'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:32845'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:37251'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:41159'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:37401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:38395'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:35705'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:39589'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:35323'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:46149'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:42301'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:33345'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:44013'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:39105'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:34121'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:41855'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:46617'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:35301'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:39187'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.142:45911'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:37577
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:37577
distributed.worker - INFO -          dashboard at:      198.202.101.142:39101
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-h5mygz92
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:44377
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:44377
distributed.worker - INFO -          dashboard at:      198.202.101.142:42455
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4d2iqlnj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:43195
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:43195
distributed.worker - INFO -          dashboard at:      198.202.101.142:36771
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jz9idpgv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:44861
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:44861
distributed.worker - INFO -          dashboard at:      198.202.101.142:35479
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8c4kuxbi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:41997
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:41997
distributed.worker - INFO -          dashboard at:      198.202.101.142:42397
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-x9nnf0ts
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:45821
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:45821
distributed.worker - INFO -          dashboard at:      198.202.101.142:36855
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-egnhpdgx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:44617
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:44617
distributed.worker - INFO -          dashboard at:      198.202.101.142:37947
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5sfozmsk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:47083
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:47083
distributed.worker - INFO -          dashboard at:      198.202.101.142:34611
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sownw2gr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:45557
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:45557
distributed.worker - INFO -          dashboard at:      198.202.101.142:38357
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7r43m4n8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:40881
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:40881
distributed.worker - INFO -          dashboard at:      198.202.101.142:39849
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-392bvda8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:33103
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:33103
distributed.worker - INFO -          dashboard at:      198.202.101.142:46727
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qmgiarj2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:45193
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:45193
distributed.worker - INFO -          dashboard at:      198.202.101.142:33707
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ty3s1nmw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:41435
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:41435
distributed.worker - INFO -          dashboard at:      198.202.101.142:35535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lcfazd2z
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:46335
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:46335
distributed.worker - INFO -          dashboard at:      198.202.101.142:43977
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lfxweefh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5da00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.128:39535 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.128:39535 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:40281
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:40281
distributed.worker - INFO -          dashboard at:      198.202.101.142:33433
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1wy87_k4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:33443
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:33443
distributed.worker - INFO -          dashboard at:      198.202.101.142:45393
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gvvdy2k6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:45271
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:45271
distributed.worker - INFO -          dashboard at:      198.202.101.142:45037
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0oht5at9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:41551
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:41551
distributed.worker - INFO -          dashboard at:      198.202.101.142:44795
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ksk7l070
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:34507
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:34507
distributed.worker - INFO -          dashboard at:      198.202.101.142:34897
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-f6o1e0hq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:38127
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:38127
distributed.worker - INFO -          dashboard at:      198.202.101.142:35867
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uhroftct
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:46453
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:46453
distributed.worker - INFO -          dashboard at:      198.202.101.142:36557
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n_m73lh1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:39933
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:39933
distributed.worker - INFO -          dashboard at:      198.202.101.142:34079
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-v1xmuh9a
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:34533
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:34533
distributed.worker - INFO -          dashboard at:      198.202.101.142:37749
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-g9mq3tdy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:46109
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:46109
distributed.worker - INFO -          dashboard at:      198.202.101.142:39289
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-a9sal28f
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:37347
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:37347
distributed.worker - INFO -          dashboard at:      198.202.101.142:35929
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fz79rldy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:46581
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:46581
distributed.worker - INFO -          dashboard at:      198.202.101.142:46137
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-v3rz57u1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:34089
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:34089
distributed.worker - INFO -          dashboard at:      198.202.101.142:46555
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-aah42vwy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:36907
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:36907
distributed.worker - INFO -          dashboard at:      198.202.101.142:44099
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hho1eqrf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:39771
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:39771
distributed.worker - INFO -          dashboard at:      198.202.101.142:43389
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1lts2nbn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:46743
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:46743
distributed.worker - INFO -          dashboard at:      198.202.101.142:40975
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-evwb7d5u
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:33199
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:33199
distributed.worker - INFO -          dashboard at:      198.202.101.142:37465
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-a9kxqozj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:39695
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:39695
distributed.worker - INFO -          dashboard at:      198.202.101.142:35217
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-anw0k561
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:40259
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:40259
distributed.worker - INFO -          dashboard at:      198.202.101.142:45959
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mnqonni1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:44981
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:44981
distributed.worker - INFO -          dashboard at:      198.202.101.142:46409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-b15kdr_v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:37341
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:37341
distributed.worker - INFO -          dashboard at:      198.202.101.142:45209
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-a8lwe2m6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:43419
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:43419
distributed.worker - INFO -          dashboard at:      198.202.101.142:36463
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-i05pw0tk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:36617
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:36617
distributed.worker - INFO -          dashboard at:      198.202.101.142:33267
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-reowh_2m
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:44819
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:44819
distributed.worker - INFO -          dashboard at:      198.202.101.142:44997
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-q_t9ki1z
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:43341
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:43341
distributed.worker - INFO -          dashboard at:      198.202.101.142:46287
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-44cpazmy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:32893
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:32893
distributed.worker - INFO -          dashboard at:      198.202.101.142:40703
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-v5phdh4l
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:39835
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:39835
distributed.worker - INFO -          dashboard at:      198.202.101.142:46179
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-nw_x5yro
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:38019
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:38019
distributed.worker - INFO -          dashboard at:      198.202.101.142:37807
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bgc23sub
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:44149
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:44149
distributed.worker - INFO -          dashboard at:      198.202.101.142:44041
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8sd708z4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:34123
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:34123
distributed.worker - INFO -          dashboard at:      198.202.101.142:44475
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-az0xw269
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:41459
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:41459
distributed.worker - INFO -          dashboard at:      198.202.101.142:42181
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-nnwg2znv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:39559
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:39559
distributed.worker - INFO -          dashboard at:      198.202.101.142:35889
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kqeyc3hr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:36883
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:36883
distributed.worker - INFO -          dashboard at:      198.202.101.142:35629
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uljkkkzg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:35325
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:35325
distributed.worker - INFO -          dashboard at:      198.202.101.142:38181
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-av9xpb6e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:45323
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:45323
distributed.worker - INFO -          dashboard at:      198.202.101.142:39781
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vagnpmqz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:34339
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:34339
distributed.worker - INFO -          dashboard at:      198.202.101.142:33201
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w4wj543a
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:41225
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:41225
distributed.worker - INFO -          dashboard at:      198.202.101.142:46161
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-56g2whi6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:35443
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:35443
distributed.worker - INFO -          dashboard at:      198.202.101.142:43133
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-a2i11c61
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:34945
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:34945
distributed.worker - INFO -          dashboard at:      198.202.101.142:33867
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-o5z4v5kj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:34157
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:34157
distributed.worker - INFO -          dashboard at:      198.202.101.142:36933
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pcj4p8xt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:43693
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:43693
distributed.worker - INFO -          dashboard at:      198.202.101.142:36389
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xe0rwnpm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:41245
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:41245
distributed.worker - INFO -          dashboard at:      198.202.101.142:37473
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-132yzucl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:34167
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:34167
distributed.worker - INFO -          dashboard at:      198.202.101.142:36547
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6sq5368h
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:46213
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:46213
distributed.worker - INFO -          dashboard at:      198.202.101.142:42079
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9uikl08x
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:34559
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:34559
distributed.worker - INFO -          dashboard at:      198.202.101.142:42899
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8ev8bsze
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:36733
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:36733
distributed.worker - INFO -          dashboard at:      198.202.101.142:41725
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-a7zwzgft
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:39385
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:39385
distributed.worker - INFO -          dashboard at:      198.202.101.142:45381
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-z6pcbdz8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:42055
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:42055
distributed.worker - INFO -          dashboard at:      198.202.101.142:45225
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-aih6p2mw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:40793
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:40793
distributed.worker - INFO -          dashboard at:      198.202.101.142:38935
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-r71dk3jc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:37729
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:37729
distributed.worker - INFO -          dashboard at:      198.202.101.142:43365
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pvf0qhmr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:40429
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:40429
distributed.worker - INFO -          dashboard at:      198.202.101.142:44417
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-itxjs7sd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:37833
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:37833
distributed.worker - INFO -          dashboard at:      198.202.101.142:38691
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5ak7qlsg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:35233
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:35233
distributed.worker - INFO -          dashboard at:      198.202.101.142:43461
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9k8wqea_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:38335
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:38335
distributed.worker - INFO -          dashboard at:      198.202.101.142:41233
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2n33v1vf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:45607
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:45607
distributed.worker - INFO -          dashboard at:      198.202.101.142:42609
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ia2rd3wm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:42149
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:42149
distributed.worker - INFO -          dashboard at:      198.202.101.142:44349
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7da6y_t4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:33287
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:33287
distributed.worker - INFO -          dashboard at:      198.202.101.142:40157
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mukqdtlj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:37455
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:37455
distributed.worker - INFO -          dashboard at:      198.202.101.142:45833
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2pr7xxfi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:33985
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:33985
distributed.worker - INFO -          dashboard at:      198.202.101.142:36061
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8nfoqew1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:42833
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:42833
distributed.worker - INFO -          dashboard at:      198.202.101.142:44021
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7ya0vf0c
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:42425
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:42425
distributed.worker - INFO -          dashboard at:      198.202.101.142:41983
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-m9exuqi4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:35751
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:35751
distributed.worker - INFO -          dashboard at:      198.202.101.142:34633
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tvxk4_aa
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:42865
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:42865
distributed.worker - INFO -          dashboard at:      198.202.101.142:41861
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-o0gn_jqq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:33927
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:33927
distributed.worker - INFO -          dashboard at:      198.202.101.142:37545
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-arc4i2ym
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:39059
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:39059
distributed.worker - INFO -          dashboard at:      198.202.101.142:34541
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_3oqv3xh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:38693
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:38693
distributed.worker - INFO -          dashboard at:      198.202.101.142:43275
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vcfqutgl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:36479
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:36479
distributed.worker - INFO -          dashboard at:      198.202.101.142:46863
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rpzzkkq8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:37577
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:33451
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:33451
distributed.worker - INFO -          dashboard at:      198.202.101.142:33255
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gx_vzivr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:46915
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:46915
distributed.worker - INFO -          dashboard at:      198.202.101.142:42109
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_sk1nxgn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:46295
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:46295
distributed.worker - INFO -          dashboard at:      198.202.101.142:42793
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jw7rz6bj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:41313
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:41313
distributed.worker - INFO -          dashboard at:      198.202.101.142:41373
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jw40jbv8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:41471
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:41471
distributed.worker - INFO -          dashboard at:      198.202.101.142:45925
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zeufsv0i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:34869
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:34869
distributed.worker - INFO -          dashboard at:      198.202.101.142:35561
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qn6rqnl_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:35287
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:35287
distributed.worker - INFO -          dashboard at:      198.202.101.142:42169
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-x9spq411
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:37099
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:37099
distributed.worker - INFO -          dashboard at:      198.202.101.142:41763
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3xtz4y9e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:37505
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:37505
distributed.worker - INFO -          dashboard at:      198.202.101.142:45619
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cxbjkjz2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:37905
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:37905
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO -          dashboard at:      198.202.101.142:44813
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-10ska2cf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:46255
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:46255
distributed.worker - INFO -          dashboard at:      198.202.101.142:40121
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-t96syvqp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:46021
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:46021
distributed.worker - INFO -          dashboard at:      198.202.101.142:44605
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yegrgn51
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:38633
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:38633
distributed.worker - INFO -          dashboard at:      198.202.101.142:33553
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fwtnsjqt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:47097
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:47097
distributed.worker - INFO -          dashboard at:      198.202.101.142:34017
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zias29ap
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:34987
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:34987
distributed.worker - INFO -          dashboard at:      198.202.101.142:42937
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8jqu96dc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:34343
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:34343
distributed.worker - INFO -          dashboard at:      198.202.101.142:34695
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3f707ofk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:35543
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:35543
distributed.worker - INFO -          dashboard at:      198.202.101.142:41511
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5i4m5ga0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:45141
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:45141
distributed.worker - INFO -          dashboard at:      198.202.101.142:35359
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8es5felw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.142:38075
distributed.worker - INFO -          Listening to: tcp://198.202.101.142:38075
distributed.worker - INFO -          dashboard at:      198.202.101.142:40073
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5zg3i1rj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:44389'
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:44179'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:44955'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:33443
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:38139'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:38639'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:37347
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:47083
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:40891'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:38281'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:38127
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:43167'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:40281
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:45271
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:38005'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:44377
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:43195
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:33469'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:39323'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:35061'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:45821
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:36287'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:33103
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:45457'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:44861
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:45299'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:41997
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:35541'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:46581
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:34377'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:41551
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:36469'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:41435
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:39883'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:44617
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:36823'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:46453
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:35975'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:40881
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:46673'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:36907
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:34281'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:34507
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:45193
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:42081'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:39771
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:42665'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:46455'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:39933
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:39327'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:34533
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:36571'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:45557
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:33809'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:34089
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:46109
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:39695
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:35563'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:44957'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:35035'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:34937'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:33199
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:46743
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:33765'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:36617
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:46493'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:44981
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:33417'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:43419
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:40935'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:39871'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:37341
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:44819
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:37159'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:40259
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:45965'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:38019
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:44069'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:43341
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:36589'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:34123
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:39169'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:39559
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:42881'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:32893
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:41087'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:39835
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:42629'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:35325
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:38401'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:44149
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:40331'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:41459
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:45675'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:36883
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:42351'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:45323
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:39453'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:34339
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:34521'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:33291'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:35443
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:41225
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:38161'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:34945
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:37715'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:34167
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:33605'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:41245
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:43693
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:45395'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:45679'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:46213
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:41631'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:34157
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:44103'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:34559
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:43347'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:33287
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:40793
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:37455
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:45991'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:34721'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:46945'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:36733
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:33963'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:39385
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:33753'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:37833
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:42055
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:40055'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:37193'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:38335
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:33739'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:45607
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:42833
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:46183'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:39339'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:35233
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:38233'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:42149
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:42781'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:42865
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:34687'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:41313
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:43657'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:33927
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:41109'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:36479
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:43907'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:35751
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:41895'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:38693
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:40313'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:37099
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:38689'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:34869
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:42743'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:35287
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:32845'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:33451
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:41471
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:37251'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:46295
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:41159'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:37401'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:46915
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:38395'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:35705'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:46021
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:37729
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:39589'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:40429
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:37905
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:35323'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:46149'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:33985
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:42301'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:38633
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:33345'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:46255
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:44013'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:37505
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:39105'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:34121'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:34343
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:34987
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:41855'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:42425
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:46617'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:47097
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:35301'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:35543
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:39187'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:39059
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:46335
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:38075
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.142:45911'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.142:45141
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130577 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130579 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130573 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130571 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130575 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130567 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130569 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130565 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130563 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130557 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130561 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130554 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130550 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130548 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130544 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130541 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130537 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130534 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130531 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130526 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130529 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130523 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130520 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130518 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130513 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130508 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130510 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130502 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130499 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130494 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130492 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130489 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130497 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130483 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130485 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130475 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130478 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130471 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130464 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130465 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130461 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130466 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130454 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130446 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130449 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130445 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130442 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130447 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130433 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130435 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130431 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130425 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130428 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130423 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130421 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130416 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130413 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130409 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130407 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130404 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130402 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130398 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130397 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130390 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130388 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130384 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130382 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130378 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130376 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130373 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130371 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130368 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130365 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130361 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130358 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130354 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130352 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130348 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130346 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130342 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130339 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130336 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130335 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130331 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130328 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130325 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130323 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130317 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130319 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130314 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130309 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130307 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130290 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130301 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130298 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130293 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130292 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130294 parent=130195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=130288 parent=130195 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    assert exitcode is not None
AssertionError
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
