distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:33535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38175'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:46057'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:39841'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:43373'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:40337'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:43439'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:37925'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:36457'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:45715'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:33209'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:43045'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:40483'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:36919'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:34795'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:34873'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38239'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38821'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:39535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:33257'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:32923'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:41305'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:34571'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:44311'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38165'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38639'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:37063'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:45245'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:44791'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:39017'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:33893'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:42729'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:46427'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:46143'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:39857'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:42663'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:39459'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:35653'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:37505'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:35443'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:41729'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38881'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:33565'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:43627'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:45869'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:40247'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38083'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:34691'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:41203'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:34405'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38435'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:45375'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:39747'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:43785'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:34935'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38909'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:36303'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38983'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:39671'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:35127'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:46293'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:43601'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38023'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:32961'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:39925'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:40491'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:43891'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:46175'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:36591'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:44903'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38549'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:39911'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38581'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:40949'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:46839'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:45589'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:41477'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:33709'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:36199'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:33227'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:42321'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38073'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:36237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:45529'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:41229'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:36309'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:37137'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:43073'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:34749'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:33583'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:43523'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:37879'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:42795'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:34995'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:40437'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:36581'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:46613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:33147'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.243:38865'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:42087
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:42087
distributed.worker - INFO -          dashboard at:      198.202.102.243:37593
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-c5n9wq9f
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:42631
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:42631
distributed.worker - INFO -          dashboard at:      198.202.102.243:36605
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1vwjj1sa
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:39307
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:39307
distributed.worker - INFO -          dashboard at:      198.202.102.243:38927
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-09cm3vbp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:41873
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:41873
distributed.worker - INFO -          dashboard at:      198.202.102.243:40793
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-eb3sbly7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:37341
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:37341
distributed.worker - INFO -          dashboard at:      198.202.102.243:33905
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ckob_cws
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:36969
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:36969
distributed.worker - INFO -          dashboard at:      198.202.102.243:36911
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-byel0flo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:47093
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:47093
distributed.worker - INFO -          dashboard at:      198.202.102.243:39451
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_rezkf85
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:36981
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:36981
distributed.worker - INFO -          dashboard at:      198.202.102.243:46653
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ry85p0a6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:34279
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:34279
distributed.worker - INFO -          dashboard at:      198.202.102.243:35863
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3s3ar6jv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:39073
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:39073
distributed.worker - INFO -          dashboard at:      198.202.102.243:45499
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hzabcay9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:43663
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:43663
distributed.worker - INFO -          dashboard at:      198.202.102.243:38525
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-j90q3m0u
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:41279
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:41279
distributed.worker - INFO -          dashboard at:      198.202.102.243:34209
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7wqflzxc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:35275
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:35275
distributed.worker - INFO -          dashboard at:      198.202.102.243:36493
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-v9hsds97
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:46241
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:46241
distributed.worker - INFO -          dashboard at:      198.202.102.243:35159
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-aovddidb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:33833
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:33833
distributed.worker - INFO -          dashboard at:      198.202.102.243:35953
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ohq6yf3n
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:42583
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:42583
distributed.worker - INFO -          dashboard at:      198.202.102.243:45733
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mn1331ot
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:33221
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:33221
distributed.worker - INFO -          dashboard at:      198.202.102.243:35655
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-j61cdmth
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:42333
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:42333
distributed.worker - INFO -          dashboard at:      198.202.102.243:45557
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-afp8hp71
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:43743
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:43743
distributed.worker - INFO -          dashboard at:      198.202.102.243:45659
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-unmixl5c
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:33255
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:33255
distributed.worker - INFO -          dashboard at:      198.202.102.243:35281
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-x7ztbw6d
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:39795
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:39795
distributed.worker - INFO -          dashboard at:      198.202.102.243:36623
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-iuabp3s0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:38703
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:38703
distributed.worker - INFO -          dashboard at:      198.202.102.243:35479
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jpiu0frh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:44843
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:44843
distributed.worker - INFO -          dashboard at:      198.202.102.243:42785
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6xwad_l2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:41273
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:41273
distributed.worker - INFO -          dashboard at:      198.202.102.243:44233
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ofcp_0n2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:39645
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:39645
distributed.worker - INFO -          dashboard at:      198.202.102.243:41745
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fgel7d9l
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:33925
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:33925
distributed.worker - INFO -          dashboard at:      198.202.102.243:38887
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n25okvnj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:40089
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:40089
distributed.worker - INFO -          dashboard at:      198.202.102.243:39881
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bzbs7gpg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:38623
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:38623
distributed.worker - INFO -          dashboard at:      198.202.102.243:41969
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vne65xdc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:33359
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:33359
distributed.worker - INFO -          dashboard at:      198.202.102.243:35173
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tfca91v8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:42969
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:42969
distributed.worker - INFO -          dashboard at:      198.202.102.243:34487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-znuql8rx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:38511
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:38511
distributed.worker - INFO -          dashboard at:      198.202.102.243:42681
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-smbzw6dn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:37399
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:37399
distributed.worker - INFO -          dashboard at:      198.202.102.243:46773
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gmvl2wpa
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:38791
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:38791
distributed.worker - INFO -          dashboard at:      198.202.102.243:39125
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7cmbq3a8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:44579
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:44579
distributed.worker - INFO -          dashboard at:      198.202.102.243:37339
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-76_x55bd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:36535
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:36535
distributed.worker - INFO -          dashboard at:      198.202.102.243:41043
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mjd2tkfp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:35027
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:35027
distributed.worker - INFO -          dashboard at:      198.202.102.243:35019
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-o05dkh50
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:39153
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:39153
distributed.worker - INFO -          dashboard at:      198.202.102.243:34495
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5b2clomu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:39859
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:39859
distributed.worker - INFO -          dashboard at:      198.202.102.243:39057
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-o2k4ivbb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:36047
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:36047
distributed.worker - INFO -          dashboard at:      198.202.102.243:35291
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-46gjiua0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:41271
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:41271
distributed.worker - INFO -          dashboard at:      198.202.102.243:37829
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-98a4beag
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:38191
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:38191
distributed.worker - INFO -          dashboard at:      198.202.102.243:33673
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vjv1hxhq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:33501
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:33501
distributed.worker - INFO -          dashboard at:      198.202.102.243:41789
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-f10phj16
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:39541
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:39541
distributed.worker - INFO -          dashboard at:      198.202.102.243:41069
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8baagtmd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:46547
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:46547
distributed.worker - INFO -          dashboard at:      198.202.102.243:34111
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6jwoprar
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:40005
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:40005
distributed.worker - INFO -          dashboard at:      198.202.102.243:33225
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rz6jinvm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:45395
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:45395
distributed.worker - INFO -          dashboard at:      198.202.102.243:36889
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-do1d98a7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:33041
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:33041
distributed.worker - INFO -          dashboard at:      198.202.102.243:35359
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0_qq7bst
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:35261
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:35261
distributed.worker - INFO -          dashboard at:      198.202.102.243:43145
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w7isaxfg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:39453
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:39453
distributed.worker - INFO -          dashboard at:      198.202.102.243:44203
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-029fi4px
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:37589
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:37589
distributed.worker - INFO -          dashboard at:      198.202.102.243:35923
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-erzbq53k
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:43205
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:43205
distributed.worker - INFO -          dashboard at:      198.202.102.243:42249
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pl5qb5_v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:38137
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:38137
distributed.worker - INFO -          dashboard at:      198.202.102.243:32813
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-x41n9egd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:35905
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:35905
distributed.worker - INFO -          dashboard at:      198.202.102.243:34069
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ewn6cwum
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:41177
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:41177
distributed.worker - INFO -          dashboard at:      198.202.102.243:36095
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uxf7styo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:38051
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:38051
distributed.worker - INFO -          dashboard at:      198.202.102.243:46629
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-az3qdco4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:45243
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:45243
distributed.worker - INFO -          dashboard at:      198.202.102.243:40235
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-of0l4rnq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:45101
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:45101
distributed.worker - INFO -          dashboard at:      198.202.102.243:44873
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rde_zjaq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:37015
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:37015
distributed.worker - INFO -          dashboard at:      198.202.102.243:34563
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-b09922tw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:41331
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:41331
distributed.worker - INFO -          dashboard at:      198.202.102.243:44297
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1w36e5nz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:40583
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:40583
distributed.worker - INFO -          dashboard at:      198.202.102.243:42069
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-na897bbl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:33083
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:33083
distributed.worker - INFO -          dashboard at:      198.202.102.243:38655
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3zkl0pva
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:34681
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:34681
distributed.worker - INFO -          dashboard at:      198.202.102.243:36277
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w6ogeynr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:43753
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:43753
distributed.worker - INFO -          dashboard at:      198.202.102.243:40135
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ps2pn4mn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:37163
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:37163
distributed.worker - INFO -          dashboard at:      198.202.102.243:46979
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vo91l5o2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:40465
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:40465
distributed.worker - INFO -          dashboard at:      198.202.102.243:42189
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-f6zzu0tc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:34981
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:34981
distributed.worker - INFO -          dashboard at:      198.202.102.243:46133
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uobjrl_h
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:47013
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:47013
distributed.worker - INFO -          dashboard at:      198.202.102.243:35363
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-oqs1kat1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:42549
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:42549
distributed.worker - INFO -          dashboard at:      198.202.102.243:37337
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wiwcmet8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:40017
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:40017
distributed.worker - INFO -          dashboard at:      198.202.102.243:35353
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-f9vy9h0t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:38929
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:38929
distributed.worker - INFO -          dashboard at:      198.202.102.243:40169
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sjf0qa3m
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:38069
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:38069
distributed.worker - INFO -          dashboard at:      198.202.102.243:41661
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4u7krm_d
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:37927
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:37927
distributed.worker - INFO -          dashboard at:      198.202.102.243:40197
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0iimd5qc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:36791
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:36791
distributed.worker - INFO -          dashboard at:      198.202.102.243:38535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rgpm73fg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:42929
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:42929
distributed.worker - INFO -          dashboard at:      198.202.102.243:36073
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-h3nhjl6w
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:35301
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:35301
distributed.worker - INFO -          dashboard at:      198.202.102.243:34551
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2_uzk95k
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:35095
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:35095
distributed.worker - INFO -          dashboard at:      198.202.102.243:33455
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n8_k5mpm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:42103
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:42103
distributed.worker - INFO -          dashboard at:      198.202.102.243:45565
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_2s6jvmc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:38619
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:38619
distributed.worker - INFO -          dashboard at:      198.202.102.243:40853
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jrwmp_7_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:39569
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:39569
distributed.worker - INFO -          dashboard at:      198.202.102.243:36459
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-10ghqekp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:32969
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:32969
distributed.worker - INFO -          dashboard at:      198.202.102.243:35565
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-__uudczw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:42079
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:42079
distributed.worker - INFO -          dashboard at:      198.202.102.243:35967
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u55hgxmp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:36923
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:36923
distributed.worker - INFO -          dashboard at:      198.202.102.243:38473
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pye0momg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:44459
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:44459
distributed.worker - INFO -          dashboard at:      198.202.102.243:41673
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-y86n1bwo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:33449
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:33449
distributed.worker - INFO -          dashboard at:      198.202.102.243:40371
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0_f1qlnw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:42477
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:42477
distributed.worker - INFO -          dashboard at:      198.202.102.243:40173
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-teklbdsj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:33059
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:33059
distributed.worker - INFO -          dashboard at:      198.202.102.243:44787
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w55vpcj2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:41315
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:41315
distributed.worker - INFO -          dashboard at:      198.202.102.243:44403
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vkj3cwm6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:45439
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:45439
distributed.worker - INFO -          dashboard at:      198.202.102.243:40051
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-t_rpges5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:36123
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:36123
distributed.worker - INFO -          dashboard at:      198.202.102.243:35081
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-18swsx22
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:39173
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:39173
distributed.worker - INFO -          dashboard at:      198.202.102.243:43883
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-g_km3my3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:37419
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:37419
distributed.worker - INFO -          dashboard at:      198.202.102.243:38787
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7ydkq4yz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:43805
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:43805
distributed.worker - INFO -          dashboard at:      198.202.102.243:36161
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vus57cbv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:40091
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:40091
distributed.worker - INFO -          dashboard at:      198.202.102.243:35293
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3m34fcgw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:42911
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:42911
distributed.worker - INFO -          dashboard at:      198.202.102.243:43241
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5u0sxo62
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:38819
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:38819
distributed.worker - INFO -          dashboard at:      198.202.102.243:44845
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-e3z8vuo4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:40171
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:40171
distributed.worker - INFO -          dashboard at:      198.202.102.243:42875
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tqym80dp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:36065
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:36065
distributed.worker - INFO -          dashboard at:      198.202.102.243:43025
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-e37_o9op
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:44801
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:44801
distributed.worker - INFO -          dashboard at:      198.202.102.243:33279
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0hnipl4v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:45845
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:45845
distributed.worker - INFO -          dashboard at:      198.202.102.243:36157
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ahoag30c
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.243:46831
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.102.243:46831
distributed.worker - INFO -          dashboard at:      198.202.102.243:35429
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1zg7e7w6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:39535
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:33535'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38175'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:46057'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:44843
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:39841'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:40089
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:43373'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:43743
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:40337'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:42333
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:43439'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:39645
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:37925'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:42631
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:36457'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:36981
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:45715'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:43663
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:33209'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:39307
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:43045'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:33925
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:40483'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:33833
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:36919'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:39073
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:36535
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:34795'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:41873
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:34873'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38239'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:37341
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:39795
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38821'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:39535'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:46241
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:33257'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:33221
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:32923'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:42583
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:41305'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:34571'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:41273
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:38703
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:44311'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:47093
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38165'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:33255
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38639'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:35275
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:37063'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:42087
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:45245'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:34279
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:44791'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:38623
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:39017'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:33359
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:33893'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:42969
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:42729'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:35027
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:38791
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:46427'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:46143'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:37399
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:39857'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:38511
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:42663'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:44579
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:39459'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:39153
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:35653'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:39859
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:37505'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:36047
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:35443'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:41271
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:41729'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:33501
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38881'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:39541
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:33565'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:38191
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:43627'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:43205
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:45869'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:40247'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:46547
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:40005
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:33041
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38083'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:34691'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:45395
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:41203'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:41177
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:35261
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:34405'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38435'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:39453
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:45375'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:39747'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:37589
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:43785'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:38051
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:38137
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:34935'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:45243
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38909'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:35905
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:36303'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:34681
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38983'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:33083
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:39671'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:45101
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:35127'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:41331
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:46293'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:40583
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38317'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:47013
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:43753
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:34981
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:43601'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38023'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:32961'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:40017
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:39925'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:40491'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:38929
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:38069
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:43891'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:37163
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:46175'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:36591'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:42549
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:40465
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:42103
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:44903'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38549'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:37927
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:39911'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:36791
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38581'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:35095
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:40949'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:44459
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:46839'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:35301
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:45589'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:42929
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:41477'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:36923
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:33709'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:42477
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:36199'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:42079
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:33227'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:33059
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:42321'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:33449
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38073'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:39569
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:36237'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:32969
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:45529'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:37015
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:41229'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:45439
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:36309'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:37137'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:38619
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:41315
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:36123
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:43073'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:34749'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:36969
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:33583'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:43523'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:39173
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:37419
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:37879'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:43805
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:42795'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:38819
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:34995'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:41279
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:40437'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:40091
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:36581'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:40171
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:46613'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:42911
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:33147'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:36065
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.243:38865'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:44801
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:45845
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.243:46831
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57518 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57516 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57513 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57512 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57508 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57506 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57502 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57510 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57504 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57500 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57497 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57491 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57493 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57487 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57479 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57482 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57477 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57473 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57462 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57468 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57471 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57466 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57464 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57455 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57450 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57444 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57447 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57452 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57440 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57434 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57432 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57437 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57428 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57419 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57418 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57421 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57414 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57412 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57409 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57403 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57406 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57396 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57397 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57389 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57391 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57386 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57377 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57375 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57366 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57363 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57360 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57352 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57349 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57346 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57342 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57337 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57339 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57333 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57330 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57327 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57323 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57321 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57316 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57317 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57313 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57308 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57305 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57303 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57299 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57295 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57297 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57291 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57285 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57283 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57278 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57276 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57270 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57264 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57252 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57248 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57245 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57242 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57234 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57232 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57229 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57227 parent=57140 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57225 parent=57140 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
