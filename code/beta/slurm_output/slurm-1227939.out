distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:34013'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:41525'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:35277'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:40837'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:45587'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:36999'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:38481'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:44527'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:33787'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:37361'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:46687'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:45307'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:34619'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:35969'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:34773'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:39563'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:37151'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:40053'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:45317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:36157'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:41255'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:45709'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:45501'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:41929'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:38645'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:45031'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:39215'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:38325'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:43055'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:38155'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:36449'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:41687'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:45885'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:42297'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:41443'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:46695'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:43553'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:44621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:34303'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:36839'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:36299'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:33979'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:36131'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:34113'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:44059'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:38387'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:38001'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:45387'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:36877'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:46763'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:38891'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:38985'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:44759'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:45019'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:36965'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:37813'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:44203'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:38213'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:33053'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:34029'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:33089'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:46233'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:45195'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:46177'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:34035'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:36807'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:35501'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:39647'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:40627'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:43809'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:42755'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:35061'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:40363'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:44909'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:45873'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:34727'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:42471'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:43895'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:46611'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:41917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:33365'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:34545'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:46535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:37259'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:46977'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:39383'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:37383'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:37187'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:37547'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:42293'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:42745'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:35945'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:44687'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:39791'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:38191'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:39435'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:39897'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:40403'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:38683'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.245:40477'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:45963
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:45963
distributed.worker - INFO -          dashboard at:      198.202.102.245:46975
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-v7q8otuk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:45125
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:45125
distributed.worker - INFO -          dashboard at:      198.202.102.245:36271
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u807r0qd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:46259
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:46259
distributed.worker - INFO -          dashboard at:      198.202.102.245:45639
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zusogzwd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:39425
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:39425
distributed.worker - INFO -          dashboard at:      198.202.102.245:46245
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-d_szz5_g
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:35773
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:35773
distributed.worker - INFO -          dashboard at:      198.202.102.245:36881
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gd6xc8ko
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:37397
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:37397
distributed.worker - INFO -          dashboard at:      198.202.102.245:34911
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ddq9i21o
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:36639
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:36639
distributed.worker - INFO -          dashboard at:      198.202.102.245:32987
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u533n4yv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:41967
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:41967
distributed.worker - INFO -          dashboard at:      198.202.102.245:35183
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-y__4parj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:44953
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:44953
distributed.worker - INFO -          dashboard at:      198.202.102.245:39867
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-08le3uu6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:45105
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:45105
distributed.worker - INFO -          dashboard at:      198.202.102.245:33153
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4t78rkb8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:33003
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:33003
distributed.worker - INFO -          dashboard at:      198.202.102.245:33771
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9qilbxka
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:37217
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:37217
distributed.worker - INFO -          dashboard at:      198.202.102.245:46625
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vg0avunm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:34635
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:34635
distributed.worker - INFO -          dashboard at:      198.202.102.245:37237
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-e761ckqr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:45555
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:45555
distributed.worker - INFO -          dashboard at:      198.202.102.245:45319
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-h23vv14t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:36285
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:36285
distributed.worker - INFO -          dashboard at:      198.202.102.245:33917
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ni9z0z42
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:45117
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:45117
distributed.worker - INFO -          dashboard at:      198.202.102.245:41217
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ycayexxn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:46061
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:46061
distributed.worker - INFO -          dashboard at:      198.202.102.245:38941
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_ls5e5fb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:46455
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:46455
distributed.worker - INFO -          dashboard at:      198.202.102.245:39517
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2dprijc8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:37697
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:37697
distributed.worker - INFO -          dashboard at:      198.202.102.245:40659
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9k490aok
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:37355
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:37355
distributed.worker - INFO -          dashboard at:      198.202.102.245:41237
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-g2ou8g43
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:45133
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:45133
distributed.worker - INFO -          dashboard at:      198.202.102.245:39333
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-j5isiyfl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:43561
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:43561
distributed.worker - INFO -          dashboard at:      198.202.102.245:44713
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wu1tt7wd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:44153
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:44153
distributed.worker - INFO -          dashboard at:      198.202.102.245:35729
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-utdmtf_l
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:43015
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:43015
distributed.worker - INFO -          dashboard at:      198.202.102.245:33399
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vzeozyws
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:43105
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:43105
distributed.worker - INFO -          dashboard at:      198.202.102.245:43961
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ehz2fxmb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:34445
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:34445
distributed.worker - INFO -          dashboard at:      198.202.102.245:43619
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_o31ar33
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:40149
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:40149
distributed.worker - INFO -          dashboard at:      198.202.102.245:43621
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0kw39izt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:43919
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:43919
distributed.worker - INFO -          dashboard at:      198.202.102.245:38641
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ogjv37wt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:41389
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:41389
distributed.worker - INFO -          dashboard at:      198.202.102.245:36269
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ogfmmw7j
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:46435
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:46435
distributed.worker - INFO -          dashboard at:      198.202.102.245:43165
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-b8lu4a7g
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:33131
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:33131
distributed.worker - INFO -          dashboard at:      198.202.102.245:46567
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1el4jy80
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:42075
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:42075
distributed.worker - INFO -          dashboard at:      198.202.102.245:39005
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-iui9ekam
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:46457
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:46457
distributed.worker - INFO -          dashboard at:      198.202.102.245:42907
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-avm7q3wd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:33731
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:33731
distributed.worker - INFO -          dashboard at:      198.202.102.245:33513
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_iy39o4w
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:40967
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:40967
distributed.worker - INFO -          dashboard at:      198.202.102.245:42337
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-dq7qe6dz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:43595
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:43595
distributed.worker - INFO -          dashboard at:      198.202.102.245:40153
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uqjcb70m
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:34507
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:34507
distributed.worker - INFO -          dashboard at:      198.202.102.245:37285
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uwek_xpo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:41983
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:41983
distributed.worker - INFO -          dashboard at:      198.202.102.245:34717
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gqyb3edv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:43017
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:43017
distributed.worker - INFO -          dashboard at:      198.202.102.245:46555
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6r0d0jdl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:36037
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:36037
distributed.worker - INFO -          dashboard at:      198.202.102.245:43145
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sa0y3wnx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:37981
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:37981
distributed.worker - INFO -          dashboard at:      198.202.102.245:36925
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zo1r0osj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:44313
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:44313
distributed.worker - INFO -          dashboard at:      198.202.102.245:46045
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n04_m304
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:46197
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:46197
distributed.worker - INFO -          dashboard at:      198.202.102.245:34775
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xxm2p4sq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:34413
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:34413
distributed.worker - INFO -          dashboard at:      198.202.102.245:45915
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-g5axm6ln
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:44031
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:44031
distributed.worker - INFO -          dashboard at:      198.202.102.245:38999
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kz6jrqqx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:46433
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:46433
distributed.worker - INFO -          dashboard at:      198.202.102.245:42285
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n06cnsgr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:37087
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:37087
distributed.worker - INFO -          dashboard at:      198.202.102.245:36125
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-x1qcjpfa
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:34699
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:34699
distributed.worker - INFO -          dashboard at:      198.202.102.245:33627
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-i2qhe9n9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:43199
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:43199
distributed.worker - INFO -          dashboard at:      198.202.102.245:41107
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9l06ra1b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:46673
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:46673
distributed.worker - INFO -          dashboard at:      198.202.102.245:39643
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-la1r_on_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:35927
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:35927
distributed.worker - INFO -          dashboard at:      198.202.102.245:33205
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0y66wgqm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:39081
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:39081
distributed.worker - INFO -          dashboard at:      198.202.102.245:42711
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2q5tqcy2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:35829
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:35829
distributed.worker - INFO -          dashboard at:      198.202.102.245:38355
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kc22k6vo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:45955
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:45955
distributed.worker - INFO -          dashboard at:      198.202.102.245:38015
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-37c_1l_m
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:37509
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:37509
distributed.worker - INFO -          dashboard at:      198.202.102.245:33725
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rf7vctep
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:43381
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:43381
distributed.worker - INFO -          dashboard at:      198.202.102.245:45843
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mkpiv7ui
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:34893
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:34893
distributed.worker - INFO -          dashboard at:      198.202.102.245:37783
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gvza2zgb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:44307
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:44307
distributed.worker - INFO -          dashboard at:      198.202.102.245:39187
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-voo64m6x
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:41569
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:41569
distributed.worker - INFO -          dashboard at:      198.202.102.245:45485
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7x7_9lzs
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:46361
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:46361
distributed.worker - INFO -          dashboard at:      198.202.102.245:35015
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-eaq0fgon
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:46657
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:46657
distributed.worker - INFO -          dashboard at:      198.202.102.245:33977
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zalsgzhm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:45435
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:45435
distributed.worker - INFO -          dashboard at:      198.202.102.245:34809
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4fhpb2kp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:38895
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:38895
distributed.worker - INFO -          dashboard at:      198.202.102.245:39291
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-owgpd3z3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:42369
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:42369
distributed.worker - INFO -          dashboard at:      198.202.102.245:47095
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kws3ulz1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:38313
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:38313
distributed.worker - INFO -          dashboard at:      198.202.102.245:46677
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lbrb7s4g
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:46831
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:46831
distributed.worker - INFO -          dashboard at:      198.202.102.245:33443
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ne5m6021
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:37111
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:37111
distributed.worker - INFO -          dashboard at:      198.202.102.245:41349
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-y2bjkl17
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:44567
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:44567
distributed.worker - INFO -          dashboard at:      198.202.102.245:36359
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xtad2_cl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:40457
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:40457
distributed.worker - INFO -          dashboard at:      198.202.102.245:40013
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pogaff12
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:35835
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:35835
distributed.worker - INFO -          dashboard at:      198.202.102.245:35697
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cij98_bl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:37561
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:37561
distributed.worker - INFO -          dashboard at:      198.202.102.245:36709
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xutt_vs6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:45729
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:45729
distributed.worker - INFO -          dashboard at:      198.202.102.245:37607
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jmmg4f38
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:45347
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:45347
distributed.worker - INFO -          dashboard at:      198.202.102.245:34287
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:34785
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:34785
distributed.worker - INFO -          dashboard at:      198.202.102.245:36699
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-11rovdtl
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-063v914e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:35753
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:35753
distributed.worker - INFO -          dashboard at:      198.202.102.245:37719
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-m_y0x71a
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:37435
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:37435
distributed.worker - INFO -          dashboard at:      198.202.102.245:37009
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-d_ak6l9u
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:45291
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:45291
distributed.worker - INFO -          dashboard at:      198.202.102.245:34381
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-smwwpc68
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:35165
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:35165
distributed.worker - INFO -          dashboard at:      198.202.102.245:35081
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mppi_7t3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:39083
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:39083
distributed.worker - INFO -          dashboard at:      198.202.102.245:42939
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4htnsp_t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:38693
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:38693
distributed.worker - INFO -          dashboard at:      198.202.102.245:44529
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qyv3gvef
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:32999
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:32999
distributed.worker - INFO -          dashboard at:      198.202.102.245:38253
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-v6sod_8e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:46015
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:46015
distributed.worker - INFO -          dashboard at:      198.202.102.245:33107
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9bpf9j7h
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:42443
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:42443
distributed.worker - INFO -          dashboard at:      198.202.102.245:37579
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-adw1z_zf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:39709
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:39709
distributed.worker - INFO -          dashboard at:      198.202.102.245:39833
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ryum3fna
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:41281
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:41281
distributed.worker - INFO -          dashboard at:      198.202.102.245:40049
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5wurc8il
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:40109
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:40109
distributed.worker - INFO -          dashboard at:      198.202.102.245:37357
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zcyi49ws
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:41577
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:41577
distributed.worker - INFO -          dashboard at:      198.202.102.245:37017
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:44957
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:44957
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:      198.202.102.245:37243
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0mft40bk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:40769
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u7ocr8sj
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:40769
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:      198.202.102.245:33821
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7rj_5xyk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:39637
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:39637
distributed.worker - INFO -          dashboard at:      198.202.102.245:33285
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-s_enl90e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:38091
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:38091
distributed.worker - INFO -          dashboard at:      198.202.102.245:33793
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zve8w_zz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:42269
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:42269
distributed.worker - INFO -          dashboard at:      198.202.102.245:40401
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:41403
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:41403
distributed.worker - INFO -          dashboard at:      198.202.102.245:46341
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-adxqgj3i
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gakb3ssk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:41307
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:41307
distributed.worker - INFO -          dashboard at:      198.202.102.245:39003
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2xw3w7i2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:39061
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:39061
distributed.worker - INFO -          dashboard at:      198.202.102.245:45359
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-y2s8aa11
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:43675
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:43675
distributed.worker - INFO -          dashboard at:      198.202.102.245:34493
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-c_1a5j6x
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:46013
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:46013
distributed.worker - INFO -          dashboard at:      198.202.102.245:33753
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ijx3l0d2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:35427
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:35427
distributed.worker - INFO -          dashboard at:      198.202.102.245:36217
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vwg5aijo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:37573
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:37573
distributed.worker - INFO -          dashboard at:      198.202.102.245:40107
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-o46kwl87
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.245:45055
distributed.worker - INFO -          Listening to: tcp://198.202.102.245:45055
distributed.worker - INFO -          dashboard at:      198.202.102.245:33261
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-k23feoib
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:34013'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:41525'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:35277'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:33003
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:40837'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:36639
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:45587'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:35773
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:36999'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:45555
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:38481'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:46061
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:44527'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:37217
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:33787'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:45105
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:37361'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:46259
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:34635
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:46687'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:45307'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:39425
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:37697
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:34619'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:35969'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:45125
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:34773'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:40149
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:41967
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:39563'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:37151'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:45963
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:40053'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:36285
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:45317'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:43015
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:36157'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:37355
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:41255'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:45709'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:46455
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:44153
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:45501'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:34445
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:44567
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:41929'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:38645'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:40967
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:45031'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:43561
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:39215'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:42075
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:38325'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:43105
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:43055'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:45117
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:38155'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:41389
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:36449'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:33131
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:41687'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:41983
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:45885'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:43919
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:42297'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:43595
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:41443'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:46197
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:46695'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:34507
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:43017
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:43553'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:44621'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:33731
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:34303'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:37509
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:36839'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:36037
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:36299'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:46435
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:33979'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:45133
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:36131'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:34413
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:34113'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:39081
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:44059'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:44313
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:38387'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:42369
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:38001'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:44031
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:45387'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:38895
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:36877'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:46457
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:46763'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:46361
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:38891'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:39637
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:38985'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:39083
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:44759'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:42443
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:45019'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:46673
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:36965'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:37397
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:37813'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:35927
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:35829
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:46657
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:44203'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:38213'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:33053'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:43199
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:34029'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:34699
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:33089'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:43381
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:46233'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:37111
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:45195'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:35835
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:44307
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:46177'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:45291
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:34035'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:36807'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:38693
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:35501'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:44953
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:39647'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:35753
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:40627'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:46433
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:43809'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:45347
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:42755'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:38313
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:35061'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:32999
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:40363'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:37087
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:44909'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:45055
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:45873'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:35165
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:34727'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:46831
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:42471'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:41569
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:43895'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:34893
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:46611'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:34785
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:41917'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:45955
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:33365'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:37435
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:34545'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:39061
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:46535'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:38091
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:44957
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:37259'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:46977'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:37561
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:39383'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:39709
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:37383'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:37187'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:46015
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:37547'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:37981
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:40109
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:42293'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:40457
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:45729
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:42745'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:35945'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:35427
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:44687'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:41403
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:39791'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:45435
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:43675
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:41577
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:38191'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:39435'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:39897'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:37573
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:40403'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:41307
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:38683'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:40769
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.245:40477'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:42269
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:41281
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.245:46013
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4828 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4826 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4821 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4820 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4824 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4818 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4814 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4816 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4810 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4812 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4806 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4804 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4802 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4808 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4795 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4791 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4787 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4780 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4782 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4778 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4774 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4768 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4771 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4760 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4763 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4755 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4757 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4752 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4749 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4743 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4741 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4732 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4746 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4738 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4735 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4729 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4727 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4725 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4712 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4715 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4710 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4698 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4695 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4700 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4689 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4679 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4688 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4684 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4686 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4673 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4668 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4665 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4659 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4653 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4656 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4644 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4647 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4650 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4634 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4637 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4612 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4601 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4593 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4574 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4576 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4572 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4569 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4565 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4564 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4566 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4559 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4557 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4561 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4554 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4552 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4548 parent=4458 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4546 parent=4458 started daemon>
