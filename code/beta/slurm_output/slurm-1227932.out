distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:39543'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44531'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36961'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37211'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46789'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36855'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42943'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46393'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44391'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36353'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:39845'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34507'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35361'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46305'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44473'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35569'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:33381'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37593'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35637'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46865'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:43431'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37669'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34587'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:38489'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35313'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37153'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37035'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44021'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42039'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34525'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40437'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44957'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:39155'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37901'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44255'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:45261'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:41385'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36225'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:41669'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:38153'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36387'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40269'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:41741'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34049'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37411'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44187'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42963'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44777'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:43563'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40979'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40253'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:33201'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:38391'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:39751'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34747'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:43163'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:32983'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44845'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35227'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:45055'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:41331'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:43089'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37261'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36077'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42753'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37527'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:41803'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35113'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46983'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34413'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42919'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:47099'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40081'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:39945'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:45679'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46999'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35853'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40805'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40671'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44395'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:45919'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37305'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44277'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46219'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:43569'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42127'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46193'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36759'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:41591'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:43129'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44763'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:38623'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37115'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:41047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35091'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42287'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36161'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:39477'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34019
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:37935
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34019
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:37935
distributed.worker - INFO -          dashboard at:      198.202.101.116:34033
distributed.worker - INFO -          dashboard at:      198.202.101.116:33495
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p02hf_ht
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-e_il6mql
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44805
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44805
distributed.worker - INFO -          dashboard at:      198.202.101.116:38597
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ay_47kdl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38321
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38321
distributed.worker - INFO -          dashboard at:      198.202.101.116:38379
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ntlstqkz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:37581
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:37581
distributed.worker - INFO -          dashboard at:      198.202.101.116:35879
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hlrt71qd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36785
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36785
distributed.worker - INFO -          dashboard at:      198.202.101.116:38863
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-32wn7enr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46827
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46827
distributed.worker - INFO -          dashboard at:      198.202.101.116:42659
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6umg944o
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:35167
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:35167
distributed.worker - INFO -          dashboard at:      198.202.101.116:46025
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jr7q9loo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:35739
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:35739
distributed.worker - INFO -          dashboard at:      198.202.101.116:45783
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-nh1q74r0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34463
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34463
distributed.worker - INFO -          dashboard at:      198.202.101.116:40357
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jlepype6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:43905
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:43905
distributed.worker - INFO -          dashboard at:      198.202.101.116:45773
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-45fygbdb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41999
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41999
distributed.worker - INFO -          dashboard at:      198.202.101.116:35375
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-22kzn193
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42421
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42421
distributed.worker - INFO -          dashboard at:      198.202.101.116:38471
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-c2d7nssd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34093
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34093
distributed.worker - INFO -          dashboard at:      198.202.101.116:43965
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-s4a8jv0r
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38611
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38611
distributed.worker - INFO -          dashboard at:      198.202.101.116:42135
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0i2uz9gy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36733
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36733
distributed.worker - INFO -          dashboard at:      198.202.101.116:36901
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-66a57fv7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:45151
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:45151
distributed.worker - INFO -          dashboard at:      198.202.101.116:33141
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3zw1l499
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46885
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46885
distributed.worker - INFO -          dashboard at:      198.202.101.116:39785
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p2zbm534
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42785
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42785
distributed.worker - INFO -          dashboard at:      198.202.101.116:33811
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44189
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44189
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:      198.202.101.116:37385
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jnm96mxq
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4yjqu6c_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44953
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44953
distributed.worker - INFO -          dashboard at:      198.202.101.116:42425
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-347761cg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46359
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46359
distributed.worker - INFO -          dashboard at:      198.202.101.116:36449
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wdz8m7ta
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:35273
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:35273
distributed.worker - INFO -          dashboard at:      198.202.101.116:42239
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u0zfl2ky
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34203
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34203
distributed.worker - INFO -          dashboard at:      198.202.101.116:39523
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-h9oolbqo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44731
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44731
distributed.worker - INFO -          dashboard at:      198.202.101.116:43775
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cl79ivbh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42689
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42689
distributed.worker - INFO -          dashboard at:      198.202.101.116:38085
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-oy4osh1v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38519
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38519
distributed.worker - INFO -          dashboard at:      198.202.101.116:40923
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9kardco4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:45569
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:45569
distributed.worker - INFO -          dashboard at:      198.202.101.116:41659
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6nd2d_3k
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:47011
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:47011
distributed.worker - INFO -          dashboard at:      198.202.101.116:38091
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-32bq7kvm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34659
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34659
distributed.worker - INFO -          dashboard at:      198.202.101.116:35877
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ngq8929p
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42661
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42661
distributed.worker - INFO -          dashboard at:      198.202.101.116:42539
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ydw0d3km
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46675
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46675
distributed.worker - INFO -          dashboard at:      198.202.101.116:40127
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-a1mklkj3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34343
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34343
distributed.worker - INFO -          dashboard at:      198.202.101.116:36089
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8c4oyqeg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:43119
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:43119
distributed.worker - INFO -          dashboard at:      198.202.101.116:38741
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xupe28fv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46577
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46577
distributed.worker - INFO -          dashboard at:      198.202.101.116:35201
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-c75levbk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34831
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34831
distributed.worker - INFO -          dashboard at:      198.202.101.116:41123
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-z8v7bq7z
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42955
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42955
distributed.worker - INFO -          dashboard at:      198.202.101.116:35543
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ydquikfd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44481
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44481
distributed.worker - INFO -          dashboard at:      198.202.101.116:39537
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zyxf9x4t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36461
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36461
distributed.worker - INFO -          dashboard at:      198.202.101.116:37097
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hemlzqxl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42587
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42587
distributed.worker - INFO -          dashboard at:      198.202.101.116:39933
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hk6k2ujb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42637
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42637
distributed.worker - INFO -          dashboard at:      198.202.101.116:45595
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-z4tb1c2u
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41907
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41907
distributed.worker - INFO -          dashboard at:      198.202.101.116:36355
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p8r5nmp6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:32877
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:32877
distributed.worker - INFO -          dashboard at:      198.202.101.116:38069
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0onv_0o7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:39339
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:39339
distributed.worker - INFO -          dashboard at:      198.202.101.116:39215
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ydrwt7xw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38241
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38241
distributed.worker - INFO -          dashboard at:      198.202.101.116:33447
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gd7vvqsa
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:35401
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:35401
distributed.worker - INFO -          dashboard at:      198.202.101.116:44311
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vttrhedy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:43591
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:43591
distributed.worker - INFO -          dashboard at:      198.202.101.116:42795
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-o8t0uo18
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36643
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36643
distributed.worker - INFO -          dashboard at:      198.202.101.116:45317
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xgqwh1b9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38717
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38717
distributed.worker - INFO -          dashboard at:      198.202.101.116:46229
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-nys2chtt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46355
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46355
distributed.worker - INFO -          dashboard at:      198.202.101.116:42231
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-28gbtqrr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:37983
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:37983
distributed.worker - INFO -          dashboard at:      198.202.101.116:34287
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4tpx_rp7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:39073
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:39073
distributed.worker - INFO -          dashboard at:      198.202.101.116:37645
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ek04kicj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36675
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36675
distributed.worker - INFO -          dashboard at:      198.202.101.116:46989
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:35463
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:35463
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5xc0vz3z
distributed.worker - INFO -          dashboard at:      198.202.101.116:41905
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qpk7n6m8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42701
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42701
distributed.worker - INFO -          dashboard at:      198.202.101.116:35553
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wz7ctnaf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:32891
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:32891
distributed.worker - INFO -          dashboard at:      198.202.101.116:44807
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bf5qarye
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:33845
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:33845
distributed.worker - INFO -          dashboard at:      198.202.101.116:35653
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sf_uerkw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34481
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34481
distributed.worker - INFO -          dashboard at:      198.202.101.116:47037
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2_ag6ad8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41493
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41493
distributed.worker - INFO -          dashboard at:      198.202.101.116:33053
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-52erzcvh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:45201
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:45201
distributed.worker - INFO -          dashboard at:      198.202.101.116:43675
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-x6zny0bj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46475
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46475
distributed.worker - INFO -          dashboard at:      198.202.101.116:33995
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-e0y2ok88
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:40537
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:40537
distributed.worker - INFO -          dashboard at:      198.202.101.116:36335
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ngb2t80s
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34221
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34221
distributed.worker - INFO -          dashboard at:      198.202.101.116:44465
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xvoas8gr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44627
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44627
distributed.worker - INFO -          dashboard at:      198.202.101.116:40861
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2dugdpt9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36951
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36951
distributed.worker - INFO -          dashboard at:      198.202.101.116:40627
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ibqxfaya
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34053
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34053
distributed.worker - INFO -          dashboard at:      198.202.101.116:46411
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lc3zro4b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42279
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42279
distributed.worker - INFO -          dashboard at:      198.202.101.116:41177
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-o061dgqh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:33389
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:33389
distributed.worker - INFO -          dashboard at:      198.202.101.116:43393
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-msnjk17i
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:43603
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:43603
distributed.worker - INFO -          dashboard at:      198.202.101.116:33345
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ajkxefxu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34859
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34859
distributed.worker - INFO -          dashboard at:      198.202.101.116:43725
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tpus9h_9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:39581
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:39581
distributed.worker - INFO -          dashboard at:      198.202.101.116:44273
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_18eucma
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:40411
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:40411
distributed.worker - INFO -          dashboard at:      198.202.101.116:37825
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-icezotot
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34795
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34795
distributed.worker - INFO -          dashboard at:      198.202.101.116:38595
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ud510is2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:37717
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:37717
distributed.worker - INFO -          dashboard at:      198.202.101.116:43449
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8li0_9se
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:35983
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:35983
distributed.worker - INFO -          dashboard at:      198.202.101.116:37683
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-g9uqdh85
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:45153
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:45153
distributed.worker - INFO -          dashboard at:      198.202.101.116:34015
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fxct813h
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34465
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34465
distributed.worker - INFO -          dashboard at:      198.202.101.116:43417
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-f5qy74ji
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:37905
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:37905
distributed.worker - INFO -          dashboard at:      198.202.101.116:39877
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cdm7w2f9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:39595
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:39595
distributed.worker - INFO -          dashboard at:      198.202.101.116:45045
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gzg66bhh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:33339
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:33339
distributed.worker - INFO -          dashboard at:      198.202.101.116:41513
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cwbinte4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:39209
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:39209
distributed.worker - INFO -          dashboard at:      198.202.101.116:33449
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3wg06hwy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:43659
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:43659
distributed.worker - INFO -          dashboard at:      198.202.101.116:39497
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mneom65d
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42259
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42259
distributed.worker - INFO -          dashboard at:      198.202.101.116:35645
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7mzc7019
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41381
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41381
distributed.worker - INFO -          dashboard at:      198.202.101.116:46491
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1lnphlzr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38939
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38939
distributed.worker - INFO -          dashboard at:      198.202.101.116:45847
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pb1u3d_z
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:37899
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:37899
distributed.worker - INFO -          dashboard at:      198.202.101.116:41987
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qz34wgx7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:33059
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:33059
distributed.worker - INFO -          dashboard at:      198.202.101.116:36607
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1sbpte3m
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36715
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36715
distributed.worker - INFO -          dashboard at:      198.202.101.116:46489
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wrnicqnq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:40935
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:40935
distributed.worker - INFO -          dashboard at:      198.202.101.116:44407
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-um247_8i
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:43655
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:43655
distributed.worker - INFO -          dashboard at:      198.202.101.116:35925
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9fmzzwvk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:40385
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:40385
distributed.worker - INFO -          dashboard at:      198.202.101.116:42529
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w8r5uola
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38551
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38551
distributed.worker - INFO -          dashboard at:      198.202.101.116:42175
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0qqd5awf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:37399
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:37399
distributed.worker - INFO -          dashboard at:      198.202.101.116:36331
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4wpla131
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:43131
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:43131
distributed.worker - INFO -          dashboard at:      198.202.101.116:37933
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8cvjesvy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:43681
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:43681
distributed.worker - INFO -          dashboard at:      198.202.101.116:43287
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1xmvkq1n
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41321
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41321
distributed.worker - INFO -          dashboard at:      198.202.101.116:34215
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-y9ha7t_e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36507
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36507
distributed.worker - INFO -          dashboard at:      198.202.101.116:39955
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rs0gxn9t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44369
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44369
distributed.worker - INFO -          dashboard at:      198.202.101.116:45861
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-a3tr7grd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:39447
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:39447
distributed.worker - INFO -          dashboard at:      198.202.101.116:46471
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-albmzv7h
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:47021
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:47021
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -          dashboard at:      198.202.101.116:37681
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gehf2qoi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:39543'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44531'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36961'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:37581
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37211'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:37935
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46789'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44805
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34019
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36855'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42943'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38321
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46393'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:35739
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44391'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36785
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36353'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:35167
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:39845'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34463
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:43905
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34507'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35361'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41999
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46305'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46827
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44473'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42421
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34093
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35569'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38611
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:33381'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37593'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:45151
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35637'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36733
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46865'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46885
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:43431'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44189
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37669'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42785
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34587'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44953
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:38489'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46359
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35313'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:35273
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37153'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34203
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37035'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44731
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44021'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42039'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38519
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42689
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34525'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:45569
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40437'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34659
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44957'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46675
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:39155'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34343
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37901'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:47011
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44255'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44481
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:45261'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42955
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:41385'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36225'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42661
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34831
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:41669'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46577
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:38153'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:43119
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42637
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36387'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40269'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36461
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41907
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:41741'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34049'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42587
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37411'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:32877
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44187'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38241
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42963'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:35401
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44777'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:43563'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:43591
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:39339
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40979'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36643
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40253'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42701
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:33201'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46355
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37401'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34481
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:38391'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:33845
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:39751'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:39073
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34747'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46475
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:43163'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:37983
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:32983'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:40537
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44845'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36675
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35227'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38717
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:45055'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:32891
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:41331'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41493
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:43089'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44627
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37261'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34221
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:45201
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:33389
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36077'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42753'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37527'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34053
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:41803'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:39581
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35113'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42279
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:43603
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46983'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34413'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:35983
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42919'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:40411
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:47099'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36951
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36237'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:37717
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40081'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:39595
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:39945'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:45679'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:40935
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36507
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46999'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:39209
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35853'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:43659
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40805'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34859
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40671'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:35463
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44395'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:45919'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41381
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:37399
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37305'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:37905
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44277'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34465
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46219'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:43569'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42259
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38551
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42127'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:45153
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46193'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36759'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:33339
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:39447
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:41591'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:43131
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:43129'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44369
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44763'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34795
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41321
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:38623'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37115'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:43655
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:41047'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38939
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35091'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:47021
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42287'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:37899
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36161'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:40385
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:39477'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:33059
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36715
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:43681
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33201 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33199 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33194 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33197 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33192 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33188 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33185 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33190 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33183 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33178 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33176 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33181 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33172 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33167 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33165 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33162 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33159 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33155 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33148 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33145 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33140 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33136 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33132 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33130 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33128 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33121 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33150 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33123 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33116 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33111 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33112 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33097 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33104 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33100 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33093 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33091 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33087 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33081 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33077 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33078 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33070 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33066 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33059 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33056 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33062 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33051 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33048 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33042 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33040 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33036 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33032 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33029 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33021 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33018 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33015 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33011 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33009 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33006 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33002 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32997 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32996 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32994 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32991 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32987 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32984 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32982 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32977 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32975 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32971 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32969 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32966 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32963 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32960 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32956 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32954 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32950 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32945 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32941 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32939 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32937 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32933 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32930 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32927 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32924 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32920 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32915 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32917 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32912 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32909 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32906 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32900 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32901 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32896 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32894 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32889 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32891 parent=32814 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32887 parent=32814 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
