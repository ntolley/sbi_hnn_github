distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:33107'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:39553'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:43861'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:36537'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:35193'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:39057'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:46811'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:36813'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:35035'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:36265'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:42109'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:41381'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:38469'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:42851'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:43841'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:37903'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:42279'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:40043'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:41307'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:43561'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:39355'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:40239'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:41629'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:37525'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:42433'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:44009'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:40525'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:45051'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:46599'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:37923'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:42483'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:37231'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:45565'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:41059'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:33739'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:39669'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:39349'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:43989'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:42459'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:40737'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:41193'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:37843'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:45437'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:37641'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:41597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:44299'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:41621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:37637'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:35237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:35121'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:38227'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:45237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:40703'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:34281'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:36743'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:37485'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:44387'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:39773'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:38501'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:33769'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:42091'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:36771'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:38769'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:42121'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:37935'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:41215'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:39603'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:41791'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:33179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:34307'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:44863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:46347'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:46981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:34537'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:36943'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:43499'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:44217'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:43445'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:42363'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:33905'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:43283'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:37541'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:45787'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:41283'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:45197'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:43059'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:46207'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:39901'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:36855'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:37905'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:32897'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:37085'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:33651'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:35931'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:42783'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:40535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:46121'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:43419'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:46937'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.34:39395'
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kdc2axwx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6s7n_3ox', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jotxyyht', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u0_9w2gx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5ccjw7r2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kj_divun', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1o7vs_lg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-crxapq30', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n3st5aim', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jel25qe1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ae03evr8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w1ckcdwc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-m_k0ofo9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-th1ejbyv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3w_qjxcq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-88hwhx0j', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-g0mgay7a', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-nqve77ux', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-reyxkkes', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fezk5iux', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-d5u301jr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-i9chd_s8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-41wwjmel', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vx1f86je', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_2thqrmn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-den453fo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-nih2pm6s', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zssh7_6k', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6f7bv2bt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qtv63yvp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-b51v23qe', purging
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:41053
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:38771
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:41407
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:42667
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:41053
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:36723
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:38771
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:33315
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:41407
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:42667
distributed.worker - INFO -          dashboard at:       198.202.103.34:44487
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:34357
distributed.worker - INFO -          dashboard at:       198.202.103.34:40221
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:36723
distributed.worker - INFO -          dashboard at:       198.202.103.34:45315
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:40511
distributed.worker - INFO -          dashboard at:       198.202.103.34:39661
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:35011
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:33315
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:42291
distributed.worker - INFO -          dashboard at:       198.202.103.34:42595
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:34357
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:43497
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:40511
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:36489
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:35011
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -          dashboard at:       198.202.103.34:36829
distributed.worker - INFO -          dashboard at:       198.202.103.34:37455
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:42291
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:43497
distributed.worker - INFO -          dashboard at:       198.202.103.34:43447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:36489
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:       198.202.103.34:38879
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:45321
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:       198.202.103.34:40407
distributed.worker - INFO -          dashboard at:       198.202.103.34:44039
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:37543
distributed.worker - INFO -          dashboard at:       198.202.103.34:33783
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:46521
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:38689
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:36423
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:35487
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:37543
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:37417
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-74h6lymi
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:46521
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:46133
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:39893
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:38689
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:43515
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wsay384i
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:46433
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:36423
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-v_2kqg96
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:45075
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:       198.202.103.34:41131
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:35487
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:45555
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:46433
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:37417
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:33943
distributed.worker - INFO -          dashboard at:       198.202.103.34:37577
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3uf53v7j
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:42499
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-28bl4f5f
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:46133
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:42237
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:       198.202.103.34:33171
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:39893
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:37497
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:45321
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:43515
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:46879
distributed.worker - INFO -          dashboard at:       198.202.103.34:43271
distributed.worker - INFO -                Memory:                 1000.00 MB
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:44211
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:34633
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:40655
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:45075
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:46073
distributed.worker - INFO -          dashboard at:       198.202.103.34:45115
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:       198.202.103.34:38905
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:40781
distributed.worker - INFO -          dashboard at:       198.202.103.34:34619
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:45555
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:41617
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:33943
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -          dashboard at:       198.202.103.34:37839
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:42499
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:42237
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-akd1rnqz
distributed.worker - INFO -          dashboard at:       198.202.103.34:40063
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:37497
distributed.worker - INFO -          dashboard at:       198.202.103.34:34229
distributed.worker - INFO -          dashboard at:       198.202.103.34:39413
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-dwkixpn4
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:46879
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:44211
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yj_hi4mj
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:34633
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fytqeap9
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:40655
distributed.worker - INFO -          dashboard at:       198.202.103.34:44043
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -          dashboard at:       198.202.103.34:39079
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:46073
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:40781
distributed.worker - INFO -          dashboard at:       198.202.103.34:44603
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-f_3bh8pl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vkhd5gp5
distributed.worker - INFO -          dashboard at:       198.202.103.34:41079
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:41617
distributed.worker - INFO -          dashboard at:       198.202.103.34:43909
distributed.worker - INFO -          dashboard at:       198.202.103.34:34611
distributed.worker - INFO -          dashboard at:       198.202.103.34:34655
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -          dashboard at:       198.202.103.34:36991
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-frk3bl4c
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:       198.202.103.34:42049
distributed.worker - INFO -          dashboard at:       198.202.103.34:43273
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -          dashboard at:       198.202.103.34:36883
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:       198.202.103.34:45815
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -          dashboard at:       198.202.103.34:36681
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pa2r6qrj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gyyjs8qo
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fvpb1374
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-15mfqn1l
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-q3kohztk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ykfvdthv
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pkaom9pk
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-c54bo2e2
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bcwvij4x
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0_0yofbt
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tdfyqsm8
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-f1va2z_o
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sb8xjrn9
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9lau7lls
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-orl_7aaa
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-goi40c1g
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5hvoyrfn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qgbp0de0
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-b662piuv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cnsspsb7
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pyox1iws
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sqmpyx1s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n84ex9m3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1h584o4v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:39367
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:39367
distributed.worker - INFO -          dashboard at:       198.202.103.34:45603
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-e872w3sj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:42725
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:42725
distributed.worker - INFO -          dashboard at:       198.202.103.34:43349
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-x3alsv8b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:36663
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:36663
distributed.worker - INFO -          dashboard at:       198.202.103.34:34041
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mjjcj1k9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:41927
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:41927
distributed.worker - INFO -          dashboard at:       198.202.103.34:40475
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ohguq69_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:42905
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:42905
distributed.worker - INFO -          dashboard at:       198.202.103.34:45591
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5iu9fz6m
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:38017
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:38017
distributed.worker - INFO -          dashboard at:       198.202.103.34:47061
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-57kd4vc0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:36517
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:36517
distributed.worker - INFO -          dashboard at:       198.202.103.34:38043
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vc7_gva6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:37785
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:37785
distributed.worker - INFO -          dashboard at:       198.202.103.34:33519
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-y0wl5jme
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:37813
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:37813
distributed.worker - INFO -          dashboard at:       198.202.103.34:34547
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-deaudhee
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:44165
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:44165
distributed.worker - INFO -          dashboard at:       198.202.103.34:44655
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ckxko8ps
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:33721
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:33721
distributed.worker - INFO -          dashboard at:       198.202.103.34:39907
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mthbgva4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:34313
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:34313
distributed.worker - INFO -          dashboard at:       198.202.103.34:45545
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6hne4frh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:45763
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:45763
distributed.worker - INFO -          dashboard at:       198.202.103.34:37769
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:39885
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:39885
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:       198.202.103.34:37089
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4zr72oya
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vi5f7_1b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:40567
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:40567
distributed.worker - INFO -          dashboard at:       198.202.103.34:39133
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-82fpbsxp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:42333
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:42333
distributed.worker - INFO -          dashboard at:       198.202.103.34:44957
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_t25pxub
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:40711
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:40711
distributed.worker - INFO -          dashboard at:       198.202.103.34:42673
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n83rx_re
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:36539
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:36539
distributed.worker - INFO -          dashboard at:       198.202.103.34:40801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-k11jhmvs
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:43651
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:43651
distributed.worker - INFO -          dashboard at:       198.202.103.34:34343
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9a5tz_ma
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:41501
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:41501
distributed.worker - INFO -          dashboard at:       198.202.103.34:34435
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p9cksye2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:46775
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:46775
distributed.worker - INFO -          dashboard at:       198.202.103.34:35831
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n23fof_w
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:42939
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:42939
distributed.worker - INFO -          dashboard at:       198.202.103.34:34869
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-suucjhl9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:36535
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:36535
distributed.worker - INFO -          dashboard at:       198.202.103.34:37941
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-33ncphsc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:32937
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:32937
distributed.worker - INFO -          dashboard at:       198.202.103.34:40657
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:39297
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:39297
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ojqnk8n4
distributed.worker - INFO -          dashboard at:       198.202.103.34:45341
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-i648lghn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:33455
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:33455
distributed.worker - INFO -          dashboard at:       198.202.103.34:43063
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-201v_ggt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:39173
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:39173
distributed.worker - INFO -          dashboard at:       198.202.103.34:43673
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6bai_75q
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:33673
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:33673
distributed.worker - INFO -          dashboard at:       198.202.103.34:36433
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bollywd5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:42549
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:42549
distributed.worker - INFO -          dashboard at:       198.202.103.34:38365
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yoo2hz61
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:36283
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:36283
distributed.worker - INFO -          dashboard at:       198.202.103.34:36773
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gf1608l2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:36695
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:36695
distributed.worker - INFO -          dashboard at:       198.202.103.34:41493
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wmmyb58y
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:44261
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:44261
distributed.worker - INFO -          dashboard at:       198.202.103.34:39081
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u6_phwyv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:34879
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:34879
distributed.worker - INFO -          dashboard at:       198.202.103.34:39425
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5x8vi7g_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:40333
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:40333
distributed.worker - INFO -          dashboard at:       198.202.103.34:40047
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lrk64buk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:34607
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:34607
distributed.worker - INFO -          dashboard at:       198.202.103.34:36891
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ugo6nus2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:37615
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:37615
distributed.worker - INFO -          dashboard at:       198.202.103.34:44885
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yroc4hs1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:42143
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:42143
distributed.worker - INFO -          dashboard at:       198.202.103.34:36269
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hqpbdt9q
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:32981
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:32981
distributed.worker - INFO -          dashboard at:       198.202.103.34:40653
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1le6bwyl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:42603
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:42603
distributed.worker - INFO -          dashboard at:       198.202.103.34:41467
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mbor44hj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:44907
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:44907
distributed.worker - INFO -          dashboard at:       198.202.103.34:44527
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-r2l0dm2y
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:36187
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:46659
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:46659
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:36187
distributed.worker - INFO -          dashboard at:       198.202.103.34:45273
distributed.worker - INFO -          dashboard at:       198.202.103.34:41003
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rz9n0tc1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yxr4geu4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:44831
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:44831
distributed.worker - INFO -          dashboard at:       198.202.103.34:40811
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-x8r8hhfr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:43605
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:43605
distributed.worker - INFO -          dashboard at:       198.202.103.34:34761
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zrsep691
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:33929
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:33929
distributed.worker - INFO -          dashboard at:       198.202.103.34:46077
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:35465
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:35465
distributed.worker - INFO -          dashboard at:       198.202.103.34:41885
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-eq0foz5e
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-c3r1ifq1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:36257
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:36257
distributed.worker - INFO -          dashboard at:       198.202.103.34:38559
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-j5i_ajp3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:44449
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:44449
distributed.worker - INFO -          dashboard at:       198.202.103.34:43057
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-r6ypad79
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:39077
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:39077
distributed.worker - INFO -          dashboard at:       198.202.103.34:35555
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rzb5u71q
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:32991
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:32991
distributed.worker - INFO -          dashboard at:       198.202.103.34:40973
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rt09b7e9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:46327
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:46327
distributed.worker - INFO -          dashboard at:       198.202.103.34:40175
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ld6bzbqe
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:42035
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:42035
distributed.worker - INFO -          dashboard at:       198.202.103.34:37907
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-goer1mvg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:33959
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:33959
distributed.worker - INFO -          dashboard at:       198.202.103.34:33229
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-dbz40ma1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:35927
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:35927
distributed.worker - INFO -          dashboard at:       198.202.103.34:41295
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gpg5nqgh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:34475
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:34475
distributed.worker - INFO -          dashboard at:       198.202.103.34:34635
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rqx38s76
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:43943
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:43943
distributed.worker - INFO -          dashboard at:       198.202.103.34:39273
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jau84_o6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:36657
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:36657
distributed.worker - INFO -          dashboard at:       198.202.103.34:46875
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kl8zbwlj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:46735
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:46735
distributed.worker - INFO -          dashboard at:       198.202.103.34:44709
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-29rs4qsz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:43255
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:43255
distributed.worker - INFO -          dashboard at:       198.202.103.34:37579
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7ha_3nlx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:46601
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:46601
distributed.worker - INFO -          dashboard at:       198.202.103.34:38599
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-erznq71z
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:36185
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:36185
distributed.worker - INFO -          dashboard at:       198.202.103.34:39711
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-btvb5lux
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:38857
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:38857
distributed.worker - INFO -          dashboard at:       198.202.103.34:36425
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ihqtz39r
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:35019
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:35019
distributed.worker - INFO -          dashboard at:       198.202.103.34:39801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ndnda2rl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.34:38545
distributed.worker - INFO -          Listening to: tcp://198.202.103.34:38545
distributed.worker - INFO -          dashboard at:       198.202.103.34:35407
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_gofvese
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:43801
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:33107'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:39553'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:43861'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:38689
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:46073
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:36537'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:35193'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:38857
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:35011
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:39057'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:46811'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:36813'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:40655
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:41053
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:35035'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:42237
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:36265'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:42109'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:37417
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:40781
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:41381'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:38771
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:38469'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:34633
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:42851'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:33315
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:43841'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:46521
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:37903'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:44211
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:42279'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:40043'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:43497
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:36489
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:41307'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:46433
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:43561'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:33943
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:39355'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:36723
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:40239'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:39893
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:41629'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:37525'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:41617
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:41407
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:42433'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:37543
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:44009'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:42667
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:40525'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:40511
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:45051'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:46879
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:36423
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:46599'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:37923'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:42499
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:42483'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:34357
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:37231'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:43515
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:45565'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:45075
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:41059'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:46133
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:33739'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:37497
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:39669'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:45555
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:39349'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:42291
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:43989'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:35487
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:42459'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:39367
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:40737'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:41193'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:36663
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:42725
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:37843'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:34313
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:38017
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:45437'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:37641'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:36517
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:41597'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:45763
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:44299'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:41927
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:41621'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:42905
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:37637'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:44165
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:35237'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:33721
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:35121'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:40711
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:38227'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:46775
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:45237'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:37785
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:39885
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:37813
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:40703'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:34281'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:36743'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:41501
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:37485'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:44387'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:36695
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:39773'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:42333
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:36539
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:38501'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:33455
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:33769'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:42091'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:36535
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:32937
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:43651
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:36771'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:38769'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:39297
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:42121'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:39173
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:37935'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:42939
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:41215'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:33673
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:39603'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:42143
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:41791'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:42549
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:33179'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:34607
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:34307'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:44863'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:34879
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:46347'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:37615
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:46981'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:40567
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:44261
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:32981
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:44831
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:34537'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:36943'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:43499'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:46659
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:44217'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:43445'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:42603
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:40333
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:42363'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:44907
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:33905'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:43283'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:36283
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:33929
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:37541'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:32991
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:45787'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:41283'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:43605
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:33959
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:45197'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:36657
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:43059'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:44449
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:46207'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:35927
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:39901'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:36855'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:43943
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:34475
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:37905'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:32897'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:36185
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:39077
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:37085'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:45321
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:33651'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:35931'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:46327
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:42035
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:42783'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:43255
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:40535'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:35019
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:46121'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:36257
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:43419'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:36187
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:46937'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:46735
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.34:39395'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:46601
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:38545
distributed.worker - INFO - Stopping worker at tcp://198.202.103.34:35465
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - INFO - Worker process 109036 was killed by signal 15
distributed.nanny - INFO - Worker process 109053 was killed by signal 15
distributed.nanny - INFO - Worker process 109029 was killed by signal 15
distributed.nanny - INFO - Worker process 109018 was killed by signal 15
distributed.nanny - INFO - Worker process 109022 was killed by signal 15
distributed.nanny - INFO - Worker process 109031 was killed by signal 15
distributed.nanny - INFO - Worker process 109041 was killed by signal 15
distributed.nanny - INFO - Worker process 109050 was killed by signal 15
distributed.nanny - INFO - Worker process 109047 was killed by signal 15
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - INFO - Worker process 109683 was killed by signal 15
distributed.nanny - INFO - Worker process 109204 was killed by signal 15
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109792 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109790 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109788 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109782 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109780 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109785 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109778 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109771 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109769 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109765 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109762 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109759 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109755 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109752 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109750 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109612 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109658 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109679 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109611 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109482 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109416 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109232 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109217 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109214 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109208 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109211 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109219 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109197 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109200 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109194 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109186 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109190 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109184 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109177 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109182 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109170 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109172 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109167 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109163 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109160 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109154 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109146 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109148 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109150 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109153 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109140 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109136 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109133 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109130 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109126 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109122 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109117 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109115 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109112 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109109 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109104 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109101 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109097 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109098 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109093 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109087 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109089 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109080 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109078 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109076 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109073 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109068 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109064 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109061 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109057 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109043 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109015 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109012 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109007 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109006 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109003 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109000 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=108997 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=108994 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=108989 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=108991 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=108984 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=108982 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=108979 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=108975 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=108970 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=108969 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=108967 parent=108890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=108971 parent=108890 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
