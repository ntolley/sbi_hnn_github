distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37809'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46851'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46515'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42767'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34231'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36043'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36755'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36641'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44861'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46649'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34683'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:45105'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:45083'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37411'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46393'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37005'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46211'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46935'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36575'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36147'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42833'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35777'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34469'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34581'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:43089'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42539'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34979'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:43695'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40637'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:39465'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37519'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46679'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:39417'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:41355'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35855'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44083'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:33981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:32897'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:38669'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:33679'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36077'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:33007'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:33251'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:39951'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42041'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:43791'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42669'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:39863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44007'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35095'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:45419'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44977'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34803'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40205'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:33517'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:32917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42081'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40091'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:43263'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:33167'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37735'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37063'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42311'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46399'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:32989'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:37801'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35097'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36049'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40079'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36665'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42501'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44275'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:45159'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:41631'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40023'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:45415'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:38321'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42171'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:46867'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35885'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:33625'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:35721'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44285'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:44277'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:39799'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36739'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34675'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36219'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:36857'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34079'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:43141'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42587'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:34327'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:42923'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:33319'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40033'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.101.116:40353'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:43125
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41023
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38563
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:43125
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41023
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38563
distributed.worker - INFO -          dashboard at:      198.202.101.116:34609
distributed.worker - INFO -          dashboard at:      198.202.101.116:40751
distributed.worker - INFO -          dashboard at:      198.202.101.116:34093
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sdzrtt7y
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pch4uzl3
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gw9l406k
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34633
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34633
distributed.worker - INFO -          dashboard at:      198.202.101.116:43393
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0gy7iao2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:39657
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:39657
distributed.worker - INFO -          dashboard at:      198.202.101.116:36561
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0411j4g8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41563
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41563
distributed.worker - INFO -          dashboard at:      198.202.101.116:46439
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2g4bl3y8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38183
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38183
distributed.worker - INFO -          dashboard at:      198.202.101.116:39287
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-db4bo336
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:40573
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:40573
distributed.worker - INFO -          dashboard at:      198.202.101.116:40227
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-athdnmf0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42863
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42863
distributed.worker - INFO -          dashboard at:      198.202.101.116:33253
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pzrzeqbf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46629
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46629
distributed.worker - INFO -          dashboard at:      198.202.101.116:44671
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_cvt0546
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41423
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41423
distributed.worker - INFO -          dashboard at:      198.202.101.116:36251
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rloy3o65
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:47037
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:47037
distributed.worker - INFO -          dashboard at:      198.202.101.116:40789
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lz_b92d3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:37009
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:37009
distributed.worker - INFO -          dashboard at:      198.202.101.116:36759
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kj3dpddu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:35823
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:35823
distributed.worker - INFO -          dashboard at:      198.202.101.116:42865
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-46s6jun5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41933
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41933
distributed.worker - INFO -          dashboard at:      198.202.101.116:38961
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zow3xg41
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:37047
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:37047
distributed.worker - INFO -          dashboard at:      198.202.101.116:36253
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-da5weo0d
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:43265
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:43265
distributed.worker - INFO -          dashboard at:      198.202.101.116:35767
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-oj4iq9fw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:33053
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:33053
distributed.worker - INFO -          dashboard at:      198.202.101.116:39161
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-nbjb8z6v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38639
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38639
distributed.worker - INFO -          dashboard at:      198.202.101.116:39851
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2ktwk_ur
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:33297
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:33297
distributed.worker - INFO -          dashboard at:      198.202.101.116:45869
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mubp8_ma
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41467
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41467
distributed.worker - INFO -          dashboard at:      198.202.101.116:40387
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ceck4nk5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46125
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46125
distributed.worker - INFO -          dashboard at:      198.202.101.116:40827
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-05bnw7bp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38133
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38133
distributed.worker - INFO -          dashboard at:      198.202.101.116:44391
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mc2r9szw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:47045
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:47045
distributed.worker - INFO -          dashboard at:      198.202.101.116:45829
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ekvsqwrw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46785
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46785
distributed.worker - INFO -          dashboard at:      198.202.101.116:37117
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-99pe7wx8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44699
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44699
distributed.worker - INFO -          dashboard at:      198.202.101.116:36185
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0eug1z0v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46523
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34693
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46523
distributed.worker - INFO -          dashboard at:      198.202.101.116:33393
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34693
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-dcarl1x0
distributed.worker - INFO -          dashboard at:      198.202.101.116:35055
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gw_3u3cl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42009
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42009
distributed.worker - INFO -          dashboard at:      198.202.101.116:40613
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0d9ld7hg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44099
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44099
distributed.worker - INFO -          dashboard at:      198.202.101.116:46075
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ehzk5og8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:39697
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:37491
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:39697
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:37491
distributed.worker - INFO -          dashboard at:      198.202.101.116:46467
distributed.worker - INFO -          dashboard at:      198.202.101.116:46445
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lnxo9cch
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-k90u1tmc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42993
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42993
distributed.worker - INFO -          dashboard at:      198.202.101.116:42281
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-16_yjwf3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44097
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44097
distributed.worker - INFO -          dashboard at:      198.202.101.116:45969
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rb8m5lz4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:45411
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:45411
distributed.worker - INFO -          dashboard at:      198.202.101.116:46089
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bue7fq_o
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46011
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46011
distributed.worker - INFO -          dashboard at:      198.202.101.116:37993
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ytdvief8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36349
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36349
distributed.worker - INFO -          dashboard at:      198.202.101.116:36793
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ybsooy2b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38789
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38789
distributed.worker - INFO -          dashboard at:      198.202.101.116:38019
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ymvw7pw0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:40451
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:40451
distributed.worker - INFO -          dashboard at:      198.202.101.116:43525
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-la19pg15
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41723
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41723
distributed.worker - INFO -          dashboard at:      198.202.101.116:41279
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41025
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0glhdpwk
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41025
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:      198.202.101.116:38867
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-oz1hloex
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41609
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41609
distributed.worker - INFO -          dashboard at:      198.202.101.116:43557
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9k4okj0a
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38147
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38147
distributed.worker - INFO -          dashboard at:      198.202.101.116:39803
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0kxdrwad
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:45285
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:45285
distributed.worker - INFO -          dashboard at:      198.202.101.116:37037
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yqg4ytrm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36605
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36605
distributed.worker - INFO -          dashboard at:      198.202.101.116:41401
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tql47rc3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34271
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34271
distributed.worker - INFO -          dashboard at:      198.202.101.116:45697
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:40491
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7xvsy3h0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:40491
distributed.worker - INFO -          dashboard at:      198.202.101.116:44091
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vxy4h4bh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46379
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46379
distributed.worker - INFO -          dashboard at:      198.202.101.116:33301
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pbio6f4u
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:39377
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:39377
distributed.worker - INFO -          dashboard at:      198.202.101.116:45915
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-c1vgtnf4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46463
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46463
distributed.worker - INFO -          dashboard at:      198.202.101.116:43727
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-z66ff80_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41767
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41767
distributed.worker - INFO -          dashboard at:      198.202.101.116:40321
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41223
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41223
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:      198.202.101.116:41655
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qrwaxpzp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ehtv213p
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:45611
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:45611
distributed.worker - INFO -          dashboard at:      198.202.101.116:42785
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gh5_itm_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:35689
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:35689
distributed.worker - INFO -          dashboard at:      198.202.101.116:41425
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cgn0kvfx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36893
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36893
distributed.worker - INFO -          dashboard at:      198.202.101.116:36279
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-d2hjk4b2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46263
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46263
distributed.worker - INFO -          dashboard at:      198.202.101.116:43889
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w3q5ykw7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:33805
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:33805
distributed.worker - INFO -          dashboard at:      198.202.101.116:41455
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4tkhnwbq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38257
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38257
distributed.worker - INFO -          dashboard at:      198.202.101.116:42899
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4mfx8k9n
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41547
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41547
distributed.worker - INFO -          dashboard at:      198.202.101.116:34963
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44159
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zfm85jqm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44159
distributed.worker - INFO -          dashboard at:      198.202.101.116:41291
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u974asv0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36629
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36629
distributed.worker - INFO -          dashboard at:      198.202.101.116:41109
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sqtp70pf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34371
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34371
distributed.worker - INFO -          dashboard at:      198.202.101.116:40251
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cr4f7ip2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38963
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38963
distributed.worker - INFO -          dashboard at:      198.202.101.116:35991
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1ai179mh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41703
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41703
distributed.worker - INFO -          dashboard at:      198.202.101.116:35527
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-c1bb4t11
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42657
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42657
distributed.worker - INFO -          dashboard at:      198.202.101.116:46611
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-z0qizil3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:39093
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:39093
distributed.worker - INFO -          dashboard at:      198.202.101.116:41457
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ogabgfv6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34681
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34681
distributed.worker - INFO -          dashboard at:      198.202.101.116:33983
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ubi5l3ty
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46701
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46701
distributed.worker - INFO -          dashboard at:      198.202.101.116:46281
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8sobefnz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:43269
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:43269
distributed.worker - INFO -          dashboard at:      198.202.101.116:43707
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-litrmtti
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36577
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36577
distributed.worker - INFO -          dashboard at:      198.202.101.116:36139
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-e1muxz1f
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34135
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34135
distributed.worker - INFO -          dashboard at:      198.202.101.116:43829
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-r8r5l33o
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44875
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44875
distributed.worker - INFO -          dashboard at:      198.202.101.116:43391
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sd6a9nv6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46919
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46919
distributed.worker - INFO -          dashboard at:      198.202.101.116:38507
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bgmd6o0m
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:37749
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:37749
distributed.worker - INFO -          dashboard at:      198.202.101.116:33725
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rsqq9u3w
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:43317
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:43317
distributed.worker - INFO -          dashboard at:      198.202.101.116:46187
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bg64ovpc
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46961
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46961
distributed.worker - INFO -          dashboard at:      198.202.101.116:32871
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-s5zn26ly
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:42283
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:42283
distributed.worker - INFO -          dashboard at:      198.202.101.116:42585
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38227
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-skshj81_
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38227
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:      198.202.101.116:36707
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-suo9fg0h
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:43189
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:43189
distributed.worker - INFO -          dashboard at:      198.202.101.116:40845
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1c9gky81
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44543
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44543
distributed.worker - INFO -          dashboard at:      198.202.101.116:42647
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9hnap2qk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:37309
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:37309
distributed.worker - INFO -          dashboard at:      198.202.101.116:41935
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-malri8mw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46395
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46395
distributed.worker - INFO -          dashboard at:      198.202.101.116:41695
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-eagjz0hr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:41387
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:41387
distributed.worker - INFO -          dashboard at:      198.202.101.116:36761
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pcn2fuvl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:35557
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:35557
distributed.worker - INFO -          dashboard at:      198.202.101.116:39891
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4qg51ouz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:37459
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:37459
distributed.worker - INFO -          dashboard at:      198.202.101.116:42303
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ur5ux8pa
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:46007
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:46007
distributed.worker - INFO -          dashboard at:      198.202.101.116:35617
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mi4v_0nq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:39469
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:39469
distributed.worker - INFO -          dashboard at:      198.202.101.116:32905
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6jfs9ing
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:39429
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:39429
distributed.worker - INFO -          dashboard at:      198.202.101.116:36255
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3ezvugaw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:36769
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:36769
distributed.worker - INFO -          dashboard at:      198.202.101.116:40095
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sismr4bk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:40547
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:40547
distributed.worker - INFO -          dashboard at:      198.202.101.116:34447
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1d_2vxvz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:39043
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:39043
distributed.worker - INFO -          dashboard at:      198.202.101.116:41903
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-17fe3noo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:33659
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:33659
distributed.worker - INFO -          dashboard at:      198.202.101.116:42513
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0lp5eozv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:33633
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:33633
distributed.worker - INFO -          dashboard at:      198.202.101.116:35067
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-94l5s5sx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34795
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34795
distributed.worker - INFO -          dashboard at:      198.202.101.116:43091
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-aahkwf2s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:34533
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:34533
distributed.worker - INFO -          dashboard at:      198.202.101.116:40159
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ttg768hn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:45069
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:45069
distributed.worker - INFO -          dashboard at:      198.202.101.116:33379
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-dlj5roop
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:38943
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:38943
distributed.worker - INFO -          dashboard at:      198.202.101.116:33117
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-s5nspfsf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:40409
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:40409
distributed.worker - INFO -          dashboard at:      198.202.101.116:44579
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-41imkx7i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44483
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44483
distributed.worker - INFO -          dashboard at:      198.202.101.116:34745
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2p452bxx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.101.116:44909
distributed.worker - INFO -          Listening to: tcp://198.202.101.116:44909
distributed.worker - INFO -          dashboard at:      198.202.101.116:45227
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p_112k4r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.128:40487 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.128:40487 after 10 s
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.128:40487 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.128:40487 after 10 s
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.128:40487 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.128:40487 after 10 s
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.128:40487 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.128:40487 after 10 s
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.128:40487 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.128:40487 after 10 s
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:43125
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41023
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34633
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38563
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:39657
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34231'
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46515'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42767'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37809'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46851'
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.128:40487
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36043'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36755'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40917'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41563
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36641'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:40573
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44861'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46649'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38183
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34683'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41423
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46629
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:45105'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42863
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:45083'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:47037
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37411'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44863'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:37009
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:35823
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46393'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41933
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37005'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:37047
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46211'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:43265
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:33053
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38639
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46935'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36575'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36147'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:33297
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42833'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46125
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35777'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41467
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34469'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34581'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38133
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:47045
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:43089'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42539'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34693
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46785
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34979'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44699
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:43695'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46523
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42009
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:37491
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40637'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:39465'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37519'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:39697
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46679'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44099
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:39417'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42993
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:41355'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44097
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35855'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46011
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44083'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36349
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:33981'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:45411
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:32897'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:38669'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:40451
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38789
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:33679'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41025
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36077'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41723
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:33007'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:45285
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:33251'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41609
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:39951'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38147
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42041'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36605
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:43791'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46379
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42669'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34271
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:39863'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:40491
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44007'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:45611
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41767
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35095'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:45419'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41223
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:39377
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44977'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34803'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36893
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:35689
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46463
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40205'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:33517'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:32917'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41547
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42081'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46263
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44159
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40091'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:43263'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38257
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:33167'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:33805
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37735'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34371
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37063'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36629
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42311'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41703
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46399'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:43269
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:32989'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38963
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:37801'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:39093
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35097'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46919
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36049'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36577
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40079'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42657
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36665'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34681
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42501'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38227
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44875
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46961
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44275'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:45159'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:41631'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46701
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40023'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34135
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35401'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:43317
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:45415'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:42283
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:38321'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:37749
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42171'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44543
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:46867'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:41387
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35885'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:37309
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:33625'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:35557
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:35721'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:43189
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44285'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:40547
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:44277'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:45069
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:39799'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46395
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:46007
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36739'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:37459
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34675'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36219'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:36769
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:36857'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:39469
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34079'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:39043
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:43141'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34795
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42587'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44483
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:34327'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:39429
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:42923'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:34533
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:33319'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:33659
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40033'
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:40409
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.101.116:40353'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:44909
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:38943
distributed.worker - INFO - Stopping worker at tcp://198.202.101.116:33633
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41381 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41376 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41374 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41385 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41372 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41378 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41370 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41382 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41367 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41301 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41366 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41230 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41223 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41219 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41215 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41226 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41213 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41205 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41201 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41198 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41132 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41129 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41126 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41122 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41119 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41116 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41006 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41113 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41044 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41084 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41003 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40924 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40908 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40891 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40837 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40830 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40834 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40827 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40825 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40817 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40821 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40811 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40807 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40805 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40799 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40791 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40788 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40796 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40784 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40778 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40780 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40775 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40771 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40766 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40763 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40762 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40758 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40755 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40748 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40751 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40745 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40740 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40737 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40733 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40731 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40726 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40723 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40719 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40715 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40711 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40708 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40704 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40700 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40695 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40691 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40686 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40683 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40679 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40674 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40667 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40670 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40664 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40659 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40657 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40654 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40651 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40648 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40645 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40640 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40642 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40635 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40633 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40627 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40630 parent=40537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40623 parent=40537 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
