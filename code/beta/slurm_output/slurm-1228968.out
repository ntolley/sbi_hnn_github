distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:42577'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:35369'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:37193'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:46277'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:44941'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:43859'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:40819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:43607'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:46795'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:42871'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:44189'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:44529'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:42165'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:39543'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:37181'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:42147'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:33161'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:42807'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:46235'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:43253'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:43537'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:37473'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:40491'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:35741'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:35557'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:35385'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:44803'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:32999'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:42219'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:35531'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:41445'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:35127'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:38717'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:34633'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:45363'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:43365'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:38853'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:33341'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:41425'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:33817'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:38043'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:36289'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:41205'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:44501'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:36545'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:42861'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:46009'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:43801'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:42089'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:34029'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:40567'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:45773'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:40661'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:35523'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:40205'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:46269'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:32917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:40519'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:44755'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:45123'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:37371'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:43693'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:33973'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:45031'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:35629'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:39111'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:38519'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:43491'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:42671'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:35173'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:45611'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:40867'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:36457'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:45883'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:43473'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:46945'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:36325'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:38475'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:34119'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:34087'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:43185'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:36201'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:46051'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:38615'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:35461'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:45169'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:36477'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:38953'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:36773'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:43403'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:36999'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:36905'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:45071'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:43415'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:37353'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:35337'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:36173'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:36543'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:41771'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.183:42023'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:36707
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:36707
distributed.worker - INFO -          dashboard at:      198.202.103.183:38895
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-58m3n6zq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:45469
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:45469
distributed.worker - INFO -          dashboard at:      198.202.103.183:39921
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-55sj_418
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:45971
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:45971
distributed.worker - INFO -          dashboard at:      198.202.103.183:40505
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-52pg1y2s
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:42203
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:42203
distributed.worker - INFO -          dashboard at:      198.202.103.183:42873
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:35747
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gqqbp08h
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:35747
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:      198.202.103.183:33637
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:35569
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:35569
distributed.worker - INFO -          dashboard at:      198.202.103.183:43635
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-aqz7so1c
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ek5r9ihv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:32965
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:32965
distributed.worker - INFO -          dashboard at:      198.202.103.183:34171
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-oshvfmu9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:44199
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:44199
distributed.worker - INFO -          dashboard at:      198.202.103.183:32981
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hyjlggnx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:34147
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:34147
distributed.worker - INFO -          dashboard at:      198.202.103.183:40881
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ml1hp44b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:46827
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:46827
distributed.worker - INFO -          dashboard at:      198.202.103.183:35665
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bgw1wt52
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:45427
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:45427
distributed.worker - INFO -          dashboard at:      198.202.103.183:43201
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0v1cegdq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:40639
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:40639
distributed.worker - INFO -          dashboard at:      198.202.103.183:34951
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mjiwq8x4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:40465
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:40465
distributed.worker - INFO -          dashboard at:      198.202.103.183:43893
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2d6a5y6b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:38037
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:38037
distributed.worker - INFO -          dashboard at:      198.202.103.183:32855
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9f6nfcdf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:36571
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:36571
distributed.worker - INFO -          dashboard at:      198.202.103.183:43145
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zk1pr4fn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:35907
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:35907
distributed.worker - INFO -          dashboard at:      198.202.103.183:45131
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mushc2ik
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:35433
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:35433
distributed.worker - INFO -          dashboard at:      198.202.103.183:38271
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:46407
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:46407
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:      198.202.103.183:45107
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mq79x5ha
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ag9cr_z2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:40021
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:40021
distributed.worker - INFO -          dashboard at:      198.202.103.183:35751
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-o5n60qve
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:43057
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:43057
distributed.worker - INFO -          dashboard at:      198.202.103.183:39655
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uya12861
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:37043
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:37043
distributed.worker - INFO -          dashboard at:      198.202.103.183:41791
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u3jlz_8l
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:37611
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:37611
distributed.worker - INFO -          dashboard at:      198.202.103.183:42243
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-l22xpk3e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:40713
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:40713
distributed.worker - INFO -          dashboard at:      198.202.103.183:38179
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wl54lsy0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:40741
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:40741
distributed.worker - INFO -          dashboard at:      198.202.103.183:44335
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-omiilr7l
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:36079
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:36079
distributed.worker - INFO -          dashboard at:      198.202.103.183:34211
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rd64ycdc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:44967
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:44967
distributed.worker - INFO -          dashboard at:      198.202.103.183:33363
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_kv96c9v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:34055
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:34055
distributed.worker - INFO -          dashboard at:      198.202.103.183:39793
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jfc25ng6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:33661
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:33661
distributed.worker - INFO -          dashboard at:      198.202.103.183:35105
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xchg131r
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:46941
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:46941
distributed.worker - INFO -          dashboard at:      198.202.103.183:35177
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1xlgz6pg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:44083
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:44083
distributed.worker - INFO -          dashboard at:      198.202.103.183:36121
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w2er8u2t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:40467
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:40467
distributed.worker - INFO -          dashboard at:      198.202.103.183:38667
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8v5kpydf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:35989
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:35989
distributed.worker - INFO -          dashboard at:      198.202.103.183:34111
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-afyp7zyl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:33045
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:33045
distributed.worker - INFO -          dashboard at:      198.202.103.183:33719
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gogz4g54
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:42187
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:42187
distributed.worker - INFO -          dashboard at:      198.202.103.183:41163
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u2hnblin
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:37639
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:37639
distributed.worker - INFO -          dashboard at:      198.202.103.183:45125
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hyc3iri5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:39037
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:39037
distributed.worker - INFO -          dashboard at:      198.202.103.183:39675
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7nmwhkff
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:43251
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:43251
distributed.worker - INFO -          dashboard at:      198.202.103.183:44719
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-udoro_2j
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:33223
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:33223
distributed.worker - INFO -          dashboard at:      198.202.103.183:33399
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wtjb6xyn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:38635
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:38635
distributed.worker - INFO -          dashboard at:      198.202.103.183:41811
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lo5y04mp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:39001
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:39001
distributed.worker - INFO -          dashboard at:      198.202.103.183:38163
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-aix6u8er
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:43007
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:43007
distributed.worker - INFO -          dashboard at:      198.202.103.183:39509
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-flqiphxl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:34953
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:34953
distributed.worker - INFO -          dashboard at:      198.202.103.183:46503
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ef06u_7z
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:33319
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:33319
distributed.worker - INFO -          dashboard at:      198.202.103.183:37457
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2n1det30
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:39013
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:39013
distributed.worker - INFO -          dashboard at:      198.202.103.183:42093
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0owso63z
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:34919
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:34919
distributed.worker - INFO -          dashboard at:      198.202.103.183:40789
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5jiqvl8h
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:37573
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:37573
distributed.worker - INFO -          dashboard at:      198.202.103.183:46785
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-my9mfoff
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:44961
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:44961
distributed.worker - INFO -          dashboard at:      198.202.103.183:36209
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mkxp36r7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:39235
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:39235
distributed.worker - INFO -          dashboard at:      198.202.103.183:33051
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ztw7hzei
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:40949
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:40949
distributed.worker - INFO -          dashboard at:      198.202.103.183:39317
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-x5j8r0ey
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:45819
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:45819
distributed.worker - INFO -          dashboard at:      198.202.103.183:38477
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ozweditv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:34497
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:34497
distributed.worker - INFO -          dashboard at:      198.202.103.183:43571
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-46325wbb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:43095
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:43095
distributed.worker - INFO -          dashboard at:      198.202.103.183:44141
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-gidu5rmh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:33279
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:33279
distributed.worker - INFO -          dashboard at:      198.202.103.183:43003
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9a9x9ruu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:36783
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:36783
distributed.worker - INFO -          dashboard at:      198.202.103.183:37965
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1bdt6oso
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:46161
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:46161
distributed.worker - INFO -          dashboard at:      198.202.103.183:33193
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0hjz5aq_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:46991
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:46991
distributed.worker - INFO -          dashboard at:      198.202.103.183:46859
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xmr92x_1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:45461
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:45461
distributed.worker - INFO -          dashboard at:      198.202.103.183:39881
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-269nqn78
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:33523
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:33523
distributed.worker - INFO -          dashboard at:      198.202.103.183:42701
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0qdyapk6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:43695
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:43695
distributed.worker - INFO -          dashboard at:      198.202.103.183:43453
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:46811
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:46811
distributed.worker - INFO -          dashboard at:      198.202.103.183:43153
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wn7n6mcd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9cwdoyxu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:40427
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:40427
distributed.worker - INFO -          dashboard at:      198.202.103.183:34513
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6ijnxmdb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:38165
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:38165
distributed.worker - INFO -          dashboard at:      198.202.103.183:41793
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_n0kpyd4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:42311
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:42311
distributed.worker - INFO -          dashboard at:      198.202.103.183:40493
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zq5_0ht9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:45189
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:45189
distributed.worker - INFO -          dashboard at:      198.202.103.183:33025
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-t1n62uze
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:34917
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:34917
distributed.worker - INFO -          dashboard at:      198.202.103.183:40253
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tc_ku2zq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:45905
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:45905
distributed.worker - INFO -          dashboard at:      198.202.103.183:44853
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-t68swfbm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:34507
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:34507
distributed.worker - INFO -          dashboard at:      198.202.103.183:38099
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ja6wvy44
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:47093
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:47093
distributed.worker - INFO -          dashboard at:      198.202.103.183:34509
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_6yr2xrh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:43669
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:43669
distributed.worker - INFO -          dashboard at:      198.202.103.183:32789
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4ivn8tnc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:44345
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:44345
distributed.worker - INFO -          dashboard at:      198.202.103.183:38283
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rqhotfze
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:35705
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:35705
distributed.worker - INFO -          dashboard at:      198.202.103.183:42189
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-djhbn84u
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:46587
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:46587
distributed.worker - INFO -          dashboard at:      198.202.103.183:43823
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tbscsn34
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:45227
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:45227
distributed.worker - INFO -          dashboard at:      198.202.103.183:32849
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:33689
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:33689
distributed.worker - INFO -          dashboard at:      198.202.103.183:39943
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-un5wq01f
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-f8w4vwm2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:33543
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:33543
distributed.worker - INFO -          dashboard at:      198.202.103.183:35483
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-89p8g5la
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:39837
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:39837
distributed.worker - INFO -          dashboard at:      198.202.103.183:34585
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-h9ifz_lr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:33621
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:33621
distributed.worker - INFO -          dashboard at:      198.202.103.183:42581
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pnoynr6l
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:37949
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:37949
distributed.worker - INFO -          dashboard at:      198.202.103.183:35153
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7uuj6g4y
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:45151
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:45151
distributed.worker - INFO -          dashboard at:      198.202.103.183:36777
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-l6opne1d
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:42673
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:42673
distributed.worker - INFO -          dashboard at:      198.202.103.183:37995
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xadf2khy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:35723
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:35723
distributed.worker - INFO -          dashboard at:      198.202.103.183:46411
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-461u24bp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:38725
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:38725
distributed.worker - INFO -          dashboard at:      198.202.103.183:34641
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n53o045z
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:47045
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:47045
distributed.worker - INFO -          dashboard at:      198.202.103.183:40769
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-k538w4r1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:39349
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:39349
distributed.worker - INFO -          dashboard at:      198.202.103.183:42151
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6wrlsm2c
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:37219
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:37219
distributed.worker - INFO -          dashboard at:      198.202.103.183:46555
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-s69_w8h6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:42207
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:42207
distributed.worker - INFO -          dashboard at:      198.202.103.183:33645
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w25bcuiu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:42357
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:42357
distributed.worker - INFO -          dashboard at:      198.202.103.183:43189
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4_i_cbmk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:36845
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:36845
distributed.worker - INFO -          dashboard at:      198.202.103.183:36059
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bgt70wa1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:39463
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:39463
distributed.worker - INFO -          dashboard at:      198.202.103.183:43229
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ov1g_mrn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:45221
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:45221
distributed.worker - INFO -          dashboard at:      198.202.103.183:44395
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lyscqx2w
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:42783
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:42783
distributed.worker - INFO -          dashboard at:      198.202.103.183:38307
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9cdkhfc4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:37707
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:37707
distributed.worker - INFO -          dashboard at:      198.202.103.183:40897
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bm6fnczh
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:45261
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:45261
distributed.worker - INFO -          dashboard at:      198.202.103.183:37195
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-z473cj3b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:34501
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:34501
distributed.worker - INFO -          dashboard at:      198.202.103.183:40191
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7_1fhkj3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:32979
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:32979
distributed.worker - INFO -          dashboard at:      198.202.103.183:46425
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-orrfbgf2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:39885
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:39885
distributed.worker - INFO -          dashboard at:      198.202.103.183:42929
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8m_k07bx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:43495
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:43495
distributed.worker - INFO -          dashboard at:      198.202.103.183:34657
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9mch5m7a
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:37079
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:37079
distributed.worker - INFO -          dashboard at:      198.202.103.183:43981
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_npjeda5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:43223
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:43223
distributed.worker - INFO -          dashboard at:      198.202.103.183:40507
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bamoitqj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.183:43879
distributed.worker - INFO -          Listening to: tcp://198.202.103.183:43879
distributed.worker - INFO -          dashboard at:      198.202.103.183:34615
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qgz4co18
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca30>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca30>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca00>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:40639
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:35569
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:45427
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:36707
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:42203
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:34147
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:35747
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:45971
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:40465
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:44199
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:45469
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:40713
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:36079
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:40741
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:46827
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:32965
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:36571
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:35907
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:35433
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:37043
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:37611
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:46407
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:40021
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:43057
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:38037
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:42147'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:40819'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:42165'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:35385'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:43859'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:32917'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:44189'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:35369'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:46795'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:33161'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:44529'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:36173'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:37193'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:43607'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:46277'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:43253'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:42871'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:44941'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:42807'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:37181'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:43537'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:40491'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:39543'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:42577'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:35557'
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:46235'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:37473'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:35741'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:44967
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:46941
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:44803'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:32999'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:34055
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:33661
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:42219'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:35531'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:44083
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:41445'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:33045
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:35127'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:40467
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:38717'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:42187
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:34633'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:35989
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:45363'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:39001
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:43365'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:39037
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:38853'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:37639
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:33341'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:38635
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:41425'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:33223
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:33817'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:43251
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:34953
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:38043'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:36289'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:39013
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:43007
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:41205'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:33319
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:34919
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:44501'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:36545'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:42861'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:44961
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:46009'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:37573
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:43801'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:39235
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:42089'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:33279
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:34029'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:40949
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:40567'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:45819
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:45773'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:40661'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:43095
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:34497
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:35523'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:36783
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:40205'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:33523
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:46269'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:46161
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:40519'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:46811
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:44755'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:45461
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:45123'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:46991
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:37371'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:40427
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:43693'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:42311
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:33973'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:47093
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:45189
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:38165
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:45031'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:35629'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:39111'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:43695
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:38519'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:34917
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:45905
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:43491'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:42671'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:34507
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:35173'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:33689
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:45611'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:43669
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:40867'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:39837
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:36457'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:44345
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:45883'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:35705
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:46587
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:43473'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:46945'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:45227
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:45151
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:36325'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:38475'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:33621
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:34119'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:33543
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:34087'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:37949
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:42357
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:43185'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:36201'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:39349
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:46051'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:42207
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:38615'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:35723
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:35461'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:47045
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:45169'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:38725
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:36477'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:42673
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:38953'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:37219
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:36773'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:45261
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:43403'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:45221
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:36999'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:37707
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:36905'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:39463
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:45071'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:42783
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:43415'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:36845
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:37353'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:39885
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:35337'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:32979
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:36543'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:43495
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:41771'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:34501
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.183:42023'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:43879
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:43223
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.183:37079
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100353 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100351 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100349 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100344 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100343 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100341 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100339 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100337 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100334 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100332 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100330 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100322 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100325 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100318 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100314 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100316 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100311 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100309 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100305 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100299 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100302 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100293 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100296 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100286 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100288 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100281 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100283 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100275 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100269 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100271 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100267 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100264 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100261 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100256 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100252 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100248 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100198 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100195 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100190 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100191 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100187 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100182 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100178 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100179 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100172 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100173 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100166 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100162 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100158 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100164 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100155 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100150 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100147 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100144 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100142 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100138 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100135 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100127 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100120 parent=99985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=100111 parent=99985 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
