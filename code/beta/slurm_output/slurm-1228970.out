distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:37539'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:45803'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:45729'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:43053'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:40243'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:40111'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:38605'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:42383'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:36251'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:46807'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:42463'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:39017'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:43445'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:37275'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:46147'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:40541'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:36345'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:38105'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:46257'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:41981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:33227'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:40703'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:38871'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:39077'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:32851'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:42095'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:42019'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:37773'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:33047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:35245'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:40645'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:43363'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:37177'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:37671'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:41151'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:44997'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:42835'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:42327'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:39299'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:41523'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:38963'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:40085'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:44515'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:34493'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:36365'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:42587'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:41481'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:45823'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:36277'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:41841'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:33437'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:42571'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:34779'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:45345'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:41287'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:33789'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:38151'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:35135'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:43349'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:43087'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:37383'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:37917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:36541'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:33509'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:40871'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:39371'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:42679'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:44969'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:34701'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:34543'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:38201'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:37855'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:42893'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:39063'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:36413'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:38533'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:44677'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:37895'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:37179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:41043'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:33499'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:38527'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:33059'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:40593'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:44051'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:46801'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:34367'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:35633'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:40373'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:44475'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:39459'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:43721'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:43453'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:34303'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:40221'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:46621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:33149'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:45219'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:42041'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.69:39949'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:46803
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:37635
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:46803
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:37635
distributed.worker - INFO -          dashboard at:       198.202.103.69:36673
distributed.worker - INFO -          dashboard at:       198.202.103.69:39047
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-c7isi27f
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-uly3q4od
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:37193
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:37193
distributed.worker - INFO -          dashboard at:       198.202.103.69:37977
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_l0reoec
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:46485
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:46485
distributed.worker - INFO -          dashboard at:       198.202.103.69:43307
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-h389h9hy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:33319
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:33319
distributed.worker - INFO -          dashboard at:       198.202.103.69:44579
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:37975
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:37975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:       198.202.103.69:41353
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pznm69h_
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2xfec552
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:40695
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:40695
distributed.worker - INFO -          dashboard at:       198.202.103.69:34785
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:42017
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vc69sien
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:39321
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:42017
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:39321
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:       198.202.103.69:46149
distributed.worker - INFO -          dashboard at:       198.202.103.69:33871
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:44849
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:44849
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4fmco24k
distributed.worker - INFO -          dashboard at:       198.202.103.69:41425
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5kgmk3kd
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-c33klz2_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:34341
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:34341
distributed.worker - INFO -          dashboard at:       198.202.103.69:46873
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w8qvjqy4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:46513
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:46513
distributed.worker - INFO -          dashboard at:       198.202.103.69:39019
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-th8her1_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:44305
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:44305
distributed.worker - INFO -          dashboard at:       198.202.103.69:43181
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7a5d0toq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:45101
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:45101
distributed.worker - INFO -          dashboard at:       198.202.103.69:43801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lq1abxp5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:40797
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:40797
distributed.worker - INFO -          dashboard at:       198.202.103.69:46015
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-pv_abxlm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:37741
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:37741
distributed.worker - INFO -          dashboard at:       198.202.103.69:33571
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-h5yi9qhq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:43185
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:43185
distributed.worker - INFO -          dashboard at:       198.202.103.69:36677
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-y3ldiqw1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:45361
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:45361
distributed.worker - INFO -          dashboard at:       198.202.103.69:34027
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-oe04_vwk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:36787
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:36787
distributed.worker - INFO -          dashboard at:       198.202.103.69:44575
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xh5lxy7w
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:43139
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:43139
distributed.worker - INFO -          dashboard at:       198.202.103.69:45577
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vtc58cjb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:33009
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:33009
distributed.worker - INFO -          dashboard at:       198.202.103.69:35025
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-raxn45lx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:42889
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:42889
distributed.worker - INFO -          dashboard at:       198.202.103.69:37209
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-iwrq_oos
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:39795
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:39795
distributed.worker - INFO -          dashboard at:       198.202.103.69:40581
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-orz0jho4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:40833
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:40833
distributed.worker - INFO -          dashboard at:       198.202.103.69:43599
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-s4wg94h8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:33079
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:33079
distributed.worker - INFO -          dashboard at:       198.202.103.69:36281
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9mv9r0_b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:35587
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:35587
distributed.worker - INFO -          dashboard at:       198.202.103.69:37125
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4dhpdrvt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:45977
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:45977
distributed.worker - INFO -          dashboard at:       198.202.103.69:34653
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xo0z0tip
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:42523
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:42523
distributed.worker - INFO -          dashboard at:       198.202.103.69:39941
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-59xazh3t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:46699
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:46699
distributed.worker - INFO -          dashboard at:       198.202.103.69:46919
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-45fe4iz8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:41299
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:41299
distributed.worker - INFO -          dashboard at:       198.202.103.69:39869
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-otczz556
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:34083
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:34083
distributed.worker - INFO -          dashboard at:       198.202.103.69:38763
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fgpbx729
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:45191
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:45191
distributed.worker - INFO -          dashboard at:       198.202.103.69:44021
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-t3lhvw_k
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:39625
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:39625
distributed.worker - INFO -          dashboard at:       198.202.103.69:44531
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hbath0n2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:37463
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:37463
distributed.worker - INFO -          dashboard at:       198.202.103.69:36695
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w1dp127_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:40649
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:40649
distributed.worker - INFO -          dashboard at:       198.202.103.69:38307
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-dvo_e22v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:43979
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:43979
distributed.worker - INFO -          dashboard at:       198.202.103.69:38943
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-o2lgpfr3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:45177
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:45177
distributed.worker - INFO -          dashboard at:       198.202.103.69:44683
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8xhb9ae9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:34041
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:34041
distributed.worker - INFO -          dashboard at:       198.202.103.69:35451
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n6ymqaj_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:36967
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:36967
distributed.worker - INFO -          dashboard at:       198.202.103.69:45137
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cs974vhq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:36579
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:36579
distributed.worker - INFO -          dashboard at:       198.202.103.69:40911
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:34067
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:34067
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6jz1kvlv
distributed.worker - INFO -          dashboard at:       198.202.103.69:33889
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-da6lh87w
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:45863
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:45863
distributed.worker - INFO -          dashboard at:       198.202.103.69:44815
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cvym0_zu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:42161
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:42161
distributed.worker - INFO -          dashboard at:       198.202.103.69:38551
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-r70umwz3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:34571
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:34571
distributed.worker - INFO -          dashboard at:       198.202.103.69:33585
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-umnrlgev
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:33501
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:33501
distributed.worker - INFO -          dashboard at:       198.202.103.69:45699
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ny142baj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:37251
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:37251
distributed.worker - INFO -          dashboard at:       198.202.103.69:37317
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-tweo9u6s
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:42165
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:42165
distributed.worker - INFO -          dashboard at:       198.202.103.69:47033
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lze3fsno
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:38541
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:38541
distributed.worker - INFO -          dashboard at:       198.202.103.69:43351
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9xk_2e4e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:40895
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:40895
distributed.worker - INFO -          dashboard at:       198.202.103.69:38707
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-i4f9z8iu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:36545
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:36545
distributed.worker - INFO -          dashboard at:       198.202.103.69:34121
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ziksjtid
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:43197
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:43197
distributed.worker - INFO -          dashboard at:       198.202.103.69:45163
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-w2cjftan
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:39799
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:39799
distributed.worker - INFO -          dashboard at:       198.202.103.69:36231
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bu10w13z
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:42479
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:42479
distributed.worker - INFO -          dashboard at:       198.202.103.69:46851
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9d0f98be
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:36215
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:36215
distributed.worker - INFO -          dashboard at:       198.202.103.69:36797
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-chi0zvoi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:45831
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:45831
distributed.worker - INFO -          dashboard at:       198.202.103.69:38781
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-q5kf301y
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:39851
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:39851
distributed.worker - INFO -          dashboard at:       198.202.103.69:38689
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-e7g3ii0_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:39853
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:39853
distributed.worker - INFO -          dashboard at:       198.202.103.69:40217
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-b3ve3m5v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:40631
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:40631
distributed.worker - INFO -          dashboard at:       198.202.103.69:46057
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-23fciq1u
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:34813
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:34813
distributed.worker - INFO -          dashboard at:       198.202.103.69:34011
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:42003
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n3ugz9b_
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:42003
distributed.worker - INFO -          dashboard at:       198.202.103.69:33217
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-oqd4gra8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:34385
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:34385
distributed.worker - INFO -          dashboard at:       198.202.103.69:33333
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-y16wgb5e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:46659
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:46659
distributed.worker - INFO -          dashboard at:       198.202.103.69:46499
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-o0uax11n
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:34259
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:34259
distributed.worker - INFO -          dashboard at:       198.202.103.69:36065
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jvy_qolg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:43191
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:43191
distributed.worker - INFO -          dashboard at:       198.202.103.69:34311
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u3eewqd0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:35929
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:35929
distributed.worker - INFO -          dashboard at:       198.202.103.69:42191
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jwx7jogq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:42957
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:42957
distributed.worker - INFO -          dashboard at:       198.202.103.69:44809
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-rvt2x5eq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:34671
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:34671
distributed.worker - INFO -          dashboard at:       198.202.103.69:33629
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-b2tjr1xf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:44901
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:44901
distributed.worker - INFO -          dashboard at:       198.202.103.69:41169
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-t8e6z43x
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:36009
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:36009
distributed.worker - INFO -          dashboard at:       198.202.103.69:33555
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-usi6_tc7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:38691
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:38691
distributed.worker - INFO -          dashboard at:       198.202.103.69:44031
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-27c171cm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:46661
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:46661
distributed.worker - INFO -          dashboard at:       198.202.103.69:41013
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-icy1xvko
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:36621
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:36621
distributed.worker - INFO -          dashboard at:       198.202.103.69:42489
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-i08v7m96
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:36559
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:36559
distributed.worker - INFO -          dashboard at:       198.202.103.69:41121
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-djqkn4i3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:46259
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:46259
distributed.worker - INFO -          dashboard at:       198.202.103.69:38829
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sw8z8_od
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:46785
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:46785
distributed.worker - INFO -          dashboard at:       198.202.103.69:39759
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-79qk_3m8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:38169
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:38169
distributed.worker - INFO -          dashboard at:       198.202.103.69:39813
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ew2jswv6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:37959
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:37959
distributed.worker - INFO -          dashboard at:       198.202.103.69:42925
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8rx5szfe
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:41483
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:41483
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:37655
distributed.worker - INFO -          dashboard at:       198.202.103.69:46721
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:37655
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:       198.202.103.69:37239
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-el5f3hl9
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ni1sqz2a
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:39253
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:39253
distributed.worker - INFO -          dashboard at:       198.202.103.69:34659
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-keokobkw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:44803
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:44803
distributed.worker - INFO -          dashboard at:       198.202.103.69:44707
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-my6gywyb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:45173
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:45173
distributed.worker - INFO -          dashboard at:       198.202.103.69:44713
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mrtaqrzo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:33741
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:33741
distributed.worker - INFO -          dashboard at:       198.202.103.69:34565
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8adyp2j3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:41075
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:41075
distributed.worker - INFO -          dashboard at:       198.202.103.69:39601
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-s0516qy1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:32769
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:32769
distributed.worker - INFO -          dashboard at:       198.202.103.69:45501
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zqhmrar0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:43871
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:43871
distributed.worker - INFO -          dashboard at:       198.202.103.69:34111
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-zgi4bs67
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:41871
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:41871
distributed.worker - INFO -          dashboard at:       198.202.103.69:43877
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-s2dl29ap
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:38021
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:38021
distributed.worker - INFO -          dashboard at:       198.202.103.69:34875
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-q4be29b6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:33361
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:33361
distributed.worker - INFO -          dashboard at:       198.202.103.69:46009
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sjoup503
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:41359
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:41359
distributed.worker - INFO -          dashboard at:       198.202.103.69:33517
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-stry1pmo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:36293
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:36293
distributed.worker - INFO -          dashboard at:       198.202.103.69:41817
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3w8zeyqp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:35221
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:35221
distributed.worker - INFO -          dashboard at:       198.202.103.69:44211
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qz_4_01r
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:33439
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:33439
distributed.worker - INFO -          dashboard at:       198.202.103.69:42231
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5gm3i3d5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:39963
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:39963
distributed.worker - INFO -          dashboard at:       198.202.103.69:42033
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xnpta2q_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:35079
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:35079
distributed.worker - INFO -          dashboard at:       198.202.103.69:37063
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5kv2oukm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:33609
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:33609
distributed.worker - INFO -          dashboard at:       198.202.103.69:39319
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-iwa_qwnt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:36629
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:36629
distributed.worker - INFO -          dashboard at:       198.202.103.69:46079
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wkj07nfg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:38363
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:38363
distributed.worker - INFO -          dashboard at:       198.202.103.69:43165
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-k1wpj4ac
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:33607
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:33607
distributed.worker - INFO -          dashboard at:       198.202.103.69:43581
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-io0qg99e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.69:33587
distributed.worker - INFO -          Listening to: tcp://198.202.103.69:33587
distributed.worker - INFO -          dashboard at:       198.202.103.69:42955
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8w2kbvgh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca30>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca60>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca30>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca30>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca30>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca30>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca30>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca30>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca30>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/iostream.py", line 867, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/iostream.py", line 1140, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 122, in convert_stream_closed_error
    raise CommClosedError(
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5ca30>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/iostream.py", line 867, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/iostream.py", line 1140, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 122, in convert_stream_closed_error
    raise CommClosedError(
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:34083
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:43185
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:46803
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:46513
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:45361
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:37635
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:37975
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:40797
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:44849
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:39321
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:36787
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:45101
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:33319
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:46485
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:34341
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:41299
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:37193
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:42017
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:40695
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:37741
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:44305
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:33009
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:45977
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:35587
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:39795
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:33079
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:42889
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:43139
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:40833
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:46699
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:42523
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:46147'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:42095'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:37539'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:42463'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:40111'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:42019'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:37773'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:40243'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:45729'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:40541'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:39017'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:39077'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:37275'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:36345'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:38871'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:43363'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:33227'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:33047'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:45803'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:38605'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:46257'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:42383'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:43053'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:46807'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:36251'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:45823'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:43445'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:41981'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:40645'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:38105'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:40703'
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:32851'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:35245'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:37177'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:39625
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:37671'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:45191
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:45177
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:41151'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:44997'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:37463
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:42835'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:34041
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:42327'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:40649
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:39299'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:43979
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:41523'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:37251
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:38963'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:36967
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:34571
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:33501
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:40085'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:44515'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:34493'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:42165
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:36365'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:45863
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:42587'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:36545
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:41481'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:34067
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:36277'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:38541
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:41841'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:42161
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:33437'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:40895
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:43197
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:39799
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:42571'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:34779'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:45345'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:36579
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:41287'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:45831
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:33789'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:42479
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:38151'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:39851
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:35135'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:36215
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:43349'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:39853
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:43087'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:40631
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:34259
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:37383'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:37917'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:46659
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:36541'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:42003
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:33509'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:34671
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:40871'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:34813
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:39371'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:43191
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:42679'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:36009
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:44969'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:36621
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:34701'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:34385
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:34543'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:44901
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:38201'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:42957
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:37855'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:36559
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:46661
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:38691
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:42893'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:39063'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:36413'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:46259
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:38533'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:35929
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:44677'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:41483
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:37895'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:38169
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:37179'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:46785
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:41043'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:37959
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:33499'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:32769
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:38527'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:44803
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:33059'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:37655
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:40593'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:39253
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:44051'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:33741
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:41075
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:46801'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:34367'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:43871
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:35633'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:45173
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:40373'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:38021
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:44475'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:41359
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:39459'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:36293
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:43721'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:41871
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:43453'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:35221
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:34303'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:33361
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:40221'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:33439
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:46621'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:38363
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:35079
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:33149'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:45219'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:33609
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:42041'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:33587
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.69:39949'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:39963
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:33607
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.69:36629
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23315 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23309 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23307 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23311 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23313 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23304 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23300 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23295 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23291 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23302 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23288 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23297 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23285 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23282 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23279 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23275 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23273 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23267 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23265 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23262 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23257 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23255 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23251 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23246 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23242 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23244 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23238 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23235 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23232 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23228 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23224 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23221 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23217 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23215 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23209 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23207 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23213 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23201 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23198 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23195 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23193 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23191 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23180 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23174 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23179 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23177 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23185 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23173 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23165 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23160 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23157 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23150 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23145 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23143 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23139 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23133 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23135 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23130 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23125 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23123 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23120 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23117 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23110 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23109 parent=22924 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23105 parent=22924 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
