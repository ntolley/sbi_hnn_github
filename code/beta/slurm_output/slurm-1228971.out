distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:43987'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:43451'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36813'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:41099'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:38289'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:43841'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:32911'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:46939'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:42849'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:33525'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:35805'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36803'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:43843'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36361'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36441'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36647'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:34533'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:37313'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:40657'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:35689'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:42985'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:34797'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:44647'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:38379'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:44909'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:35687'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:33503'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:39849'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:34659'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:46795'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:43857'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:33905'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:35791'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:41615'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:40883'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:35895'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:40313'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:46435'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:44761'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36327'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:33369'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:43607'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:39899'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:45427'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:37375'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:45321'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:34717'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:39835'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:32959'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:44813'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:39301'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:37453'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:40147'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:35569'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36945'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:39819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:41289'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:40167'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:42349'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:43447'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:44003'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:33297'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:42725'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:38973'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:42479'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:38305'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:42379'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:40645'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:42317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:46959'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36509'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36881'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:38801'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:44741'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:37993'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:45979'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:42679'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:38967'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:41679'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:40513'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:35453'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:34769'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:34911'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36039'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:42473'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:39365'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:44229'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:39451'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:46089'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36425'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:45737'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:39195'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:38437'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:39187'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:38613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:44479'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:37953'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.219:36665'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:40469
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:38807
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:40469
distributed.worker - INFO -          dashboard at:      198.202.102.219:43799
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:38807
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO -          dashboard at:      198.202.102.219:40577
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-b2gzs329
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1mtui70n
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:34271
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:34271
distributed.worker - INFO -          dashboard at:      198.202.102.219:36145
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-seolw6ke
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:38439
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:38439
distributed.worker - INFO -          dashboard at:      198.202.102.219:40265
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-emoybbeg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:32945
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:32945
distributed.worker - INFO -          dashboard at:      198.202.102.219:34035
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hs1qeqkl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:42279
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:42279
distributed.worker - INFO -          dashboard at:      198.202.102.219:41257
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9y8cmqs7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:33929
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:33929
distributed.worker - INFO -          dashboard at:      198.202.102.219:35199
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9y8lljp5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:33129
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:33129
distributed.worker - INFO -          dashboard at:      198.202.102.219:40579
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cir7orbq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:34791
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:34575
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:34791
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:34575
distributed.worker - INFO -          dashboard at:      198.202.102.219:43179
distributed.worker - INFO -          dashboard at:      198.202.102.219:42261
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5icr_uks
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bs7k_oaj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:39033
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:39033
distributed.worker - INFO -          dashboard at:      198.202.102.219:37135
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hc4l7e6w
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:38227
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:38227
distributed.worker - INFO -          dashboard at:      198.202.102.219:44389
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0rc2489t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:40259
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:40259
distributed.worker - INFO -          dashboard at:      198.202.102.219:42405
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-k47y9fbo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:41571
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:41571
distributed.worker - INFO -          dashboard at:      198.202.102.219:38683
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-25u1_r_p
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:35191
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:35191
distributed.worker - INFO -          dashboard at:      198.202.102.219:39677
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-v8pxh47s
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:32999
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:32999
distributed.worker - INFO -          dashboard at:      198.202.102.219:46325
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9kmv65mh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:38275
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:38275
distributed.worker - INFO -          dashboard at:      198.202.102.219:40245
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vbtb3fq7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:35865
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:35865
distributed.worker - INFO -          dashboard at:      198.202.102.219:37585
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-x77jqaae
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:39489
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:39489
distributed.worker - INFO -          dashboard at:      198.202.102.219:44359
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-il0pjnzf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:34809
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:34809
distributed.worker - INFO -          dashboard at:      198.202.102.219:36165
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6brb85z6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:41019
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:41019
distributed.worker - INFO -          dashboard at:      198.202.102.219:37253
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jjfmiucc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:34475
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:34475
distributed.worker - INFO -          dashboard at:      198.202.102.219:40491
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kzyk8sq_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:45559
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:45559
distributed.worker - INFO -          dashboard at:      198.202.102.219:40779
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ls9l8lw6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:34559
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:34559
distributed.worker - INFO -          dashboard at:      198.202.102.219:45959
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fdh_0zwm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:38817
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:38817
distributed.worker - INFO -          dashboard at:      198.202.102.219:43229
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-0mwtd_d_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:41635
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:41635
distributed.worker - INFO -          dashboard at:      198.202.102.219:46047
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jkjh8fkh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:33389
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:33389
distributed.worker - INFO -          dashboard at:      198.202.102.219:42513
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-klzyb9ck
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:40741
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:40741
distributed.worker - INFO -          dashboard at:      198.202.102.219:32815
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bnqcllxn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:33793
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:33793
distributed.worker - INFO -          dashboard at:      198.202.102.219:45137
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-p64h8zd_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:36849
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:36849
distributed.worker - INFO -          dashboard at:      198.202.102.219:43233
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7_km1kyb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:42911
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:42911
distributed.worker - INFO -          dashboard at:      198.202.102.219:38111
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4ioda3lf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:46887
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:46887
distributed.worker - INFO -          dashboard at:      198.202.102.219:37773
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5dx2is6v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:40947
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:40947
distributed.worker - INFO -          dashboard at:      198.202.102.219:43117
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-j4u2_jpj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:40933
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:40933
distributed.worker - INFO -          dashboard at:      198.202.102.219:33501
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6o6n4fu4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:44953
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:44953
distributed.worker - INFO -          dashboard at:      198.202.102.219:41627
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4jsnp4sc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:39837
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:39837
distributed.worker - INFO -          dashboard at:      198.202.102.219:40099
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6m62fb7f
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:42547
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:42547
distributed.worker - INFO -          dashboard at:      198.202.102.219:36343
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4os9gdvx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:44887
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:35059
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:44887
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:35059
distributed.worker - INFO -          dashboard at:      198.202.102.219:45921
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO -          dashboard at:      198.202.102.219:35215
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yxpo5vfc
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-kbw5cwio
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:41609
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:41609
distributed.worker - INFO -          dashboard at:      198.202.102.219:44603
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ne_01nta
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:37299
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:37299
distributed.worker - INFO -          dashboard at:      198.202.102.219:43267
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qgmyx3wh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:46765
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:46765
distributed.worker - INFO -          dashboard at:      198.202.102.219:37619
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-lez6903s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:39647
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:39647
distributed.worker - INFO -          dashboard at:      198.202.102.219:41381
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-oot8btr6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:36273
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:36273
distributed.worker - INFO -          dashboard at:      198.202.102.219:37909
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6dq4yrkz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:45011
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:45011
distributed.worker - INFO -          dashboard at:      198.202.102.219:39273
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-bonyd7t7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:35259
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:35259
distributed.worker - INFO -          dashboard at:      198.202.102.219:37489
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sxnedvkv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:38247
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:38247
distributed.worker - INFO -          dashboard at:      198.202.102.219:39423
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-49cbnakm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:36563
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:36563
distributed.worker - INFO -          dashboard at:      198.202.102.219:42877
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fxgda2tj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:41049
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:41049
distributed.worker - INFO -          dashboard at:      198.202.102.219:39505
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-cvfu6g5l
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:41235
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:41235
distributed.worker - INFO -          dashboard at:      198.202.102.219:35299
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-hd048fdr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:36867
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:36867
distributed.worker - INFO -          dashboard at:      198.202.102.219:34925
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-htgxi_if
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:34205
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:34205
distributed.worker - INFO -          dashboard at:      198.202.102.219:39889
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-_mv6r9mh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:33505
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:33505
distributed.worker - INFO -          dashboard at:      198.202.102.219:32807
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wpesgb6r
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:43829
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:43829
distributed.worker - INFO -          dashboard at:      198.202.102.219:46975
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-7ecwcnxe
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:32917
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:32917
distributed.worker - INFO -          dashboard at:      198.202.102.219:46809
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xgj0dtr4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:38769
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:38769
distributed.worker - INFO -          dashboard at:      198.202.102.219:33631
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-k0t3813j
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:38575
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:38575
distributed.worker - INFO -          dashboard at:      198.202.102.219:33739
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-u5e7r66o
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:40325
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:40325
distributed.worker - INFO -          dashboard at:      198.202.102.219:33235
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-t7way7xt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:43327
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:43327
distributed.worker - INFO -          dashboard at:      198.202.102.219:36613
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-xv64i_e5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:41183
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:41183
distributed.worker - INFO -          dashboard at:      198.202.102.219:46401
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-en012pz3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:44527
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:44527
distributed.worker - INFO -          dashboard at:      198.202.102.219:46363
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n4q6m3b4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:44377
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:44377
distributed.worker - INFO -          dashboard at:      198.202.102.219:43111
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4zawv3kx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:34353
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:34353
distributed.worker - INFO -          dashboard at:      198.202.102.219:37419
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-fqfncq1n
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:37877
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:37877
distributed.worker - INFO -          dashboard at:      198.202.102.219:46453
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:46847
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-h4ddjnxi
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:46847
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:      198.202.102.219:34139
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-2jy115fr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:33811
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:33811
distributed.worker - INFO -          dashboard at:      198.202.102.219:35795
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4myme4al
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:36155
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:36155
distributed.worker - INFO -          dashboard at:      198.202.102.219:42123
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-jdmfa3se
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:41497
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:41497
distributed.worker - INFO -          dashboard at:      198.202.102.219:40857
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-9odwdcuv
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:41697
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:41697
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:      198.202.102.219:46023
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vo6mbdp4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:40251
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:40251
distributed.worker - INFO -          dashboard at:      198.202.102.219:42909
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-h_4iy7bw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:34517
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:34517
distributed.worker - INFO -          dashboard at:      198.202.102.219:36623
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-5hl2h8il
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:37505
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:37505
distributed.worker - INFO -          dashboard at:      198.202.102.219:46843
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-mdddu9hc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:45985
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:45985
distributed.worker - INFO -          dashboard at:      198.202.102.219:43131
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-dzv3v62u
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:38389
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:38389
distributed.worker - INFO -          dashboard at:      198.202.102.219:39155
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-a8i0nyjc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:41821
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:41821
distributed.worker - INFO -          dashboard at:      198.202.102.219:43301
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-86l6z_0d
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:42079
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:42079
distributed.worker - INFO -          dashboard at:      198.202.102.219:45429
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-r88oelzi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:33535
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:33535
distributed.worker - INFO -          dashboard at:      198.202.102.219:34679
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-3cn59970
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:35071
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:35071
distributed.worker - INFO -          dashboard at:      198.202.102.219:45333
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-sle8gz5x
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:39969
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:39969
distributed.worker - INFO -          dashboard at:      198.202.102.219:36825
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-toabyos2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:37463
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:37463
distributed.worker - INFO -          dashboard at:      198.202.102.219:43099
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-52ihnby7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:36057
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:36057
distributed.worker - INFO -          dashboard at:      198.202.102.219:44481
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-1wj6ptwo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:38085
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:38085
distributed.worker - INFO -          dashboard at:      198.202.102.219:33181
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-6fzabqwt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:35139
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:35139
distributed.worker - INFO -          dashboard at:      198.202.102.219:43051
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-a3e4s37z
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:40957
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:40957
distributed.worker - INFO -          dashboard at:      198.202.102.219:37635
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8pbd7qow
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:41803
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:41803
distributed.worker - INFO -          dashboard at:      198.202.102.219:37963
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-89zp58f6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:38921
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:38921
distributed.worker - INFO -          dashboard at:      198.202.102.219:38219
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-omspmadn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:37219
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:37219
distributed.worker - INFO -          dashboard at:      198.202.102.219:43129
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qiqfoe55
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:34671
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:34671
distributed.worker - INFO -          dashboard at:      198.202.102.219:45307
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-wecn9ik5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:42683
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:42683
distributed.worker - INFO -          dashboard at:      198.202.102.219:36805
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-a0rzx7gn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:41885
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:41885
distributed.worker - INFO -          dashboard at:      198.202.102.219:37327
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-vkc26qts
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:33261
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:33261
distributed.worker - INFO -          dashboard at:      198.202.102.219:47039
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-dqb8z0lu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:35417
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:35417
distributed.worker - INFO -          dashboard at:      198.202.102.219:44061
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-n0wj_4x1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:39965
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:39965
distributed.worker - INFO -          dashboard at:      198.202.102.219:38309
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-qayo8ze5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:41081
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:41081
distributed.worker - INFO -          dashboard at:      198.202.102.219:41441
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-k8_oz0ts
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:35327
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:35327
distributed.worker - INFO -          dashboard at:      198.202.102.219:44607
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-yxg9idfy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:42975
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:42975
distributed.worker - INFO -          dashboard at:      198.202.102.219:45779
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4t1jgor4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:33487
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:33487
distributed.worker - INFO -          dashboard at:      198.202.102.219:39833
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-ncl0zu2k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:44171
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:44171
distributed.worker - INFO -          dashboard at:      198.202.102.219:45171
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-83q1pri9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:38353
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:38353
distributed.worker - INFO -          dashboard at:      198.202.102.219:38273
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-4or3gfne
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.219:33977
distributed.worker - INFO -          Listening to: tcp://198.202.102.219:33977
distributed.worker - INFO -          dashboard at:      198.202.102.219:43543
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/beta/dask-worker-space/dask-worker-space/worker-8_q0lvx5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.234:37799
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 187, in read
    n_frames = await stream.read_bytes(8)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc)) from exc
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x1554c9a5caf0>>, <Task finished name='Task-11' coro=<Worker.heartbeat() done, defined at /home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py:930> exception=OSError('Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s')>)
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/iostream.py", line 867, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/iostream.py", line 1140, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 491, in wait_for
    return fut.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 202, in read
    convert_stream_closed_error(self, e)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/tcp.py", line 122, in convert_stream_closed_error
    raise CommClosedError(
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 976, in heartbeat
    raise e
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/worker.py", line 936, in heartbeat
    response = await retry_operation(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 384, in retry_operation
    return await retry(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/utils_comm.py", line 369, in retry
    return await coro()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 855, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 1006, in connect
    comm = await connect(
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/comm/core.py", line 324, in connect
    raise IOError(
OSError: Timed out during handshake while connecting to tcp://198.202.103.234:37799 after 10 s
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:32945
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:35865
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:38439
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:40469
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:38275
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:42279
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:38807
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:34271
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:34575
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:39033
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:33129
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:33929
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:40259
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:38227
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:41571
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:35191
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:34791
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:32999
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36803'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36813'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:43857'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:42849'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:39849'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:42479'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:34659'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:46939'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:41615'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:38379'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:43841'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:46795'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:41099'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:43987'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:43843'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:37313'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:34769'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:40513'
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.234:37799
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:43451'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:38289'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:32911'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:42911
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:33525'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:42547
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:35805'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:34559
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36361'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:36849
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36441'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:34475
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:40741
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36647'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:34533'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:35059
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:41635
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:40657'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:35689'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:45559
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:42985'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:44953
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:34797'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:40933
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:44647'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:38817
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:44909'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:46765
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:35687'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:33389
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:33503'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:41609
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:33905'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:33793
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:35791'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:46887
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:40883'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:41019
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:35895'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:44887
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:40313'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:36273
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:46435'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:39647
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:44761'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:39837
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36327'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:35259
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:33369'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:40947
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:43607'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:37299
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:39899'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:38247
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:45427'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:45011
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:37375'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:41049
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:45321'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:36563
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:34717'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:34205
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:39835'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:36867
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:32959'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:41235
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:44813'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:39301'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:43829
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:32917
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:37453'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:40325
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:40147'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:38575
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:35569'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:33505
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36945'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:38769
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:39819'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:43327
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:41289'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:34353
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:40167'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:44527
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:42349'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:44377
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:41497
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:43447'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:44003'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:41183
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:33297'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:37877
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:42725'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:45985
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:38973'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:36155
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36917'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:33811
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:41697
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:38305'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:42379'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:34517
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:40645'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:46847
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:42317'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:35071
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:46959'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:37505
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36509'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:33535
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36881'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:37463
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36317'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:40251
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:38801'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:38389
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:44741'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:42079
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:37993'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:41821
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:45979'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:36057
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:42679'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:38921
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:38967'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:40957
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:34671
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:41679'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:35453'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:39969
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:34911'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:41885
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36039'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:41803
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:42473'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:35139
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:39365'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:33261
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:44229'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:38085
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:39451'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:37219
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:46089'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:35327
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36425'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:35417
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:45737'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:39965
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:39195'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:34809
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:38437'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:44171
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:39187'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:42683
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:38613'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:41081
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:39489
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:33487
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:44479'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:37953'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.219:36665'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:42975
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:38353
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.219:33977
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110851 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110849 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110845 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110847 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110843 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110837 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110835 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110840 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110839 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110833 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110824 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110831 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110827 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110820 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110813 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110815 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110808 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110798 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110790 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110800 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110796 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110787 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110793 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110775 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110784 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110771 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110780 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110769 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110762 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110766 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110756 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110758 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110749 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110747 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110738 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110744 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110740 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110735 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110728 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110721 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110725 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110717 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110723 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110715 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110712 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110706 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110703 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110701 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110697 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110693 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110690 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110686 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110682 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110681 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110677 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110674 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110671 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110667 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110662 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110661 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110664 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110656 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110648 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110649 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110633 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110627 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110630 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110619 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110618 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110610 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110611 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110609 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110602 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110599 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110596 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110593 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110583 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110580 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110572 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110566 parent=110483 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110557 parent=110483 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
