distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:33949'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:44011'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:45665'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:39979'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:39097'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:45291'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:35407'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:42213'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:36881'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:35443'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:39033'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:35493'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:46297'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:34705'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:41031'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:38511'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:40439'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:41343'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:42559'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:46519'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:42657'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:37267'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:46685'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:39501'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:47031'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:35681'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:37345'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:40693'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:34159'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:46803'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:35147'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:41651'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:39471'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:38257'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:39333'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:45081'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:42865'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:45251'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:34607'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:34313'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:44501'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:44267'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:34013'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:45159'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:44403'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:36969'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:35073'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:33787'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:35375'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.112:34745'
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:34737
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:45741
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:40185
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:33549
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:33583
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:33549
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:45741
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:34737
distributed.worker - INFO -          dashboard at:      198.202.103.112:33223
distributed.worker - INFO -          dashboard at:      198.202.103.112:40013
distributed.worker - INFO -          dashboard at:      198.202.103.112:45925
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:32949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:33583
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:36657
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:40185
distributed.worker - INFO -          dashboard at:      198.202.103.112:44115
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO -          dashboard at:      198.202.103.112:43773
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:36577
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:35289
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:36657
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:32949
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:35289
distributed.worker - INFO -          dashboard at:      198.202.103.112:36769
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-5aavuc8n
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -          dashboard at:      198.202.103.112:41871
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-to620e13
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO -          dashboard at:      198.202.103.112:40799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-_2uzp103
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-l5e56us3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:36577
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:      198.202.103.112:39243
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-qnmqdm6y
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-fqseeid5
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-v4m4n5lg
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-_08b6jff
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-1u_w69pk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:38605
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:38605
distributed.worker - INFO -          dashboard at:      198.202.103.112:39229
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ljo1vgyo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:34537
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:34537
distributed.worker - INFO -          dashboard at:      198.202.103.112:34875
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-rr7wps6_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:43165
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:43165
distributed.worker - INFO -          dashboard at:      198.202.103.112:36377
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-29wvcm3t
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:43543
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:43543
distributed.worker - INFO -          dashboard at:      198.202.103.112:34653
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-m3lo2oyb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:42503
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:42503
distributed.worker - INFO -          dashboard at:      198.202.103.112:35313
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-d5hv1vqy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:39103
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:39103
distributed.worker - INFO -          dashboard at:      198.202.103.112:34413
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-eu5go69x
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:38557
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:38557
distributed.worker - INFO -          dashboard at:      198.202.103.112:46121
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-6i5lw8z_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:44153
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:44153
distributed.worker - INFO -          dashboard at:      198.202.103.112:34215
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-on4vrv_6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:41751
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:41751
distributed.worker - INFO -          dashboard at:      198.202.103.112:45861
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-d55w9uuq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:42169
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:42169
distributed.worker - INFO -          dashboard at:      198.202.103.112:44759
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-btnk8m43
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:35637
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:35637
distributed.worker - INFO -          dashboard at:      198.202.103.112:36461
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-50tuw6h0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:33413
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:33413
distributed.worker - INFO -          dashboard at:      198.202.103.112:36681
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-haqlovz7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:44273
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:44273
distributed.worker - INFO -          dashboard at:      198.202.103.112:45613
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-up80_821
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:34499
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:34499
distributed.worker - INFO -          dashboard at:      198.202.103.112:45137
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-6uqras9p
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:45583
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:45583
distributed.worker - INFO -          dashboard at:      198.202.103.112:34239
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-7brtj8se
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:35615
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:35615
distributed.worker - INFO -          dashboard at:      198.202.103.112:39431
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-clz0wfdy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:33483
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:33483
distributed.worker - INFO -          dashboard at:      198.202.103.112:36213
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-e560lbpf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:35355
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:35355
distributed.worker - INFO -          dashboard at:      198.202.103.112:38971
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-_rq091d9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:41759
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:41759
distributed.worker - INFO -          dashboard at:      198.202.103.112:46163
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-u1labqw8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:33963
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:33963
distributed.worker - INFO -          dashboard at:      198.202.103.112:37669
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-s65nwq75
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:46897
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:46897
distributed.worker - INFO -          dashboard at:      198.202.103.112:37683
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-3h3518a6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:38775
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:38775
distributed.worker - INFO -          dashboard at:      198.202.103.112:45423
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-4_opb66a
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:38417
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:38417
distributed.worker - INFO -          dashboard at:      198.202.103.112:37661
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-qlpt__ca
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:41915
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:41915
distributed.worker - INFO -          dashboard at:      198.202.103.112:40379
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-iwmyd_7i
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:46335
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:46335
distributed.worker - INFO -          dashboard at:      198.202.103.112:36745
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-li7cfs37
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:35577
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:35577
distributed.worker - INFO -          dashboard at:      198.202.103.112:45353
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-7mpnxage
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:46445
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:46445
distributed.worker - INFO -          dashboard at:      198.202.103.112:33883
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-obg3ldym
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:38451
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:38451
distributed.worker - INFO -          dashboard at:      198.202.103.112:34739
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-7zmnht3p
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:33793
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:33793
distributed.worker - INFO -          dashboard at:      198.202.103.112:46397
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ebhj9dsq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:46609
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:46609
distributed.worker - INFO -          dashboard at:      198.202.103.112:45057
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-8jjde_dw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:45939
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:45939
distributed.worker - INFO -          dashboard at:      198.202.103.112:41043
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-v4eu_k_g
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:35519
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:35519
distributed.worker - INFO -          dashboard at:      198.202.103.112:44355
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-gesgmgty
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:34087
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:34087
distributed.worker - INFO -          dashboard at:      198.202.103.112:34115
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-m_4pu0og
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:45557
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:45557
distributed.worker - INFO -          dashboard at:      198.202.103.112:35815
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vtfgy8t1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:42353
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:42353
distributed.worker - INFO -          dashboard at:      198.202.103.112:43421
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:41649
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-bmn5_jr8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:41649
distributed.worker - INFO -          dashboard at:      198.202.103.112:41927
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-9dd0y_w7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:45479
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:45479
distributed.worker - INFO -          dashboard at:      198.202.103.112:38231
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-imujta3r
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:35479
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:35479
distributed.worker - INFO -          dashboard at:      198.202.103.112:36209
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-49havr1m
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:36719
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:36719
distributed.worker - INFO -          dashboard at:      198.202.103.112:45105
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-tl0c3jpu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:34179
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:34179
distributed.worker - INFO -          dashboard at:      198.202.103.112:46695
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-116lslre
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.112:39475
distributed.worker - INFO -          Listening to: tcp://198.202.103.112:39475
distributed.worker - INFO -          dashboard at:      198.202.103.112:37371
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ownpsms3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:34547
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:38775
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:46609
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:43543
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:34537
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:35479
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:38417
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:42353
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:42503
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:38451
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:45557
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:34087
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:36657
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:42169
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:41649
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:44273
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:45939
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:33413
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:44153
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:39475
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:35615
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:32949
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:35577
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:33793
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:33583
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:34737
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:46445
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:45479
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:33963
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:33549
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:45583
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:46897
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:34179
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:38605
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:36577
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:40185
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:35637
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:41759
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:46335
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:35519
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:41915
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:41751
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:43165
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:36719
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:33483
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:34499
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:38557
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:35289
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:45741
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:39103
distributed.worker - INFO - Stopping worker at tcp://198.202.103.112:35355
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:35147'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:47031'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:34705'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:35073'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:45251'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:44403'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:42559'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:46803'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:42213'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:45665'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:46519'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:39979'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:37267'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:39333'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:44011'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:33787'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:38257'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:39033'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:46297'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:37345'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:34159'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:44267'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:41651'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:36969'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:45159'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:38511'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:34013'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:39097'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:41031'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:40693'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:42865'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:34607'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:45291'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:35375'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:39471'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:34313'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:36881'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:39501'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:35407'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:42657'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:40439'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:35681'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:33949'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:41343'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:44501'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:45081'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:46685'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:35443'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:35493'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.112:34745'
distributed.dask_worker - INFO - End worker
