distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:43231'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:35269'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:44261'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:35457'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:38573'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:45307'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:39023'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:37453'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:33363'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:35283'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:38001'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:45883'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:34023'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:45723'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:41849'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:45121'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:34805'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:43891'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:33439'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:35019'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:44437'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:46531'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:42411'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:34871'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:35163'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:40105'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:42543'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:40745'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:45951'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:37271'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:39779'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:43981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:36891'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:42815'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:41557'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:37329'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:38071'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:33297'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:40601'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:37617'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:34999'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:36761'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:44063'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:35533'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:32855'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:35821'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:46787'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:36971'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:44903'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.215:36361'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:44139
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:44139
distributed.worker - INFO -          dashboard at:      198.202.102.215:45613
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-tbsnp_ur
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:41225
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:41225
distributed.worker - INFO -          dashboard at:      198.202.102.215:32915
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-znw6j156
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:43765
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:43765
distributed.worker - INFO -          dashboard at:      198.202.102.215:40907
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-dswekunr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:46275
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:46275
distributed.worker - INFO -          dashboard at:      198.202.102.215:44547
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-_ey2pr62
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:39401
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:39401
distributed.worker - INFO -          dashboard at:      198.202.102.215:44769
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-g9ftvqt_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:33609
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:33609
distributed.worker - INFO -          dashboard at:      198.202.102.215:41351
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-r7mohgfe
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:34745
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:34745
distributed.worker - INFO -          dashboard at:      198.202.102.215:40375
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-6pjzuuy9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:37375
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:37375
distributed.worker - INFO -          dashboard at:      198.202.102.215:46601
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-079qadn3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:38877
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:38877
distributed.worker - INFO -          dashboard at:      198.202.102.215:41363
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-k0gyl38m
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:44771
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:44771
distributed.worker - INFO -          dashboard at:      198.202.102.215:37475
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-9uf06prf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:45675
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:45675
distributed.worker - INFO -          dashboard at:      198.202.102.215:37445
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-dr2ddjx5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:34357
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:34357
distributed.worker - INFO -          dashboard at:      198.202.102.215:38633
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-inqpp87j
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:41699
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:41699
distributed.worker - INFO -          dashboard at:      198.202.102.215:40239
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-wulkgc1q
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:33401
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:33401
distributed.worker - INFO -          dashboard at:      198.202.102.215:35979
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-u9uuxyvc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:36183
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:36183
distributed.worker - INFO -          dashboard at:      198.202.102.215:42217
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-mru3wt_i
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:42867
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:42867
distributed.worker - INFO -          dashboard at:      198.202.102.215:44605
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-l5dxbrgu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:33083
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:33083
distributed.worker - INFO -          dashboard at:      198.202.102.215:46863
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-99p1m2ln
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:41429
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:41429
distributed.worker - INFO -          dashboard at:      198.202.102.215:41161
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-jvjtg07e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:35799
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:35799
distributed.worker - INFO -          dashboard at:      198.202.102.215:42611
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-v4qvu3qk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:34371
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:34371
distributed.worker - INFO -          dashboard at:      198.202.102.215:36415
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-rnxogy1t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:38519
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:38519
distributed.worker - INFO -          dashboard at:      198.202.102.215:34527
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-muargisd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:45527
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:45527
distributed.worker - INFO -          dashboard at:      198.202.102.215:43477
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-blx1dnsf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:36587
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:36587
distributed.worker - INFO -          dashboard at:      198.202.102.215:37747
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-3fbp5s5d
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:46367
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:46367
distributed.worker - INFO -          dashboard at:      198.202.102.215:32775
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-d6dg8sbw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:39319
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:39319
distributed.worker - INFO -          dashboard at:      198.202.102.215:46679
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-qz8qy0u_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:40951
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:40951
distributed.worker - INFO -          dashboard at:      198.202.102.215:44947
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-r1xszm3r
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:45319
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:45319
distributed.worker - INFO -          dashboard at:      198.202.102.215:43847
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-r79lav1f
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:35629
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:35629
distributed.worker - INFO -          dashboard at:      198.202.102.215:34423
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-0mce6fdf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:37509
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:37509
distributed.worker - INFO -          dashboard at:      198.202.102.215:42399
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-r33yy2fc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:33639
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:33639
distributed.worker - INFO -          dashboard at:      198.202.102.215:35641
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-wu3d1zl_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:41271
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:41271
distributed.worker - INFO -          dashboard at:      198.202.102.215:40431
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:32957
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:32957
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:      198.202.102.215:38645
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-esx29zck
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-cg3q17ju
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:35671
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:35671
distributed.worker - INFO -          dashboard at:      198.202.102.215:33093
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-wniybtfa
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:45531
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:45531
distributed.worker - INFO -          dashboard at:      198.202.102.215:45259
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-1h5tymw7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:35395
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:35395
distributed.worker - INFO -          dashboard at:      198.202.102.215:38373
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-scoijxy2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:37929
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:37929
distributed.worker - INFO -          dashboard at:      198.202.102.215:41245
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kuun7qts
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:44861
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:44861
distributed.worker - INFO -          dashboard at:      198.202.102.215:37215
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:34473
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:34473
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -          dashboard at:      198.202.102.215:38169
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-_q218_2t
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-gz1n9u00
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:33377
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:33377
distributed.worker - INFO -          dashboard at:      198.202.102.215:34977
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-driuvtz4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:40843
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:40843
distributed.worker - INFO -          dashboard at:      198.202.102.215:33271
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ucxfbtap
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:46329
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:46329
distributed.worker - INFO -          dashboard at:      198.202.102.215:38603
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-hvzwc84g
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:44879
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:44879
distributed.worker - INFO -          dashboard at:      198.202.102.215:43929
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-8yo0lwd7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:33417
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:33417
distributed.worker - INFO -          dashboard at:      198.202.102.215:39535
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-hv_tx8x8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:38161
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:38161
distributed.worker - INFO -          dashboard at:      198.202.102.215:43229
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-_gg4t3sy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:46007
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:46007
distributed.worker - INFO -          dashboard at:      198.202.102.215:45457
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-zf1fii_n
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:41671
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:41671
distributed.worker - INFO -          dashboard at:      198.202.102.215:44597
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-jvle_yux
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:41031
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:41031
distributed.worker - INFO -          dashboard at:      198.202.102.215:39963
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-7gtgc644
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:37943
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:37943
distributed.worker - INFO -          dashboard at:      198.202.102.215:34755
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-8nh57utl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:45711
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:45711
distributed.worker - INFO -          dashboard at:      198.202.102.215:43107
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vu558kyy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.215:46711
distributed.worker - INFO -          Listening to: tcp://198.202.102.215:46711
distributed.worker - INFO -          dashboard at:      198.202.102.215:37417
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.00 GB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-26ga6h8e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://198.202.103.184:39127
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:46711
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:33417
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:38161
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:35629
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:41031
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:41671
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:33639
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:35671
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:45527
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:45675
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:37943
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:45711
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:45531
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:37509
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:38519
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:33377
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:46367
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:34371
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:44861
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:42867
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:37929
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:44879
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:37375
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:34473
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:46007
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:38877
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:36587
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:40843
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:35799
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:46329
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:35395
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:34357
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:39401
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:33401
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:32957
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:39319
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:41225
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:33609
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:40951
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:33083
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:45319
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:41271
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:41699
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:43765
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:41429
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:36183
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:46275
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:34745
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:44771
distributed.worker - INFO - Stopping worker at tcp://198.202.102.215:44139
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.core - INFO - Event loop was unresponsive in Nanny for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:36361'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:37617'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:39023'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:36761'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:46787'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:42543'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:40745'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:39779'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:45307'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:33297'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:34805'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:44903'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:38001'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:35457'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:37271'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:40601'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:37453'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:45951'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:36891'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:41557'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:44437'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:44261'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:41849'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:34999'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:45121'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:45883'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:42411'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:35269'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:45723'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:42815'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:43891'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:35533'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:37329'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:33439'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:35283'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:35019'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:46531'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:38573'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:35163'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:40105'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:43231'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:36971'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:32855'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:38071'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:44063'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:33363'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:34023'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:43981'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:35821'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.215:34871'
distributed.dask_worker - INFO - End worker
