distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:36279'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:33577'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:44431'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:43323'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:34699'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:43129'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:43501'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:45379'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:43223'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:46719'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:43611'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:37557'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:41183'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:32805'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:40301'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:34443'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:35989'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:43013'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:34921'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:37255'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:42827'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:40673'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:46615'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:45435'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:36749'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:42733'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:44433'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:35061'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:43237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:38187'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:42665'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:33869'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:36819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:36001'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:42353'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:35899'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:45157'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:40531'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:43993'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:32781'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:38799'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:43073'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:35189'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:36891'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:39211'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:35179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:32807'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:33995'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:34919'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:41423'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:35615'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:39411'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:46695'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:35301'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:38643'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:37581'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:36831'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:45493'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:38851'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:34515'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:45905'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:42951'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:45063'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:40989'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:42245'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:44523'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:44193'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:45667'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:45561'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:35073'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:40933'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:33771'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:42051'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:46713'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:34129'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:44239'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:33579'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:41485'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:43049'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:37219'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:33649'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:42777'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:38895'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:33355'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:41247'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:32867'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:47053'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:47005'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:34669'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:40049'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:33271'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:40497'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:39295'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:38621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:39265'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:40239'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:39881'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:40027'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:34347'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.102.216:37691'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:43315
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:47007
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:43315
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:47007
distributed.worker - INFO -          dashboard at:      198.202.102.216:45395
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO -          dashboard at:      198.202.102.216:44647
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-mmh6hk0e
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-tu0hpmaz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:36533
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:36533
distributed.worker - INFO -          dashboard at:      198.202.102.216:35383
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-e4hgbp5x
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:46681
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:46681
distributed.worker - INFO -          dashboard at:      198.202.102.216:44037
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-3f69nlkj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:38499
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:38499
distributed.worker - INFO -          dashboard at:      198.202.102.216:38807
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-p0h978nl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:36329
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:36329
distributed.worker - INFO -          dashboard at:      198.202.102.216:33353
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-o8e4m5t7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:38091
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:38091
distributed.worker - INFO -          dashboard at:      198.202.102.216:37453
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-57anay2u
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:42865
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:42865
distributed.worker - INFO -          dashboard at:      198.202.102.216:36823
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-hasq43vl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:38571
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:38571
distributed.worker - INFO -          dashboard at:      198.202.102.216:39681
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-drm4be4j
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:34445
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:34445
distributed.worker - INFO -          dashboard at:      198.202.102.216:37527
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-aqby9wqw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:38349
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:38349
distributed.worker - INFO -          dashboard at:      198.202.102.216:44465
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-4vvowt6n
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:45885
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:45885
distributed.worker - INFO -          dashboard at:      198.202.102.216:36239
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ne_9xduz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:43383
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:43383
distributed.worker - INFO -          dashboard at:      198.202.102.216:36719
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-f40kb3uu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:38251
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:38251
distributed.worker - INFO -          dashboard at:      198.202.102.216:46263
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ff6mt8qn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:38891
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:38891
distributed.worker - INFO -          dashboard at:      198.202.102.216:44897
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-nv75deeq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:44061
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:44061
distributed.worker - INFO -          dashboard at:      198.202.102.216:45231
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ndcswjyo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:41871
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:41871
distributed.worker - INFO -          dashboard at:      198.202.102.216:33389
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-nffokcjb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:44443
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:44443
distributed.worker - INFO -          dashboard at:      198.202.102.216:46281
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kgzg_vre
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:37803
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:37803
distributed.worker - INFO -          dashboard at:      198.202.102.216:41351
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-azhm1jdq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:45725
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:45725
distributed.worker - INFO -          dashboard at:      198.202.102.216:44641
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-pc5bcrk8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:33535
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:33535
distributed.worker - INFO -          dashboard at:      198.202.102.216:35745
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-1ztyh8zs
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:37683
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:37683
distributed.worker - INFO -          dashboard at:      198.202.102.216:45167
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-76wsaj14
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:43057
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:43057
distributed.worker - INFO -          dashboard at:      198.202.102.216:44345
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-4p_i8ewg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:44113
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:44113
distributed.worker - INFO -          dashboard at:      198.202.102.216:36707
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-0o92ahtw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:38153
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:38153
distributed.worker - INFO -          dashboard at:      198.202.102.216:36147
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-wz5ybggf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:40951
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:40951
distributed.worker - INFO -          dashboard at:      198.202.102.216:45199
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-3gxyd36_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:37839
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:37839
distributed.worker - INFO -          dashboard at:      198.202.102.216:36319
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-k4d202tt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:34753
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:34753
distributed.worker - INFO -          dashboard at:      198.202.102.216:37667
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-gbvvj58i
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:44355
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:44355
distributed.worker - INFO -          dashboard at:      198.202.102.216:34783
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-tl5yfwdx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:44687
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:44687
distributed.worker - INFO -          dashboard at:      198.202.102.216:38099
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kkqvrr4y
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:37505
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:37505
distributed.worker - INFO -          dashboard at:      198.202.102.216:42205
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-svl348o3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:46399
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:46399
distributed.worker - INFO -          dashboard at:      198.202.102.216:41751
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-19u_vbx3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:36547
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:36547
distributed.worker - INFO -          dashboard at:      198.202.102.216:33155
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-fhzf37ox
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:39149
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:39149
distributed.worker - INFO -          dashboard at:      198.202.102.216:39323
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-plnpthig
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:35955
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:35955
distributed.worker - INFO -          dashboard at:      198.202.102.216:42829
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-2x_y263v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:46997
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:46997
distributed.worker - INFO -          dashboard at:      198.202.102.216:38875
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-uvwwv8yn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:32801
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:38773
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:45377
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:41573
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:32801
distributed.worker - INFO -          dashboard at:      198.202.102.216:36155
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:38773
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:41573
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:45377
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO -          dashboard at:      198.202.102.216:32931
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:      198.202.102.216:34189
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:      198.202.102.216:37623
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vz0f5p9g
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-tj9psv90
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-aglrorg9
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-_p9wcnn0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:44423
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:44423
distributed.worker - INFO -          dashboard at:      198.202.102.216:46109
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vocrg0h4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:44425
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:44425
distributed.worker - INFO -          dashboard at:      198.202.102.216:43893
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vnkdcsyf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:37973
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:37973
distributed.worker - INFO -          dashboard at:      198.202.102.216:37161
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vylk0m1o
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:38355
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:33425
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:33425
distributed.worker - INFO -          dashboard at:      198.202.102.216:39871
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:38355
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:      198.202.102.216:39203
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-fntvm0lh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-5kcrbvzb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:39665
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:39665
distributed.worker - INFO -          dashboard at:      198.202.102.216:39953
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-3por1ll1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:39439
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:39439
distributed.worker - INFO -          dashboard at:      198.202.102.216:35387
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-jal268qt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:44933
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:44933
distributed.worker - INFO -          dashboard at:      198.202.102.216:41089
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ka6cx3mf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:46101
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:46101
distributed.worker - INFO -          dashboard at:      198.202.102.216:35491
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-pjostepl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:36759
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:36759
distributed.worker - INFO -          dashboard at:      198.202.102.216:45999
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-9bhpsa12
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:36143
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:36143
distributed.worker - INFO -          dashboard at:      198.202.102.216:42365
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ia18ppez
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:38873
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:38873
distributed.worker - INFO -          dashboard at:      198.202.102.216:36789
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-g40cvm5f
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:40437
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:40437
distributed.worker - INFO -          dashboard at:      198.202.102.216:37305
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-23x_wghk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:41389
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:40931
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:41389
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:40931
distributed.worker - INFO -          dashboard at:      198.202.102.216:34371
distributed.worker - INFO -          dashboard at:      198.202.102.216:38175
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-odco4dbq
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-058ov7cg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:46037
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:46037
distributed.worker - INFO -          dashboard at:      198.202.102.216:41613
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-39_5gz3u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:37515
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:37515
distributed.worker - INFO -          dashboard at:      198.202.102.216:43591
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-2c4h161n
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:45151
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:45151
distributed.worker - INFO -          dashboard at:      198.202.102.216:46827
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-t14_nsew
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:43831
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:43831
distributed.worker - INFO -          dashboard at:      198.202.102.216:36849
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ju4xvr_u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:36745
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:36745
distributed.worker - INFO -          dashboard at:      198.202.102.216:37347
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-fkez7vwy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:42275
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:42275
distributed.worker - INFO -          dashboard at:      198.202.102.216:42473
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-cxupgwxa
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:39011
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:39011
distributed.worker - INFO -          dashboard at:      198.202.102.216:40629
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-64c_m1_w
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:39135
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:39135
distributed.worker - INFO -          dashboard at:      198.202.102.216:40765
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-uegm2bz_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:43021
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:43021
distributed.worker - INFO -          dashboard at:      198.202.102.216:35707
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-bk16qndh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:41907
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:41907
distributed.worker - INFO -          dashboard at:      198.202.102.216:47087
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-wsh5aoet
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:41569
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:41569
distributed.worker - INFO -          dashboard at:      198.202.102.216:35559
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-j6wdxrnm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:40219
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:40219
distributed.worker - INFO -          dashboard at:      198.202.102.216:43193
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-042m3bfx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:34207
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:34207
distributed.worker - INFO -          dashboard at:      198.202.102.216:35521
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-4novequr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:35247
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:35247
distributed.worker - INFO -          dashboard at:      198.202.102.216:36323
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-6iyeatck
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:41211
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:41211
distributed.worker - INFO -          dashboard at:      198.202.102.216:41627
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-yqtz0g92
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:44237
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:44237
distributed.worker - INFO -          dashboard at:      198.202.102.216:41001
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-xwh8rck2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:46745
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:46745
distributed.worker - INFO -          dashboard at:      198.202.102.216:42971
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kwrar6oe
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:41069
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:41069
distributed.worker - INFO -          dashboard at:      198.202.102.216:37933
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-3e33ljsn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:34613
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:34613
distributed.worker - INFO -          dashboard at:      198.202.102.216:43805
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-fyb3jg_i
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:40929
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:40929
distributed.worker - INFO -          dashboard at:      198.202.102.216:46749
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-n_i7bo7d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:44809
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:44809
distributed.worker - INFO -          dashboard at:      198.202.102.216:33401
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-46wot7ve
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:38833
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:38833
distributed.worker - INFO -          dashboard at:      198.202.102.216:40977
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-t5sxto42
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:44441
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:44441
distributed.worker - INFO -          dashboard at:      198.202.102.216:44735
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lvu54qmf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:40453
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:40453
distributed.worker - INFO -          dashboard at:      198.202.102.216:34319
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-u31t9th4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:43659
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:43659
distributed.worker - INFO -          dashboard at:      198.202.102.216:33631
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-n5t7t1s9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:35171
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:35171
distributed.worker - INFO -          dashboard at:      198.202.102.216:46393
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-3f6hozqr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:33099
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:33099
distributed.worker - INFO -          dashboard at:      198.202.102.216:46721
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-0q8akgo8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:44155
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:44155
distributed.worker - INFO -          dashboard at:      198.202.102.216:41129
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-izxv9749
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:41131
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:41131
distributed.worker - INFO -          dashboard at:      198.202.102.216:46753
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-s0kf1i9t
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:45915
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:45915
distributed.worker - INFO -          dashboard at:      198.202.102.216:33845
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vnn5h34e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:46647
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:46647
distributed.worker - INFO -          dashboard at:      198.202.102.216:34717
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-la9ie3o2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:36279'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:33577'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:44431'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:43323'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:38499
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:47007
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:34699'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:46681
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:43129'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:43315
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:43501'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:36329
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:45379'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:36533
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:43223'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:38091
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:46719'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:34445
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:43611'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:38571
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:37557'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:42865
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:41183'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:45885
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:32805'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:38349
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:40301'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:38891
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:34443'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:38251
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:35989'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:43383
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:43013'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:44061
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:34921'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:45725
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:37255'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:41871
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:44443
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:42827'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:40673'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:40951
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:46615'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:36547
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:37505
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:45435'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:36749'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:37839
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:42733'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:43057
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:44433'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:46399
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:35061'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:44113
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:43237'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:38153
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:38187'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:42665'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:44423
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:37683
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:33869'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:36819'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:44687
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:44425
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:36001'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:42353'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:35955
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:44355
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:35899'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:34753
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:45157'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:40531'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:45377
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:43993'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:32801
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:37973
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:32781'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:38799'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:39149
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:38355
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:43073'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:41573
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:46997
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:38773
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:35189'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:36891'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:39211'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:39665
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:35179'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:36759
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:32807'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:33425
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:33995'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:44933
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:34919'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:41423'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:39439
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:46101
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:35615'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:38873
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:39411'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:36143
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:46695'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:41569
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:35301'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:34207
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:38643'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:37581'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:41389
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:40437
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:36831'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:43831
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:45493'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:40931
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:38851'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:46037
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:34515'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:45151
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:45905'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:42275
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:42951'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:37515
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:45063'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:39011
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:36745
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:41907
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:40989'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:42245'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:44523'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:44193'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:43021
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:45667'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:39135
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:35247
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:45561'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:46745
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:35073'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:44809
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:40933'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:33771'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:44237
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:42051'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:40219
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:34613
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:46713'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:40929
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:34129'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:38833
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:44239'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:41069
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:33579'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:44441
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:41485'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:40453
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:43049'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:37219'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:41211
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:33535
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:33649'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:42777'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:41131
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:38895'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:46647
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:33355'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:41247'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:35171
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:32867'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:47053'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:47005'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:34669'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:43659
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:40049'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:33271'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:44155
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:40497'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:39295'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:45915
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:38621'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:33099
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:39265'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:40239'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:39881'
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:37803
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:40027'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:34347'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.102.216:37691'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:36525
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:36525
distributed.worker - INFO -          dashboard at:      198.202.102.216:36725
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-pqc7blg7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:36525
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.102.216:43929
distributed.worker - INFO -          Listening to: tcp://198.202.102.216:43929
distributed.worker - INFO -          dashboard at:      198.202.102.216:45509
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-t5aij7qo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Stopping worker at tcp://198.202.102.216:43929
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - INFO - Worker process 71394 was killed by signal 15
distributed.nanny - INFO - Worker process 71377 was killed by signal 15
distributed.nanny - INFO - Worker process 71399 was killed by signal 15
distributed.nanny - INFO - Worker process 71467 was killed by signal 15
distributed.nanny - INFO - Worker process 71468 was killed by signal 15
distributed.nanny - INFO - Worker process 71453 was killed by signal 15
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72231 parent=71135 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72189 parent=71135 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72181 parent=71135 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72039 parent=71135 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72022 parent=71135 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72024 parent=71135 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72010 parent=71135 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=71948 parent=71135 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=71912 parent=71135 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=71772 parent=71135 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=71523 parent=71135 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=71498 parent=71135 started daemon>
