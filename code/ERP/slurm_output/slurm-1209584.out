distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:37595'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38185'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46331'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34865'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:39779'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46003'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38417'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:45323'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:37963'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46333'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:37543'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:44325'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38933'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36667'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34623'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:37597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:37449'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:45719'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36509'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34011'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:44465'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35781'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41527'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36757'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41625'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:40683'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34667'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34723'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:40427'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46217'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:44083'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38077'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:39967'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38497'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:43429'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34951'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:43579'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:40221'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:33143'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41333'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38531'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:40467'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38881'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:39653'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:42685'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:33485'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35123'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:40685'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:43191'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35153'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41091'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41525'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36973'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:37585'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:37523'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:39015'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:45249'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:44737'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46025'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:40529'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:39721'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35615'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:42117'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46719'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46745'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46787'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38593'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:39703'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:33761'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41481'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:44851'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41353'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:45429'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:45995'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36269'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:44131'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:42795'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34529'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:32775'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36153'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38753'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41859'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:40895'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:33817'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:33587'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34931'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38093'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:43483'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:39421'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:45037'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:40833'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:37999'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:33467'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:44047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:37349'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46793'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:43269'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38783'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:40921'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42333
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:36781
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39203
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39369
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:40069
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44149
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42333
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:36781
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39203
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39369
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:40069
distributed.worker - INFO -          dashboard at:       198.202.103.38:39435
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44149
distributed.worker - INFO -          dashboard at:       198.202.103.38:41021
distributed.worker - INFO -          dashboard at:       198.202.103.38:33955
distributed.worker - INFO -          dashboard at:       198.202.103.38:43207
distributed.worker - INFO -          dashboard at:       198.202.103.38:35133
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO -          dashboard at:       198.202.103.38:33645
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:45273
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:45273
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:       198.202.103.38:39073
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-jbygyhbo
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-9r7dci3t
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-yo_o888o
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-o1766osx
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-mnmoeh55
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ny550b33
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-g4fs7jw4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:36515
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:36515
distributed.worker - INFO -          dashboard at:       198.202.103.38:35965
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-_kkx300j
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:33767
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:33767
distributed.worker - INFO -          dashboard at:       198.202.103.38:44087
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-h3e_hzsm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:40361
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:40361
distributed.worker - INFO -          dashboard at:       198.202.103.38:38035
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ihvz0xhy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:41805
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:41805
distributed.worker - INFO -          dashboard at:       198.202.103.38:40883
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ojx6qlm6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:46367
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:46367
distributed.worker - INFO -          dashboard at:       198.202.103.38:44743
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-dwi3kl5v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:45225
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:45225
distributed.worker - INFO -          dashboard at:       198.202.103.38:45395
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-2uhuy7p7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:33725
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:33725
distributed.worker - INFO -          dashboard at:       198.202.103.38:45233
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44147
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-73t9ef5z
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44147
distributed.worker - INFO -          dashboard at:       198.202.103.38:39527
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-xtgz0juk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:33251
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:33251
distributed.worker - INFO -          dashboard at:       198.202.103.38:34295
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-7772hf5k
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44873
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44873
distributed.worker - INFO -          dashboard at:       198.202.103.38:39999
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-94bqp97q
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:36091
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:36091
distributed.worker - INFO -          dashboard at:       198.202.103.38:37601
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-isjwdmjl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42611
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42611
distributed.worker - INFO -          dashboard at:       198.202.103.38:45889
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-6m5kh7zz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:33511
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:33511
distributed.worker - INFO -          dashboard at:       198.202.103.38:42235
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-w42xbawg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44437
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44437
distributed.worker - INFO -          dashboard at:       198.202.103.38:38235
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-d9n390va
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:36867
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:36867
distributed.worker - INFO -          dashboard at:       198.202.103.38:41193
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-7pcwv1ku
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:33367
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:33367
distributed.worker - INFO -          dashboard at:       198.202.103.38:38251
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-0sa8k0hi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44057
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44057
distributed.worker - INFO -          dashboard at:       198.202.103.38:46737
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-yi625j9r
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42853
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42853
distributed.worker - INFO -          dashboard at:       198.202.103.38:35971
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-hvqwrfbc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:41171
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:41171
distributed.worker - INFO -          dashboard at:       198.202.103.38:35771
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-j994ks0v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:38759
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:38759
distributed.worker - INFO -          dashboard at:       198.202.103.38:42923
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-n9wj3gkk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:37497
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:37497
distributed.worker - INFO -          dashboard at:       198.202.103.38:35723
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-atve8bdm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42773
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42773
distributed.worker - INFO -          dashboard at:       198.202.103.38:42815
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-fhzfmuab
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42781
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42781
distributed.worker - INFO -          dashboard at:       198.202.103.38:40367
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-auuadl5j
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:37535
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:37535
distributed.worker - INFO -          dashboard at:       198.202.103.38:37451
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-i9yklz4l
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:43931
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:43931
distributed.worker - INFO -          dashboard at:       198.202.103.38:42905
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-rsr6y3_8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39359
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39359
distributed.worker - INFO -          dashboard at:       198.202.103.38:42515
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-k0059s8i
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:46357
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:46357
distributed.worker - INFO -          dashboard at:       198.202.103.38:40545
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-a1xdf1ph
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39863
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39863
distributed.worker - INFO -          dashboard at:       198.202.103.38:38907
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-33fnkyyn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39175
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39175
distributed.worker - INFO -          dashboard at:       198.202.103.38:43123
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-g7zvzzo0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:43003
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:43003
distributed.worker - INFO -          dashboard at:       198.202.103.38:33197
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-rgr3phg6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42345
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42345
distributed.worker - INFO -          dashboard at:       198.202.103.38:44897
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-29ob7pm7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39835
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39835
distributed.worker - INFO -          dashboard at:       198.202.103.38:37291
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-6z0bue4i
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:34213
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:34213
distributed.worker - INFO -          dashboard at:       198.202.103.38:45605
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-4y6z6wxm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42449
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42449
distributed.worker - INFO -          dashboard at:       198.202.103.38:36259
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-nj5i1hsx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:35499
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:35499
distributed.worker - INFO -          dashboard at:       198.202.103.38:32971
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-u0w68v4f
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:41355
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:41355
distributed.worker - INFO -          dashboard at:       198.202.103.38:38923
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-gei93i96
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:40903
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:40903
distributed.worker - INFO -          dashboard at:       198.202.103.38:42909
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-rx5sa11a
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:45779
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:45779
distributed.worker - INFO -          dashboard at:       198.202.103.38:42015
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-1ioe92ke
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39979
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39979
distributed.worker - INFO -          dashboard at:       198.202.103.38:37789
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-jxty_c3c
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39697
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39697
distributed.worker - INFO -          dashboard at:       198.202.103.38:44567
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-h8brgs6e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39593
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39593
distributed.worker - INFO -          dashboard at:       198.202.103.38:35131
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-g_nh72zs
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:41105
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:41105
distributed.worker - INFO -          dashboard at:       198.202.103.38:33547
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kh87uggm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:43219
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:43219
distributed.worker - INFO -          dashboard at:       198.202.103.38:44503
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-sguurbdx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44093
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44093
distributed.worker - INFO -          dashboard at:       198.202.103.38:37461
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-t_5d_uhl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:43143
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:43143
distributed.worker - INFO -          dashboard at:       198.202.103.38:39671
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-i51oww7m
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44225
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44225
distributed.worker - INFO -          dashboard at:       198.202.103.38:40587
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vnkhtv7b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42837
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42837
distributed.worker - INFO -          dashboard at:       198.202.103.38:34255
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-_7qn5wug
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39133
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39133
distributed.worker - INFO -          dashboard at:       198.202.103.38:39765
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-fws9plyq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:41669
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:41669
distributed.worker - INFO -          dashboard at:       198.202.103.38:46603
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-2jlsanee
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39917
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39917
distributed.worker - INFO -          dashboard at:       198.202.103.38:39587
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-q9sr37jd
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42171
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42171
distributed.worker - INFO -          dashboard at:       198.202.103.38:43011
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-diwsoyqy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:33969
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:33969
distributed.worker - INFO -          dashboard at:       198.202.103.38:39915
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-oflzzpnb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:43111
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:43111
distributed.worker - INFO -          dashboard at:       198.202.103.38:46723
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-8azta0cm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:40855
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:40855
distributed.worker - INFO -          dashboard at:       198.202.103.38:43417
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-2wm7shpp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:41121
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:41121
distributed.worker - INFO -          dashboard at:       198.202.103.38:45151
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lnqk3cle
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44987
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44987
distributed.worker - INFO -          dashboard at:       198.202.103.38:34579
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-zrpjv0im
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:43901
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:43901
distributed.worker - INFO -          dashboard at:       198.202.103.38:44107
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ngz9ml8y
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:45175
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:45175
distributed.worker - INFO -          dashboard at:       198.202.103.38:45845
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-nfnklk1g
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:35417
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:35417
distributed.worker - INFO -          dashboard at:       198.202.103.38:40847
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-sp2h7gb1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44155
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44155
distributed.worker - INFO -          dashboard at:       198.202.103.38:44615
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ggmaygkk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42065
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42065
distributed.worker - INFO -          dashboard at:       198.202.103.38:46067
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:45277
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:45277
distributed.worker - INFO -          dashboard at:       198.202.103.38:42753
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-apn2g2_t
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-gnaplbxr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:41743
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:41743
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:45679
distributed.worker - INFO -          dashboard at:       198.202.103.38:38253
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:45679
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO -          dashboard at:       198.202.103.38:36949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-bmrvv4te
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-qww32qzh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:34361
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:34361
distributed.worker - INFO -          dashboard at:       198.202.103.38:38423
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-q2v9z2rw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:33763
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:33763
distributed.worker - INFO -          dashboard at:       198.202.103.38:39881
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-01g4p83g
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:46513
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:46513
distributed.worker - INFO -          dashboard at:       198.202.103.38:39971
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-qwo_r2ez
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:35241
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:35241
distributed.worker - INFO -          dashboard at:       198.202.103.38:35051
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-1bit8h7g
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:46153
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:46153
distributed.worker - INFO -          dashboard at:       198.202.103.38:35181
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-jct8iv5i
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:33819
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:33819
distributed.worker - INFO -          dashboard at:       198.202.103.38:34435
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ndtkm6xm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:35517
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:35517
distributed.worker - INFO -          dashboard at:       198.202.103.38:41799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ncb9dkfg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:41789
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:41789
distributed.worker - INFO -          dashboard at:       198.202.103.38:36895
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-5zzkr_gt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44051
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44051
distributed.worker - INFO -          dashboard at:       198.202.103.38:42295
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-we3ki7b1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:35155
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:35155
distributed.worker - INFO -          dashboard at:       198.202.103.38:36189
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kuzxuyln
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:36983
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:36983
distributed.worker - INFO -          dashboard at:       198.202.103.38:40811
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-1f0p7jgl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:45721
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:45721
distributed.worker - INFO -          dashboard at:       198.202.103.38:35021
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-dpd6dd7e
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:36391
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:36391
distributed.worker - INFO -          dashboard at:       198.202.103.38:34431
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-i2gl2c6u
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:34473
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:34473
distributed.worker - INFO -          dashboard at:       198.202.103.38:45253
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lr_2xk95
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:33097
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:33097
distributed.worker - INFO -          dashboard at:       198.202.103.38:39693
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-hyx31i85
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42147
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42147
distributed.worker - INFO -          dashboard at:       198.202.103.38:40657
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-95qvhiwi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:37245
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:37245
distributed.worker - INFO -          dashboard at:       198.202.103.38:43015
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-2c49r4ng
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:43817
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:43817
distributed.worker - INFO -          dashboard at:       198.202.103.38:35015
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-c4ld_qj9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:33979
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:33979
distributed.worker - INFO -          dashboard at:       198.202.103.38:41857
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lkj9dd97
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:40857
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:40857
distributed.worker - INFO -          dashboard at:       198.202.103.38:36015
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-b1tre0m_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39699
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39699
distributed.worker - INFO -          dashboard at:       198.202.103.38:39737
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vr_x6ivq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:37749
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:37749
distributed.worker - INFO -          dashboard at:       198.202.103.38:35115
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-4efd41lo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:34249
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:34249
distributed.worker - INFO -          dashboard at:       198.202.103.38:44415
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:35277
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:35277
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-2j1paisl
distributed.worker - INFO -          dashboard at:       198.202.103.38:33981
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:45789
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:45789
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -          dashboard at:       198.202.103.38:47059
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-8lai6uh6
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-9k51b0oz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:35843
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:35843
distributed.worker - INFO -          dashboard at:       198.202.103.38:36657
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-o8do3nt8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:46957
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:46957
distributed.worker - INFO -          dashboard at:       198.202.103.38:44603
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ir2uzovn
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:36161
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:36161
distributed.worker - INFO -          dashboard at:       198.202.103.38:35995
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-mjf1ns7l
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42425
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42425
distributed.worker - INFO -          dashboard at:       198.202.103.38:39651
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-277g_814
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:37595'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38185'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46331'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42333
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34865'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:45273
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:39779'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39203
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46003'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:36781
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38417'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44149
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:45323'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:40069
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:37963'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39369
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46333'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:41805
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:37543'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:33767
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:44325'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:46367
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38933'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:36515
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:40361
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36667'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34623'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44147
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:37597'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:33725
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:45225
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:37449'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:45719'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:33251
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36509'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44873
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34011'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:36091
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:44465'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42611
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35781'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:33367
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41527'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44437
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36757'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:33511
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41625'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:36867
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:40683'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44057
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34667'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:41171
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34723'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:46357
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:40427'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:38759
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46217'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39359
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:44083'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:37535
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38077'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42773
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:39967'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:37497
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38497'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42781
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:43429'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:43931
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34951'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42853
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:43579'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39863
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36237'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39175
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:40221'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42345
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:33143'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:45779
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41333'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:40903
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38531'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:35499
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:40467'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39835
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38881'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39697
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:39653'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:34213
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42449
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:42685'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39593
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:33485'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35123'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:43003
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:41355
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:40685'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:43191'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:41105
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35153'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39979
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41091'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44225
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41525'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36973'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42837
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:43219
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:37585'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:43143
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:37523'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:41669
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:39015'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39133
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:45249'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:44737'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44093
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39917
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46025'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:33969
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:40529'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42171
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:39721'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:40855
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35615'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:45175
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:42117'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:43111
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46719'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:41121
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44155
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46745'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46787'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:35417
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38593'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:34473
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:39703'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:46513
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:33761'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:41789
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41481'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:45789
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:44851'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:45277
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41353'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:33763
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:45429'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:45679
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:45995'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44987
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36269'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:43901
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:44131'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42065
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:41743
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:42795'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34529'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:34361
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:32775'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:35517
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36153'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:46153
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38753'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:43817
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:45721
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:33819
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41859'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:40895'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:33817'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:37245
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:33587'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:40857
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34931'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42425
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38093'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39699
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:43483'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:36161
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:39421'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:35843
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:45037'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:36391
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:40833'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:37749
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:37999'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:35241
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:33467'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:33979
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:44047'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:36983
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:37349'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:46957
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46793'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44051
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:43269'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:35277
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38783'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:33097
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:40921'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:35155
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42147
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:34249
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38826 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38823 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38816 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38815 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38812 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38813 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38819 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38810 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38744 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38675 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38677 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38609 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38674 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38607 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38574 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38408 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38400 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38403 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38396 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38405 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38383 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38380 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38377 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38372 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38366 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38364 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38361 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38357 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38348 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38351 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38342 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38345 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38338 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38331 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38335 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38323 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38328 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38317 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38310 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38307 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38302 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38298 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38296 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38286 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38289 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38284 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38282 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38269 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38274 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38278 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38266 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38261 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38228 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38173 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38168 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38165 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38145 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38120 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38117 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38113 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38107 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38110 parent=38030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38104 parent=38030 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
