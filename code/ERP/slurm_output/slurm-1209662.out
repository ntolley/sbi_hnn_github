distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36353'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36595'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34063'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36883'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46703'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35833'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:43867'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34117'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:45605'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38629'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:40477'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:43505'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36427'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46013'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:39557'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:47059'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:33771'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35937'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38173'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35071'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41019'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36845'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35249'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38843'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41763'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38363'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:45677'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38101'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46137'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35105'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41169'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38313'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35755'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:39941'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35783'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:43079'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:45955'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36741'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34615'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:43141'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35001'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46221'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36213'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:37655'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38387'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:40401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46417'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38467'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:44497'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:33103'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:45581'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:40265'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:43707'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:42599'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:44047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:39635'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46433'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:45447'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:44267'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46087'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46705'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46847'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:40309'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36549'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38001'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:42383'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46255'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:33701'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36103'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:44053'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:41615'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36937'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:45369'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36493'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:37297'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35905'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:46285'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:44287'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:39345'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:39291'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:42349'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34541'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:35943'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:38857'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:33633'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:43101'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:36091'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:42863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:32985'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:34347'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:44579'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.38:33271'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39423
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39423
distributed.worker - INFO -          dashboard at:       198.202.103.38:40379
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44391
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44391
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:       198.202.103.38:33961
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-12a285m0
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-w9ivkvgl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:40985
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:40985
distributed.worker - INFO -          dashboard at:       198.202.103.38:37853
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lv92ddxr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44757
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44757
distributed.worker - INFO -          dashboard at:       198.202.103.38:44217
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-zri8z6tr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:43333
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:43333
distributed.worker - INFO -          dashboard at:       198.202.103.38:44603
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-qvd_zmxu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:36351
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:36351
distributed.worker - INFO -          dashboard at:       198.202.103.38:46369
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-qmmanl5v
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:33979
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:33979
distributed.worker - INFO -          dashboard at:       198.202.103.38:38985
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vzqx3msj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39267
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39267
distributed.worker - INFO -          dashboard at:       198.202.103.38:43889
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lrtxcmhh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39023
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39023
distributed.worker - INFO -          dashboard at:       198.202.103.38:38623
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-i2vs50v9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39703
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39703
distributed.worker - INFO -          dashboard at:       198.202.103.38:41555
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-pi4ro59d
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:36677
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:36677
distributed.worker - INFO -          dashboard at:       198.202.103.38:43751
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lv7tqayj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39507
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39507
distributed.worker - INFO -          dashboard at:       198.202.103.38:39333
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-xj9ulcqo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:43709
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:43709
distributed.worker - INFO -          dashboard at:       198.202.103.38:39021
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-n7dz83z9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:43335
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:43335
distributed.worker - INFO -          dashboard at:       198.202.103.38:46163
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-t1dvdufe
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:46731
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:46731
distributed.worker - INFO -          dashboard at:       198.202.103.38:42993
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-pxy3e3dy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:35495
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:35495
distributed.worker - INFO -          dashboard at:       198.202.103.38:37157
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-zkwsi0gr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:45875
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:45875
distributed.worker - INFO -          dashboard at:       198.202.103.38:38765
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-gsj_g32r
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:36901
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:36901
distributed.worker - INFO -          dashboard at:       198.202.103.38:39855
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-mlnnzl36
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44767
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44767
distributed.worker - INFO -          dashboard at:       198.202.103.38:38911
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-44wlpkq2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:33629
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:33629
distributed.worker - INFO -          dashboard at:       198.202.103.38:39087
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-27r3kds0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39429
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39429
distributed.worker - INFO -          dashboard at:       198.202.103.38:43019
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vxsc4cmg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:41481
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:41481
distributed.worker - INFO -          dashboard at:       198.202.103.38:37817
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-w5myqlct
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42953
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42953
distributed.worker - INFO -          dashboard at:       198.202.103.38:34039
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-oxicgi_t
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:36661
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:36661
distributed.worker - INFO -          dashboard at:       198.202.103.38:44383
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-75lqw7b2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44139
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44139
distributed.worker - INFO -          dashboard at:       198.202.103.38:46309
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-uwiao2p6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:45765
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:45765
distributed.worker - INFO -          dashboard at:       198.202.103.38:42799
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-85cqflwk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:40047
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:40047
distributed.worker - INFO -          dashboard at:       198.202.103.38:34911
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-91cjiyj0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44443
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44443
distributed.worker - INFO -          dashboard at:       198.202.103.38:44981
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-7o8k8__k
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:41711
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:41711
distributed.worker - INFO -          dashboard at:       198.202.103.38:43541
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-pz2qy823
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:35845
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:35845
distributed.worker - INFO -          dashboard at:       198.202.103.38:42939
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-eurxjaly
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:36987
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:36987
distributed.worker - INFO -          dashboard at:       198.202.103.38:39045
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-4fcb_xz1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:37449
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:37449
distributed.worker - INFO -          dashboard at:       198.202.103.38:33485
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-66kjk824
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:36127
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:36127
distributed.worker - INFO -          dashboard at:       198.202.103.38:39145
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-nq9enpwf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:33155
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:33155
distributed.worker - INFO -          dashboard at:       198.202.103.38:40483
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-p6ehzwuj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42639
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42639
distributed.worker - INFO -          dashboard at:       198.202.103.38:35337
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kyrohs62
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:38839
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:38839
distributed.worker - INFO -          dashboard at:       198.202.103.38:35039
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vi4zs9r7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:45517
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:45517
distributed.worker - INFO -          dashboard at:       198.202.103.38:46687
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lyryrjmv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39335
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39335
distributed.worker - INFO -          dashboard at:       198.202.103.38:44999
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-4jparv_2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:45151
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:45151
distributed.worker - INFO -          dashboard at:       198.202.103.38:39927
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-0bnj0yv8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:37701
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:37701
distributed.worker - INFO -          dashboard at:       198.202.103.38:34309
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-sanured1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42969
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42969
distributed.worker - INFO -          dashboard at:       198.202.103.38:43219
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kf346_we
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:35143
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:35143
distributed.worker - INFO -          dashboard at:       198.202.103.38:44419
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-24g5j3pg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:38497
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:38497
distributed.worker - INFO -          dashboard at:       198.202.103.38:43965
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-d88yqicv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39077
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39077
distributed.worker - INFO -          dashboard at:       198.202.103.38:45757
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-i6nl_gc4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44155
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44155
distributed.worker - INFO -          dashboard at:       198.202.103.38:40313
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-587fgmj_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:44067
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:44067
distributed.worker - INFO -          dashboard at:       198.202.103.38:35429
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lskadag5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:42959
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:42959
distributed.worker - INFO -          dashboard at:       198.202.103.38:44733
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-o1t4l818
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36353'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35047'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44757
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36595'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:40985
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34063'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:43333
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41621'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36883'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44391
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46703'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39423
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:33979
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35833'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:43867'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39023
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34117'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:36351
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:45605'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39267
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39703
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38629'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:40477'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:43335
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:43505'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36427'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:43709
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:46731
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46013'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:36677
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39507
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:39557'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:47059'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:45875
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:35495
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:33629
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:33771'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35937'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38173'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:36901
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44767
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35071'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41019'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:41481
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36845'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39429
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35249'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42953
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38843'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41763'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:45765
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38363'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:40047
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:36661
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:45677'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38101'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44139
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:37449
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46137'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:35845
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35105'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44443
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41169'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38313'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:36987
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:41711
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35755'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:36127
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:39941'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34047'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:45517
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:33155
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35783'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39335
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:43079'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:45151
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34535'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42639
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:45955'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42969
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34597'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36741'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34615'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:35143
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:38839
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:43141'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:38497
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35001'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39077
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46221'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:37701
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36213'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:37655'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44067
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38387'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:40401'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:44155
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46417'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41917'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:42959
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38467'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:44497'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:33103'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:45581'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:40265'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:43707'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:42599'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:44047'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:39635'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46433'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:45447'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:44267'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46087'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46705'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46847'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:40309'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36549'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38001'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:42383'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46255'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:33701'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36103'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:44053'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:41615'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36937'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:45369'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36493'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:37297'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35905'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:46285'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:44287'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:39345'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:39291'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:42349'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34541'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:35943'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36317'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:38857'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:33633'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:43101'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:36091'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:42863'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34179'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:32985'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:34347'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:44579'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.38:33271'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.38:39501
distributed.worker - INFO -          Listening to: tcp://198.202.103.38:39501
distributed.worker - INFO -          dashboard at:       198.202.103.38:35825
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-mjh8540u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Stopping worker at tcp://198.202.103.38:39501
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53957 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53955 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53959 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53951 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53953 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53945 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53941 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53947 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53939 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53873 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53871 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53868 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53866 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53855 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53864 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53628 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53602 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53593 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53598 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53590 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53585 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53580 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53573 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53577 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53571 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53563 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53555 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53558 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53551 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53567 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53546 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53544 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53537 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53535 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53533 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53525 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53521 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53518 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53523 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53507 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53511 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53503 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53501 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53499 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53495 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53491 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53484 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53476 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53478 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53473 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53470 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53472 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53464 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53459 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53455 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53446 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53451 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53445 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53442 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53438 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53433 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53423 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53412 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53410 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53379 parent=53233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53366 parent=53233 started daemon>
