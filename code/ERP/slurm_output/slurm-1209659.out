distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37285'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38529'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43425'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:36293'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:44597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:41333'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:36115'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42661'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42607'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:33535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:36623'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:45103'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:33153'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37921'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43079'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37277'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43511'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38533'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46149'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:41447'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:33003'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:45193'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43019'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37231'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:33063'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40951'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37013'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:33269'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46743'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38189'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35623'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:39721'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46847'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:39987'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43319'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35421'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43287'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42847'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35677'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42765'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:39161'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40803'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:45209'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40161'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37339'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:36993'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42309'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46063'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35009'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:44053'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:44295'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:39597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40515'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:33637'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35239'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35063'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46555'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40063'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:47011'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46417'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:41921'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:34497'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:34819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:44897'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38195'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37021'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37039'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46783'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:41891'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38727'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35361'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42137'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:32991'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38147'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:32925'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42533'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:44751'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:41537'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40149'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43037'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37067'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40015'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:45155'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40965'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:41847'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37611'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:39705'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:34631'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:44929'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:41735'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:34583'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:36635'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46177'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:34021'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:39013'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46147'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:44843
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:44843
distributed.worker - INFO -          dashboard at:       198.202.103.40:37653
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-0ishckdl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:41587
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:41587
distributed.worker - INFO -          dashboard at:       198.202.103.40:37681
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-y477x065
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42601
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42601
distributed.worker - INFO -          dashboard at:       198.202.103.40:36925
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-tm4h3xkm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42261
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42261
distributed.worker - INFO -          dashboard at:       198.202.103.40:46391
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ed7mujxy
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:33125
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:33125
distributed.worker - INFO -          dashboard at:       198.202.103.40:33543
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-uecrvr3n
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42399
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42399
distributed.worker - INFO -          dashboard at:       198.202.103.40:43283
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ca67nb9p
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45721
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45721
distributed.worker - INFO -          dashboard at:       198.202.103.40:33725
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-esle5ibx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:41883
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:41883
distributed.worker - INFO -          dashboard at:       198.202.103.40:32909
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-12s_xvs3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:44077
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:44077
distributed.worker - INFO -          dashboard at:       198.202.103.40:34605
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-v0yeefyj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:38405
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:38405
distributed.worker - INFO -          dashboard at:       198.202.103.40:36401
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-auwp5apt
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:36177
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:36177
distributed.worker - INFO -          dashboard at:       198.202.103.40:46953
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-r4g9qaz8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:39179
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:39179
distributed.worker - INFO -          dashboard at:       198.202.103.40:41165
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-y56vrxrg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:40369
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:40369
distributed.worker - INFO -          dashboard at:       198.202.103.40:42271
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lqjpuiey
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:44211
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:44211
distributed.worker - INFO -          dashboard at:       198.202.103.40:38689
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-2jglyov5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:41365
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:41365
distributed.worker - INFO -          dashboard at:       198.202.103.40:35135
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-p_spjdoe
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:34723
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:34723
distributed.worker - INFO -          dashboard at:       198.202.103.40:44013
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-yfhr1ysh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:43815
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:43815
distributed.worker - INFO -          dashboard at:       198.202.103.40:37119
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-tlvo9oln
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:37011
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:37011
distributed.worker - INFO -          dashboard at:       198.202.103.40:39581
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-iw1t73b6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45387
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45387
distributed.worker - INFO -          dashboard at:       198.202.103.40:42995
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-d1phdih6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:40639
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:40639
distributed.worker - INFO -          dashboard at:       198.202.103.40:40335
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-775j7818
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:40199
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:40199
distributed.worker - INFO -          dashboard at:       198.202.103.40:44409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-iifx6qo4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:46859
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:46859
distributed.worker - INFO -          dashboard at:       198.202.103.40:43805
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-iizcyusz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:46835
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:46835
distributed.worker - INFO -          dashboard at:       198.202.103.40:46989
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-2janqnzx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:43569
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:43569
distributed.worker - INFO -          dashboard at:       198.202.103.40:33843
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-v80fk0mx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:37685
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:37685
distributed.worker - INFO -          dashboard at:       198.202.103.40:46109
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-gk1e3ep2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45679
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45679
distributed.worker - INFO -          dashboard at:       198.202.103.40:45347
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-qrzlij99
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42041
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42041
distributed.worker - INFO -          dashboard at:       198.202.103.40:41107
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-3hays9u5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:33795
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:33795
distributed.worker - INFO -          dashboard at:       198.202.103.40:42299
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-nky_m_49
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:43577
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:43577
distributed.worker - INFO -          dashboard at:       198.202.103.40:44979
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ixf46sjw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:33827
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:33827
distributed.worker - INFO -          dashboard at:       198.202.103.40:43085
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lj_jnu3b
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:40373
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:40373
distributed.worker - INFO -          dashboard at:       198.202.103.40:37727
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-wx3zlufb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:34263
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:34263
distributed.worker - INFO -          dashboard at:       198.202.103.40:39217
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-khqquu0p
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35663
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35663
distributed.worker - INFO -          dashboard at:       198.202.103.40:42793
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-zn8var6i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:43685
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:43685
distributed.worker - INFO -          dashboard at:       198.202.103.40:33539
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-dvwep_91
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35475
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35475
distributed.worker - INFO -          dashboard at:       198.202.103.40:38029
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-6n5ect04
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:34701
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:34701
distributed.worker - INFO -          dashboard at:       198.202.103.40:44953
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-r905a5iq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:40825
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:40825
distributed.worker - INFO -          dashboard at:       198.202.103.40:41225
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-uw0cuupx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35933
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35933
distributed.worker - INFO -          dashboard at:       198.202.103.40:45445
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-bzznka2k
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:37683
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:37683
distributed.worker - INFO -          dashboard at:       198.202.103.40:46667
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-da8mjy8w
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42939
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42939
distributed.worker - INFO -          dashboard at:       198.202.103.40:41339
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-g0s0o062
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:38899
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:38899
distributed.worker - INFO -          dashboard at:       198.202.103.40:42767
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-9898pa6a
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:38721
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:38721
distributed.worker - INFO -          dashboard at:       198.202.103.40:37885
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-y5if7m4n
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35471
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35471
distributed.worker - INFO -          dashboard at:       198.202.103.40:45875
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-_ipnpq8g
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:40983
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:40983
distributed.worker - INFO -          dashboard at:       198.202.103.40:39367
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-qn2ab3gr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:36545
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:36545
distributed.worker - INFO -          dashboard at:       198.202.103.40:41171
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-rxm20aad
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:33731
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:33731
distributed.worker - INFO -          dashboard at:       198.202.103.40:44609
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-v8hzlqix
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:40097
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:40097
distributed.worker - INFO -          dashboard at:       198.202.103.40:33625
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ed7a92xh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:38739
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:38739
distributed.worker - INFO -          dashboard at:       198.202.103.40:33137
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:37949
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-egn61uc5
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37285'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42047'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38529'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:41587
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:44843
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43425'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:36293'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42399
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:44597'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42261
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40401'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:33125
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:41333'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42601
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:36115'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45721
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42661'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:41883
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42607'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:44077
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:33535'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:39179
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:36623'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:40369
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:45103'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:36177
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:33153'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:44211
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37921'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:41365
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43079'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:43815
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37277'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:34723
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43511'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:37011
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38533'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:40639
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46149'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:41447'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45387
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:38405
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:33003'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:46859
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:45193'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:43569
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43019'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:33827
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37231'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:40199
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:33063'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:46835
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40951'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:43577
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37013'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42041
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:33269'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46743'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45679
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:33795
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38189'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:34263
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35623'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:39721'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35663
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:40825
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46847'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:40373
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:39987'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43319'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35933
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35475
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35421'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:43685
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43287'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:34701
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42847'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42939
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35677'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35471
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42765'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:38899
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:37683
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:39161'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40803'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:40983
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:45209'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:38721
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40161'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:36545
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37339'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:33731
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:36993'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:38739
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42819'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:40097
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42309'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46063'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35009'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:44053'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:44295'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:39597'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40515'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:33637'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35239'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35063'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46555'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40063'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:47011'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46417'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:41921'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:34497'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:34819'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:44897'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38195'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37021'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37039'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46783'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:41891'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38727'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35361'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42137'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:32991'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38147'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:32925'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40819'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42533'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:44751'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:41537'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40149'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43037'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37067'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40015'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:45155'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40965'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:41847'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37611'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:39705'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:34631'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:44929'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:41735'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:37685
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:34583'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:36635'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46177'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:34021'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:39013'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46147'
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=126101 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=126103 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=126099 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=126096 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=126093 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=126089 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=126023 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125957 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125955 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125950 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125948 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125951 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125943 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125944 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125941 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125938 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125936 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125813 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125800 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125761 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125869 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125756 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125720 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125718 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125713 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125710 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125644 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125641 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125638 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125635 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125630 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125631 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125624 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125621 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125622 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125618 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125593 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125547 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125481 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125478 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125412 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125409 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125315 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125384 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125387 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125205 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125202 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125199 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125197 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125194 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125192 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125186 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125177 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125175 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125171 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125181 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125165 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125162 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125158 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125150 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125153 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125152 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125146 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125142 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125138 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125135 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125132 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125128 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125121 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125118 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125114 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125109 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125106 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125103 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125097 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125093 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125089 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125086 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125080 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125075 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125071 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125068 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125065 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125062 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125060 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125055 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125053 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125049 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125046 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125043 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125041 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125038 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125031 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125029 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125024 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125023 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125020 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125018 parent=124942 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=125017 parent=124942 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
