distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:38849'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:44979'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:41345'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:44967'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:39641'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:43825'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:45141'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:39093'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:38043'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:38819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:38607'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:33775'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:35101'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:43737'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:37265'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:33681'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:43139'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:41229'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:37571'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:32873'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:41835'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:46303'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:46235'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:34185'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:37939'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:37519'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:38937'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:38379'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:44145'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:42399'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:42889'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:40149'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:36075'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:37805'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:36275'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:37645'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:43669'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:37949'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:34917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:35521'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:33659'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:42403'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:44333'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:37537'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:37999'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:45655'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:39811'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:44795'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:35625'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:34715'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:40221'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:46677'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:42511'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:36757'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:41405'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:41175'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:32831'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:42041'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:38353'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:33337'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:34953'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:39587'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:37979'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:38659'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:44235'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:33553'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:41767'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:46469'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:46047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:41003'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:45009'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:39963'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:32949'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:37399'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:39003'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:42735'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:42405'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:32957'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:40905'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:42417'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:37005'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:34655'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:46527'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:45385'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:44731'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:34203'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:42401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:39959'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:32847'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:41723'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:42413'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:38641'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:45367'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:38005'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:46191'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:36169'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:36673'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:40209'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:43073'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.154:42223'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:36829
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:36829
distributed.worker - INFO -          dashboard at:      198.202.103.154:43037
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-q3cxuhqu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:42733
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:42733
distributed.worker - INFO -          dashboard at:      198.202.103.154:40089
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-y_26ydn2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:34789
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:34789
distributed.worker - INFO -          dashboard at:      198.202.103.154:40345
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-t5cexfaz
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:35659
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:35659
distributed.worker - INFO -          dashboard at:      198.202.103.154:40447
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-72_jx_ft
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:33609
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:33609
distributed.worker - INFO -          dashboard at:      198.202.103.154:34871
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-w2trr0so
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:32895
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:32895
distributed.worker - INFO -          dashboard at:      198.202.103.154:45657
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-jphublly
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:45275
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:45275
distributed.worker - INFO -          dashboard at:      198.202.103.154:46851
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-8f0gog35
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:40763
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:40763
distributed.worker - INFO -          dashboard at:      198.202.103.154:42651
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-07i_rvwu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:39669
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:39669
distributed.worker - INFO -          dashboard at:      198.202.103.154:42257
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-5c85bqb9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:41083
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:41083
distributed.worker - INFO -          dashboard at:      198.202.103.154:39515
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-gnf84dsl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:37667
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:37667
distributed.worker - INFO -          dashboard at:      198.202.103.154:45101
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-u3wdydu5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:33035
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:33035
distributed.worker - INFO -          dashboard at:      198.202.103.154:46635
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-qodw38zb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:46893
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:46893
distributed.worker - INFO -          dashboard at:      198.202.103.154:46801
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-0e7snqyc
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:32799
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:32799
distributed.worker - INFO -          dashboard at:      198.202.103.154:34687
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kxx16h7q
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:44991
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:44991
distributed.worker - INFO -          dashboard at:      198.202.103.154:38109
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-1buzvs7s
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:33347
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:33347
distributed.worker - INFO -          dashboard at:      198.202.103.154:42771
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-179dgam9
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:38849'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:44979'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:37667
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:41345'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:44967'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:39669
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:39641'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:43825'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:32895
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:32799
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:45141'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:33609
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:39093'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:40763
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:38043'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:38819'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:33035
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:38607'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:33347
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:33775'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:35101'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:36829
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:34789
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:35659
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:43737'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:42733
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:37265'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:33681'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:43139'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:41229'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:46893
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:37571'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:32873'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:41835'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:46303'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:44991
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:46235'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:34185'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:37939'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:37519'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:38937'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:38379'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:44145'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:42399'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:42889'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:40149'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:36075'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:37805'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:36275'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:37645'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:43669'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:37949'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:34917'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:35521'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:33659'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:42403'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:44333'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:37537'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:37999'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:45655'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:39811'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:44795'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:35625'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:34715'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:40221'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:46677'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:42511'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:36757'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:41405'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:41175'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:32831'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:42041'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:38353'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:33337'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:34953'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:39587'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:37979'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:38659'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:44235'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:33553'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:41767'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:46469'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:46047'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:41003'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:45009'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:39963'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:32949'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:37399'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:39003'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:42735'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:42405'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:32957'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:40905'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:42417'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:37005'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:34655'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:46527'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:45385'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:44731'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:34203'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:45275
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:42401'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:39959'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:32847'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:41723'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:42413'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:38641'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:45367'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:38005'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:46191'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:36169'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:36673'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:40209'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:43073'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.154:42223'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:41083
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.154:34055
distributed.worker - INFO -          Listening to: tcp://198.202.103.154:34055
distributed.worker - INFO -          dashboard at:      198.202.103.154:36141
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:35195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-f291cice
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Stopping worker at tcp://198.202.103.154:34055
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - INFO - Worker process 923 was killed by signal 15
distributed.nanny - INFO - Worker process 976 was killed by signal 15
distributed.nanny - INFO - Worker process 969 was killed by signal 15
distributed.nanny - INFO - Worker process 983 was killed by signal 15
distributed.nanny - INFO - Worker process 978 was killed by signal 15
distributed.nanny - INFO - Worker process 964 was killed by signal 15
distributed.nanny - INFO - Worker process 958 was killed by signal 15
distributed.nanny - INFO - Worker process 974 was killed by signal 15
distributed.nanny - INFO - Worker process 949 was killed by signal 15
distributed.nanny - INFO - Worker process 967 was killed by signal 15
distributed.nanny - INFO - Worker process 911 was killed by signal 15
distributed.nanny - INFO - Worker process 937 was killed by signal 15
distributed.nanny - INFO - Worker process 943 was killed by signal 15
distributed.nanny - INFO - Worker process 985 was killed by signal 15
distributed.nanny - INFO - Worker process 955 was killed by signal 15
distributed.nanny - INFO - Worker process 988 was killed by signal 15
distributed.nanny - INFO - Worker process 991 was killed by signal 15
distributed.nanny - INFO - Worker process 994 was killed by signal 15
distributed.nanny - INFO - Worker process 997 was killed by signal 15
distributed.nanny - INFO - Worker process 1014 was killed by signal 15
distributed.nanny - INFO - Worker process 1017 was killed by signal 15
distributed.nanny - INFO - Worker process 1020 was killed by signal 15
distributed.nanny - INFO - Worker process 1000 was killed by signal 15
distributed.nanny - INFO - Worker process 1002 was killed by signal 15
distributed.nanny - INFO - Worker process 1006 was killed by signal 15
distributed.nanny - INFO - Worker process 1010 was killed by signal 15
distributed.nanny - INFO - Worker process 1024 was killed by signal 15
distributed.nanny - INFO - Worker process 1022 was killed by signal 15
distributed.nanny - INFO - Worker process 1026 was killed by signal 15
distributed.nanny - INFO - Worker process 1030 was killed by signal 15
distributed.nanny - INFO - Worker process 1036 was killed by signal 15
distributed.nanny - INFO - Worker process 1040 was killed by signal 15
distributed.nanny - INFO - Worker process 934 was killed by signal 15
distributed.nanny - INFO - Worker process 1043 was killed by signal 15
distributed.nanny - INFO - Worker process 908 was killed by signal 15
distributed.nanny - INFO - Worker process 1051 was killed by signal 15
distributed.nanny - INFO - Worker process 947 was killed by signal 15
distributed.nanny - INFO - Worker process 931 was killed by signal 15
distributed.nanny - INFO - Worker process 952 was killed by signal 15
distributed.nanny - INFO - Worker process 896 was killed by signal 15
distributed.nanny - INFO - Worker process 906 was killed by signal 15
distributed.nanny - INFO - Worker process 959 was killed by signal 15
distributed.nanny - INFO - Worker process 902 was killed by signal 15
distributed.nanny - INFO - Worker process 939 was killed by signal 15
distributed.nanny - INFO - Worker process 895 was killed by signal 15
distributed.nanny - INFO - Worker process 899 was killed by signal 15
distributed.nanny - INFO - Worker process 1048 was killed by signal 15
distributed.nanny - INFO - Worker process 927 was killed by signal 15
distributed.nanny - INFO - Worker process 917 was killed by signal 15
distributed.nanny - INFO - Worker process 1098 was killed by signal 15
distributed.nanny - INFO - Worker process 1067 was killed by signal 15
distributed.nanny - INFO - Worker process 1065 was killed by signal 15
distributed.nanny - INFO - Worker process 1105 was killed by signal 15
distributed.nanny - INFO - Worker process 1108 was killed by signal 15
distributed.nanny - INFO - Worker process 1095 was killed by signal 15
distributed.nanny - INFO - Worker process 1069 was killed by signal 15
distributed.nanny - INFO - Worker process 1101 was killed by signal 15
distributed.nanny - INFO - Worker process 1125 was killed by signal 15
distributed.nanny - INFO - Worker process 1138 was killed by signal 15
distributed.nanny - INFO - Worker process 1054 was killed by signal 15
distributed.nanny - INFO - Worker process 1081 was killed by signal 15
distributed.nanny - INFO - Worker process 1076 was killed by signal 15
distributed.nanny - INFO - Worker process 1088 was killed by signal 15
distributed.nanny - INFO - Worker process 1091 was killed by signal 15
distributed.nanny - INFO - Worker process 1112 was killed by signal 15
distributed.nanny - INFO - Worker process 1059 was killed by signal 15
distributed.nanny - INFO - Worker process 1114 was killed by signal 15
distributed.nanny - INFO - Worker process 1075 was killed by signal 15
distributed.nanny - INFO - Worker process 1061 was killed by signal 15
distributed.nanny - INFO - Worker process 1122 was killed by signal 15
distributed.nanny - INFO - Worker process 1127 was killed by signal 15
distributed.nanny - INFO - Worker process 1118 was killed by signal 15
distributed.nanny - INFO - Worker process 1116 was killed by signal 15
distributed.nanny - INFO - Worker process 1134 was killed by signal 15
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1210 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1212 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1208 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1201 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1204 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1199 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1193 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1197 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1195 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1182 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1177 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1189 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1158 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1156 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1153 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1162 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1148 parent=782 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1142 parent=782 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
