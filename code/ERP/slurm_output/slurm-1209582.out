distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35151'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:36927'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42379'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43071'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:41581'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:36017'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:33733'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40521'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:39271'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42941'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:41417'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:39871'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40183'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40803'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38089'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:36787'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40889'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:41227'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43755'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:44663'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:36557'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35785'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46777'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37297'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:41957'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43035'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42899'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40153'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:44839'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:41987'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35975'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:33543'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40759'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43811'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:32769'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35751'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:36819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42777'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37609'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42253'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42971'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35987'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38429'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42441'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:33383'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40115'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:34147'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35687'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:45179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:35781'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:44761'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46809'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42997'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46905'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43567'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:45433'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42331'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:45935'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:32893'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43425'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40165'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46469'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:45177'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43885'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:44579'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:36315'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38303'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:45457'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38715'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38765'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:45207'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:33611'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37923'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43879'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42325'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:42915'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:45015'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:33149'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37575'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:37019'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:43199'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:36747'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:40789'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38883'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:44527'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:33845'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38511'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:44611'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:44379'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:34823'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:46401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:38341'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:39143'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:34393'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.40:41011'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:37619
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:37619
distributed.worker - INFO -          dashboard at:       198.202.103.40:37571
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-haxhigya
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:37881
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:37881
distributed.worker - INFO -          dashboard at:       198.202.103.40:36649
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-rgrbwxgb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45953
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45953
distributed.worker - INFO -          dashboard at:       198.202.103.40:43203
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-pp9rkr6s
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:40777
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:40777
distributed.worker - INFO -          dashboard at:       198.202.103.40:38697
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-njvu5hlu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:37637
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:37637
distributed.worker - INFO -          dashboard at:       198.202.103.40:45779
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-0_24z3lw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:39399
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:39399
distributed.worker - INFO -          dashboard at:       198.202.103.40:46781
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kv0qv_c5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45085
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45085
distributed.worker - INFO -          dashboard at:       198.202.103.40:37309
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-sq30sko5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:34223
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:34223
distributed.worker - INFO -          dashboard at:       198.202.103.40:33881
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-0lyew3lm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:41545
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:41545
distributed.worker - INFO -          dashboard at:       198.202.103.40:37237
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-s1iuiqco
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42221
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42221
distributed.worker - INFO -          dashboard at:       198.202.103.40:46089
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-bvrnf7g6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35341
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35341
distributed.worker - INFO -          dashboard at:       198.202.103.40:40147
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-t7kl17da
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35467
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35467
distributed.worker - INFO -          dashboard at:       198.202.103.40:46341
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kovejgg1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:37775
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:37775
distributed.worker - INFO -          dashboard at:       198.202.103.40:43135
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-7vnvb0ej
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:43371
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:43371
distributed.worker - INFO -          dashboard at:       198.202.103.40:42633
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-wfh_5lpq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:33885
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:33885
distributed.worker - INFO -          dashboard at:       198.202.103.40:33559
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ndcf7bj3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42039
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42039
distributed.worker - INFO -          dashboard at:       198.202.103.40:34025
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-bzxpwani
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:32923
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:32923
distributed.worker - INFO -          dashboard at:       198.202.103.40:42071
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-11j7whu7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:36099
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:36099
distributed.worker - INFO -          dashboard at:       198.202.103.40:32789
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-uit0ox5f
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45717
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45717
distributed.worker - INFO -          dashboard at:       198.202.103.40:33121
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-fe9343x_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:39095
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:39095
distributed.worker - INFO -          dashboard at:       198.202.103.40:39739
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-1wfa4eol
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42015
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42015
distributed.worker - INFO -          dashboard at:       198.202.103.40:33757
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-1b10uqc5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35339
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35339
distributed.worker - INFO -          dashboard at:       198.202.103.40:36213
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-7v3wt2n1
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45713
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45713
distributed.worker - INFO -          dashboard at:       198.202.103.40:45139
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-fwfqxm7y
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:38709
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:38709
distributed.worker - INFO -          dashboard at:       198.202.103.40:43067
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-2x8pcvh5
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:36377
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:36377
distributed.worker - INFO -          dashboard at:       198.202.103.40:42939
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-wzadjwiw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:44215
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:44215
distributed.worker - INFO -          dashboard at:       198.202.103.40:38415
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-mxk03q21
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45483
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45483
distributed.worker - INFO -          dashboard at:       198.202.103.40:35285
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-8te4nw8u
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:44603
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:44603
distributed.worker - INFO -          dashboard at:       198.202.103.40:46523
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-3e1xh5j0
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:43003
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:43003
distributed.worker - INFO -          dashboard at:       198.202.103.40:39195
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-cbnyr7l_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:34283
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:34283
distributed.worker - INFO -          dashboard at:       198.202.103.40:44529
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lqa4g4wl
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35745
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35745
distributed.worker - INFO -          dashboard at:       198.202.103.40:37379
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ylnnx4bb
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42557
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42557
distributed.worker - INFO -          dashboard at:       198.202.103.40:33295
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-n3cex136
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42255
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42255
distributed.worker - INFO -          dashboard at:       198.202.103.40:43247
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-8bhvh2rw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:33163
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:33163
distributed.worker - INFO -          dashboard at:       198.202.103.40:34189
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-0nb0xduh
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45649
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45649
distributed.worker - INFO -          dashboard at:       198.202.103.40:37883
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lbp4fbvp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:36357
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:36357
distributed.worker - INFO -          dashboard at:       198.202.103.40:38627
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-74bivz6p
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:34929
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:34929
distributed.worker - INFO -          dashboard at:       198.202.103.40:42901
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-uri9sxz8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:33037
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:33037
distributed.worker - INFO -          dashboard at:       198.202.103.40:36335
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-7vfapign
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:39193
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:39193
distributed.worker - INFO -          dashboard at:       198.202.103.40:41883
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-h0agl0ka
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45165
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45165
distributed.worker - INFO -          dashboard at:       198.202.103.40:34137
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-yb_psxwi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:33667
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:33667
distributed.worker - INFO -          dashboard at:       198.202.103.40:41389
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vqnw7kzs
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:40795
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:40795
distributed.worker - INFO -          dashboard at:       198.202.103.40:44643
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ywtywrva
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45455
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45455
distributed.worker - INFO -          dashboard at:       198.202.103.40:46903
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-sjamxzt7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:39023
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:39023
distributed.worker - INFO -          dashboard at:       198.202.103.40:36433
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-gt_higwe
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:43063
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:43063
distributed.worker - INFO -          dashboard at:       198.202.103.40:34877
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-1a7pd7xp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:34311
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:34311
distributed.worker - INFO -          dashboard at:       198.202.103.40:33109
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-6z0i51y2
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:41593
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:41593
distributed.worker - INFO -          dashboard at:       198.202.103.40:45087
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vnmhjjha
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:39539
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:39539
distributed.worker - INFO -          dashboard at:       198.202.103.40:35403
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-5hfuj1ux
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35725
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35725
distributed.worker - INFO -          dashboard at:       198.202.103.40:40547
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-xl0scjsi
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:47101
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:47101
distributed.worker - INFO -          dashboard at:       198.202.103.40:43011
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-4cy59_c3
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42579
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42579
distributed.worker - INFO -          dashboard at:       198.202.103.40:36405
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-e2asg7zu
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:47015
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:47015
distributed.worker - INFO -          dashboard at:       198.202.103.40:46509
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lkfqkdv8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:38311
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:38311
distributed.worker - INFO -          dashboard at:       198.202.103.40:37135
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-43oljw8o
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:43421
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:43421
distributed.worker - INFO -          dashboard at:       198.202.103.40:45895
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-cib1axd6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35515
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35515
distributed.worker - INFO -          dashboard at:       198.202.103.40:41885
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-6yiiuia_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45495
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42381
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45495
distributed.worker - INFO -          dashboard at:       198.202.103.40:35257
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42381
distributed.worker - INFO -          dashboard at:       198.202.103.40:42131
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-rl87v80_
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-4qf3_amx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42195
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42195
distributed.worker - INFO -          dashboard at:       198.202.103.40:36857
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-pwkxihot
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:37093
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:37093
distributed.worker - INFO -          dashboard at:       198.202.103.40:35319
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kclo4zcp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35755
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35755
distributed.worker - INFO -          dashboard at:       198.202.103.40:33969
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ed5ww9mo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:43195
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:43195
distributed.worker - INFO -          dashboard at:       198.202.103.40:33127
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-6770qo7s
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:36885
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:36885
distributed.worker - INFO -          dashboard at:       198.202.103.40:36081
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-hi34s804
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:34063
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:34063
distributed.worker - INFO -          dashboard at:       198.202.103.40:44213
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:34485
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:34485
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-hatleewe
distributed.worker - INFO -          dashboard at:       198.202.103.40:44837
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kipnsvs6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35551
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35551
distributed.worker - INFO -          dashboard at:       198.202.103.40:41083
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-9c9tju83
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45461
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45461
distributed.worker - INFO -          dashboard at:       198.202.103.40:45899
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-6d825gwx
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:44221
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:44221
distributed.worker - INFO -          dashboard at:       198.202.103.40:33675
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-18ropdz7
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42341
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42341
distributed.worker - INFO -          dashboard at:       198.202.103.40:41703
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-0shvg2mk
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:39417
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:39417
distributed.worker - INFO -          dashboard at:       198.202.103.40:36929
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-jtuzubvq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:38317
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:38317
distributed.worker - INFO -          dashboard at:       198.202.103.40:33855
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-61i5nnfp
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:36821
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:36821
distributed.worker - INFO -          dashboard at:       198.202.103.40:46363
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-i5et0kpv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:37277
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:37277
distributed.worker - INFO -          dashboard at:       198.202.103.40:45111
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ehc_e1g6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:44451
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:44451
distributed.worker - INFO -          dashboard at:       198.202.103.40:38293
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ee6h3kxr
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:34051
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:34051
distributed.worker - INFO -          dashboard at:       198.202.103.40:46559
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-retor4ld
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:32815
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:32815
distributed.worker - INFO -          dashboard at:       198.202.103.40:41947
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35637
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35637
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-50wygwe_
distributed.worker - INFO -          dashboard at:       198.202.103.40:41119
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-egy4a_ki
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:38181
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:38181
distributed.worker - INFO -          dashboard at:       198.202.103.40:37661
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-zt_akugg
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:44907
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:44907
distributed.worker - INFO -          dashboard at:       198.202.103.40:36447
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-m41re425
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42757
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42757
distributed.worker - INFO -          dashboard at:       198.202.103.40:46645
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-x2hxlrmf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:38027
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:38027
distributed.worker - INFO -          dashboard at:       198.202.103.40:35381
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-i5h81l5b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:43467
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:43467
distributed.worker - INFO -          dashboard at:       198.202.103.40:34055
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ekp_rhp8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45685
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45685
distributed.worker - INFO -          dashboard at:       198.202.103.40:44677
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ctvrmiju
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42679
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42679
distributed.worker - INFO -          dashboard at:       198.202.103.40:39937
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-65fz06c8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:38127
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:38127
distributed.worker - INFO -          dashboard at:       198.202.103.40:33449
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-dms_3hvq
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:45885
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:45885
distributed.worker - INFO -          dashboard at:       198.202.103.40:45999
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-q4ex7va6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:40583
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:40583
distributed.worker - INFO -          dashboard at:       198.202.103.40:41301
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vdkbd12b
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:37215
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:37215
distributed.worker - INFO -          dashboard at:       198.202.103.40:35963
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-v_m4adh6
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42799
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42799
distributed.worker - INFO -          dashboard at:       198.202.103.40:46673
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-a5xl5_wo
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:38579
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:38579
distributed.worker - INFO -          dashboard at:       198.202.103.40:44013
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-jllbaiaf
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:37015
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:37015
distributed.worker - INFO -          dashboard at:       198.202.103.40:44469
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-tyyue2wv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:38995
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:38995
distributed.worker - INFO -          dashboard at:       198.202.103.40:37259
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-jjuzi5fe
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:44819
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:44819
distributed.worker - INFO -          dashboard at:       198.202.103.40:39447
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:37573
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:37573
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:       198.202.103.40:37929
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-nm37e6kh
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-6m40lkjm
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:42793
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:42793
distributed.worker - INFO -          dashboard at:       198.202.103.40:42605
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-q1l2tr53
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:35885
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:35885
distributed.worker - INFO -          dashboard at:       198.202.103.40:42133
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-cj3wsj4j
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:40059
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:40059
distributed.worker - INFO -          dashboard at:       198.202.103.40:41787
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-vu_2d37q
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:36753
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:36753
distributed.worker - INFO -          dashboard at:       198.202.103.40:36181
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:38663
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-gttvbir1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:38663
distributed.worker - INFO -          dashboard at:       198.202.103.40:38949
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:44601
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-xmdlo2gy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:44601
distributed.worker - INFO -          dashboard at:       198.202.103.40:34609
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-jc1huhkw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.40:41303
distributed.worker - INFO -          Listening to: tcp://198.202.103.40:41303
distributed.worker - INFO -          dashboard at:       198.202.103.40:43787
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-zjayfwqk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35151'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:36927'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42379'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45085
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43071'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35341
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:37775
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:41581'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:36017'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45953
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:33733'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:39399
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40521'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:40777
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:39271'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:34223
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42941'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:37637
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:41417'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42221
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:39871'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35467
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40183'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:41545
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40803'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:37881
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38089'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:37619
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:36787'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:43371
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40889'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:33885
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37179'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:32923
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:41227'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42039
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43755'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:36099
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:44663'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42015
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:36557'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45717
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37917'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:39095
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35785'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45713
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46777'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:38709
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37297'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:43003
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:41957'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35745
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43035'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:44603
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42899'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:34283
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40153'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45649
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:44839'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42255
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:41987'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42557
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35975'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:34929
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:33543'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:33163
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40759'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:36357
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43811'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:33037
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:32769'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45165
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:33667
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35751'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:36819'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45455
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42777'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:40795
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37609'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35725
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42253'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:39193
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42971'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:43063
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35987'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:39023
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38429'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:34311
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:47015
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42441'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:41593
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:33383'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40115'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42579
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:34147'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:47101
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35687'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35339
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:45179'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:39539
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:35781'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35515
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:44761'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:38311
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46809'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:37093
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42997'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42381
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46905'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:43421
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42401'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:34485
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43567'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35551
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:45433'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42195
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42331'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:43195
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45495
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:45935'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:32893'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:36885
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43425'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45461
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35755
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40165'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46469'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:36821
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:45177'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:44451
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43885'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:34063
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:44221
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:44579'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:36315'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42341
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38303'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:37277
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:45457'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:38317
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37863'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:36377
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38715'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:34051
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38765'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:32815
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:45207'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:39417
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:33611'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:38181
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37923'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35637
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43879'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45885
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42325'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42679
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:42915'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:37573
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:45015'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45685
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46237'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:37215
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:33149'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:38127
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37575'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42799
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:37019'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:38579
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:43199'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:40059
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:36747'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42757
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:40789'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:43467
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38883'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:38663
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:44527'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:44819
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:33845'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:38995
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38511'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:45483
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:44611'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:35885
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:44379'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:44215
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:34823'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:44907
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:46401'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:41303
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:38341'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:40583
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:39143'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:36753
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:34393'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:42793
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.40:41011'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:38027
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:37015
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.40:44601
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - INFO - Worker process 109772 was killed by signal 15
distributed.nanny - INFO - Worker process 109854 was killed by signal 15
distributed.nanny - INFO - Worker process 109816 was killed by signal 15
distributed.nanny - INFO - Worker process 109728 was killed by signal 15
distributed.nanny - INFO - Worker process 109736 was killed by signal 15
distributed.nanny - INFO - Worker process 109755 was killed by signal 15
distributed.nanny - INFO - Worker process 109764 was killed by signal 15
distributed.nanny - INFO - Worker process 109745 was killed by signal 15
distributed.nanny - INFO - Worker process 109828 was killed by signal 15
distributed.nanny - INFO - Worker process 109739 was killed by signal 15
distributed.nanny - INFO - Worker process 109776 was killed by signal 15
distributed.nanny - INFO - Worker process 109791 was killed by signal 15
distributed.nanny - INFO - Worker process 109733 was killed by signal 15
distributed.nanny - INFO - Worker process 109730 was killed by signal 15
distributed.nanny - INFO - Worker process 109784 was killed by signal 15
distributed.nanny - INFO - Worker process 109752 was killed by signal 15
distributed.nanny - INFO - Worker process 109782 was killed by signal 15
distributed.nanny - INFO - Worker process 109843 was killed by signal 15
distributed.nanny - INFO - Worker process 109847 was killed by signal 15
distributed.nanny - INFO - Worker process 109796 was killed by signal 15
distributed.nanny - INFO - Worker process 109761 was killed by signal 15
distributed.nanny - INFO - Worker process 109758 was killed by signal 15
distributed.nanny - INFO - Worker process 109749 was killed by signal 15
distributed.nanny - INFO - Worker process 109857 was killed by signal 15
distributed.nanny - INFO - Worker process 109805 was killed by signal 15
distributed.nanny - INFO - Worker process 109767 was killed by signal 15
distributed.nanny - INFO - Worker process 109809 was killed by signal 15
distributed.nanny - INFO - Worker process 109834 was killed by signal 15
distributed.nanny - INFO - Worker process 109839 was killed by signal 15
distributed.nanny - INFO - Worker process 109813 was killed by signal 15
distributed.nanny - INFO - Worker process 109770 was killed by signal 15
distributed.nanny - INFO - Worker process 109780 was killed by signal 15
distributed.nanny - INFO - Worker process 109731 was killed by signal 15
distributed.nanny - INFO - Worker process 109819 was killed by signal 15
distributed.nanny - INFO - Worker process 109851 was killed by signal 15
distributed.nanny - INFO - Worker process 109862 was killed by signal 15
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - INFO - Worker process 109874 was killed by signal 15
distributed.nanny - INFO - Worker process 109832 was killed by signal 15
distributed.nanny - INFO - Worker process 109865 was killed by signal 15
distributed.nanny - INFO - Worker process 109919 was killed by signal 15
distributed.nanny - INFO - Worker process 109868 was killed by signal 15
distributed.nanny - INFO - Worker process 109876 was killed by signal 15
distributed.nanny - INFO - Worker process 109881 was killed by signal 15
distributed.nanny - INFO - Worker process 109884 was killed by signal 15
distributed.nanny - INFO - Worker process 109947 was killed by signal 15
distributed.nanny - INFO - Worker process 109824 was killed by signal 15
distributed.nanny - INFO - Worker process 109900 was killed by signal 15
distributed.nanny - INFO - Worker process 109867 was killed by signal 15
distributed.nanny - INFO - Worker process 109907 was killed by signal 15
distributed.nanny - INFO - Worker process 109904 was killed by signal 15
distributed.nanny - INFO - Worker process 109895 was killed by signal 15
distributed.nanny - INFO - Worker process 109940 was killed by signal 15
distributed.nanny - INFO - Worker process 109937 was killed by signal 15
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110880 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110882 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110884 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110877 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110826 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110810 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110808 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110806 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110804 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110802 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110800 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110787 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110784 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110780 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110713 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110647 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110644 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110641 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110636 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110633 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110630 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110572 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110495 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110306 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110295 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110229 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110226 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110220 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110194 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110196 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110221 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110044 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110020 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=110017 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109949 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109944 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109933 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109917 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109913 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109916 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109921 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109891 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109886 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109788 parent=109653 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=109743 parent=109653 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/process.py", line 235, in _watch_process
    assert exitcode is not None
AssertionError
