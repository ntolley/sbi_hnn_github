distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:35527'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:44251'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:45947'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36149'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:35085'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:33649'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:42465'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:43183'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36807'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:39079'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:40139'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:37203'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:44149'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:35667'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36735'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:35077'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:32977'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:34483'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:46205'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:39291'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:45633'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:42041'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:40041'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:33435'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:43321'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:37767'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:33869'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:39211'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:44827'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36059'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:39173'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:32875'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:38923'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36177'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:37477'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:41765'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:33923'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:37779'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:33655'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:33133'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:45553'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36975'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:34651'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:44087'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:45179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:45569'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:44825'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:42567'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:45705'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:44675'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:34581'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:37059'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:39503'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:41831'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:41093'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:45629'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:45289'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:39233'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:33693'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:39521'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:46693'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:40049'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:41001'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:34291'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:42551'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:34521'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:42423'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:39415'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:45613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:39533'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36079'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:34169'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:46365'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:41325'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:33455'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:43651'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:35355'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36655'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:45367'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:45011'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36793'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36531'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:39337'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:33517'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36845'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:44311'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:39359'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36489'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:46843'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:41785'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:35729'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:45833'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:43449'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:39877'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:44021'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:33835'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:36795'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:39167'
distributed.nanny - INFO -         Start Nanny at: 'tcp://198.202.103.148:41227'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:41397
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:41397
distributed.worker - INFO -          dashboard at:      198.202.103.148:43955
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-w0x0v5ch
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:44089
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:44089
distributed.worker - INFO -          dashboard at:      198.202.103.148:41701
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-rna2fqjq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:39207
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:39207
distributed.worker - INFO -          dashboard at:      198.202.103.148:40545
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-mp1gt7bv
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:44823
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:44823
distributed.worker - INFO -          dashboard at:      198.202.103.148:42105
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-lawxv6y4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:38877
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:38877
distributed.worker - INFO -          dashboard at:      198.202.103.148:44983
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-g3db_3u8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:45023
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:45023
distributed.worker - INFO -          dashboard at:      198.202.103.148:35079
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-8b73vr9i
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:39783
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:39783
distributed.worker - INFO -          dashboard at:      198.202.103.148:43725
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-1z30hh5x
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:33041
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:33041
distributed.worker - INFO -          dashboard at:      198.202.103.148:39143
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-u597_eb8
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:32929
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:32929
distributed.worker - INFO -          dashboard at:      198.202.103.148:36169
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-v_4gldlw
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:33407
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:33407
distributed.worker - INFO -          dashboard at:      198.202.103.148:42513
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ao5rzc4h
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:38371
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:38371
distributed.worker - INFO -          dashboard at:      198.202.103.148:40175
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-xjktt1ms
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:37087
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:37087
distributed.worker - INFO -          dashboard at:      198.202.103.148:43561
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-226pm_px
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:38963
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:38963
distributed.worker - INFO -          dashboard at:      198.202.103.148:41235
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-wr00vly_
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:46309
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:46309
distributed.worker - INFO -          dashboard at:      198.202.103.148:44971
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-tl2bgl43
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:38005
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:38005
distributed.worker - INFO -          dashboard at:      198.202.103.148:34905
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-i2korul4
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:39625
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:39625
distributed.worker - INFO -          dashboard at:      198.202.103.148:43447
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-xrj6jzh9
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:40433
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:40433
distributed.worker - INFO -          dashboard at:      198.202.103.148:39271
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-4notwhdj
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:34397
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:34397
distributed.worker - INFO -          dashboard at:      198.202.103.148:46525
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-8uzqehba
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:40603
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:40603
distributed.worker - INFO -          dashboard at:      198.202.103.148:42301
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-l1oibsis
distributed.worker - INFO - -------------------------------------------------
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:37795
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:37795
distributed.worker - INFO -          dashboard at:      198.202.103.148:41563
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ivpy9yv5
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:35527'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:44251'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:45947'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:39207
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36149'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:41397
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:35085'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:33041
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:33649'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:44089
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:42465'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:38877
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:43183'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:45023
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36807'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:44823
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:39079'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:33407
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:40139'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:39783
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:37203'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:38963
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:34397
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:44149'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:35667'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:38005
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36735'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:37087
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:46309
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:35077'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:39625
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:32977'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:37795
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:34483'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:38371
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:40433
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:46205'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:39291'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:45633'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:42041'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:40603
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:40041'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:33435'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:43321'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:37767'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:33869'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:39211'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:44827'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36059'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:39173'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:32875'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:38923'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36177'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:37477'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:41765'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:33923'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:37779'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:33655'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:33133'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:45553'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36975'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:34651'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:44087'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:45179'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:45569'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:44825'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:42567'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:45705'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:44675'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:34581'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:37059'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:39503'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:41831'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:41093'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:45629'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:45289'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:39233'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:33693'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:39521'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:46693'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:40049'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36237'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:41001'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:34291'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:42551'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:34521'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:42423'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:39415'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:45613'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:39533'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36079'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:34169'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:46365'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:41325'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:33455'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:43651'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:35355'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36655'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:45367'
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:32929
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:45011'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36793'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36531'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:39337'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:33517'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36845'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:44311'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:39359'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36489'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:46843'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:41785'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:35729'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:45833'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:43449'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:39877'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:44021'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:33835'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:36795'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:39167'
distributed.nanny - INFO - Closing Nanny at 'tcp://198.202.103.148:41227'
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:34631
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:34631
distributed.worker - INFO -          dashboard at:      198.202.103.148:35677
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-l0e_5d18
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:34631
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:43251
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:43251
distributed.worker - INFO -          dashboard at:      198.202.103.148:34947
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-8f7glmom
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:43251
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:35753
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:35753
distributed.worker - INFO -          dashboard at:      198.202.103.148:44187
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-ongyhiff
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:35753
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:41347
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:41347
distributed.worker - INFO -          dashboard at:      198.202.103.148:33251
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-f2p8az8l
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:41347
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:34247
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:34247
distributed.worker - INFO -          dashboard at:      198.202.103.148:33207
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-riw5bjui
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:34247
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 271, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 498, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/asyncio/tasks.py", line 692, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/site-packages/distributed/core.py", line 275, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73089 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73085 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73087 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73082 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73081 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73012 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73014 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73010 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73003 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73006 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73001 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73008 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72932 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72930 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72922 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72917 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72914 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72907 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72927 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72912 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72902 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72892 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72894 parent=72537 started daemon>
/home/ntolley/anaconda3/envs/sbi/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at: tcp://198.202.103.148:45669
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72898 parent=72537 started daemon>
distributed.worker - INFO -          Listening to: tcp://198.202.103.148:45669
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72887 parent=72537 started daemon>
distributed.worker - INFO -          dashboard at:      198.202.103.148:40547
distributed.worker - INFO - Waiting to connect to: tcp://198.202.103.227:33409
distributed.worker - INFO - -------------------------------------------------
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72882 parent=72537 started daemon>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                 1000.00 MB
distributed.worker - INFO -       Local Directory: /home/ntolley/Jones_Lab/sbi_hnn_github/code/ERP/dask-worker-space/dask-worker-space/worker-kk5gtwnf
distributed.worker - INFO - -------------------------------------------------
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72879 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72867 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72865 parent=72537 started daemon>
distributed.worker - INFO - Stopping worker at tcp://198.202.103.148:45669
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72870 parent=72537 started daemon>
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72859 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72847 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72849 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72853 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72858 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72844 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72837 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72833 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72840 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72829 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72824 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72808 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72817 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72805 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72807 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72812 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72802 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72800 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72789 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72781 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72784 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72773 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72770 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72766 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72764 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72758 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72755 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72752 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72749 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72745 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72740 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72737 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72733 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72730 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72727 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72720 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72716 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72714 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72711 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72710 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72704 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72699 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72696 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72694 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72691 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72686 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72684 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72679 parent=72537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72672 parent=72537 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/home/ntolley/anaconda3/envs/sbi/lib/python3.8/threading.py", line 932, in _bootstrap_inner
