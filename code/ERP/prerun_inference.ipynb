{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dill'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8eb280ce4f2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DISPLAY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dill'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if 'DISPLAY' in os.environ:\n",
    "    del os.environ['DISPLAY']\n",
    "import torch\n",
    "import dill\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import hnn_core\n",
    "from hnn_core import simulate_dipole, Network, read_params, JoblibBackend\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sbi.utils as utils\n",
    "from sbi.inference.base import infer\n",
    "import multiprocessing\n",
    "import datetime\n",
    "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi\n",
    "from scipy import interpolate\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import spike_train_functions\n",
    "import hnn_simnets_functions\n",
    "import numba\n",
    "import sbi_functions\n",
    "from sbi_functions import run_simulator\n",
    "from joblib import Parallel, delayed\n",
    "import umap\n",
    "import umap.plot\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = 'ERP_Yes_t1000000_02262021_124114'\n",
    "data_path = '/home/ntolley/Jones_Lab/sbi_hnn/data/ERP/prerun_simulations/' + save_name + '/'\n",
    "\n",
    "prior_dict_file = open(data_path + 'prior_dict_' + save_name + '.pkl', 'rb')\n",
    "prior_dict = dill.load(prior_dict_file)\n",
    "prior_dict_file.close()\n",
    "\n",
    "prior_file = open(data_path + 'prior_' + save_name + '.pkl', 'rb')\n",
    "prior = dill.load(prior_file)   \n",
    "prior_file.close()\n",
    "\n",
    "params_fname_file = open(data_path + 'params_fname_' + save_name + '.pkl', 'rb')\n",
    "params_fname = dill.load(params_fname_file)\n",
    "params_fname_file.close()\n",
    "\n",
    "dpl_files = sorted(glob.glob(data_path + 'data/dpl*'))\n",
    "theta_files = sorted(glob.glob(data_path + 'data/theta*'))\n",
    "spike_gids_files = sorted(glob.glob(data_path + 'data/spike_gids*'))\n",
    "spike_times_files = sorted(glob.glob(data_path + 'data/spike_times*'))\n",
    "spike_types_files = sorted(glob.glob(data_path + 'data/spike_types*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpl_preprocessed_file = open(data_path + 'dpl_ERP_preprocessed_100x_downsample.pkl', 'rb')\n",
    "dpl_preprocessed = dill.load(dpl_preprocessed_file)\n",
    "dpl_preprocessed_file.close()\n",
    "dpl, theta, sim_times = dpl_preprocessed['dpl'], dpl_preprocessed['theta'], dpl_preprocessed['sim_times']\n",
    "downsample = dpl_preprocessed['downsample']\n",
    "\n",
    "ERPYes_dpl = np.loadtxt('../../data/ERP/ERPYes_dpl.txt', delimiter=',')[::downsample]\n",
    "ERPNo_dpl = np.loadtxt('../../data/ERP/ERPNo_dpl.txt', delimiter=',')[::downsample]\n",
    "\n",
    "\n",
    "# with open(spike_gids_files[file_idx], 'rb') as f:\n",
    "#     spike_gids = dill.load(f)\n",
    "# unique_gids = np.unique([gid for l in spike_gids for gid in l[0]])\n",
    "# num_sims = len(spike_gids)\n",
    "\n",
    "# with open(spike_times_files[file_idx], 'rb') as f:\n",
    "#     spike_times = dill.load(f)\n",
    "\n",
    "# spike_times_list = []\n",
    "# for gid in unique_gids:\n",
    "#     unit_trains = []\n",
    "#     for sim_idx in range(num_sims):\n",
    "#         gid_mask = np.in1d(spike_gids[sim_idx][0], gid)\n",
    "#         unit_trains.append(np.array(spike_times[sim_idx][0])[gid_mask])\n",
    "#     spike_times_list.append(unit_trains)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_net = sbi_functions.model_ann(input_size=dpl.shape[1], output_size=10, layer_size=[200,50])\n",
    "\n",
    "neural_posterior = utils.posterior_nn(model='maf', embedding_net=embedding_net)\n",
    "inference = SNPE(prior=prior)\n",
    "inference = inference.append_simulations(torch.as_tensor(theta).float(), torch.as_tensor(dpl).float())\n",
    "density_estimator = inference.train(show_train_summary=True)\n",
    "posterior = inference.build_posterior(density_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ___Save posterior___\n",
    "# posterior_files = {\n",
    "#     'posterior': posterior,\n",
    "#     'sim_times': sim_times,\n",
    "#     'dpl_preprocessed': dpl_preprocessed\n",
    "#     }\n",
    "\n",
    "# save_file = open(data_path + 'posterior_files_ANN_embedding.pkl', 'wb')\n",
    "# dill.dump(posterior_files, save_file)\n",
    "# save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___Load posterior___\n",
    "posterior_file = open(data_path + 'posterior_files_ANN_embedding.pkl', 'rb')\n",
    "posterior = dill.load(posterior_file)\n",
    "posterior_file.close()\n",
    "locals().update(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 8\n",
    "theta_samples_No = posterior.sample((num_samples,), x=ERPNo_dpl[::100])\n",
    "theta_samples_Yes = posterior.sample((num_samples,), x=ERPYes_dpl[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_No = Parallel(n_jobs=8)(delayed(run_simulator)(theta_samples_No[sim_idx,:], params_fname, prior_dict, sim_idx) for sim_idx in range(num_samples))\n",
    "res_Yes = Parallel(n_jobs=8)(delayed(run_simulator)(theta_samples_Yes[sim_idx,:], params_fname, prior_dict, sim_idx) for sim_idx in range(num_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_times = [res_No[sim_idx][1][0] for sim_idx in range(len(res_No))]\n",
    "spike_gids = [res_No[sim_idx][2][0] for sim_idx in range(len(res_No))]\n",
    "spike_types = [res_No[sim_idx][3][0] for sim_idx in range(len(res_No))]\n",
    "spikes = hnn_simnets_functions.tmpSpikes(spike_times, spike_gids, spike_types)\n",
    "spikes_df_No = hnn_simnets_functions.make_spikes_df(spikes, num_trials=len(res_No))\n",
    "\n",
    "spike_times = [res_Yes[sim_idx][1][0] for sim_idx in range(len(res_Yes))]\n",
    "spike_gids = [res_Yes[sim_idx][2][0] for sim_idx in range(len(res_Yes))]\n",
    "spike_types = [res_Yes[sim_idx][3][0] for sim_idx in range(len(res_Yes))]\n",
    "spikes = hnn_simnets_functions.tmpSpikes(spike_times, spike_gids, spike_types)\n",
    "spikes_df_Yes = hnn_simnets_functions.make_spikes_df(spikes, num_trials=len(res_Yes))\n",
    "\n",
    "spikes_df_Yes['detected'] = np.repeat('Yes', len(spikes_df_Yes))\n",
    "spikes_df_No['detected'] = np.repeat('No', len(spikes_df_No))\n",
    "spikes_df_No['gid'] = spikes_df_No['gid'] + np.max(spikes_df_Yes['gid'].values)\n",
    "\n",
    "spikes_df_all = pd.concat([spikes_df_Yes, spikes_df_No])\n",
    "\n",
    "#Add colormap for type\n",
    "type_color = {cell_type:type_idx for type_idx, cell_type in enumerate(np.unique(spikes_df_all['type'].values))}\n",
    "spikes_df_all['color'] = [type_color[cell_type] for cell_type in spikes_df_all['type'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spikes_df = spikes_df_all.groupby('gid').agg(lambda x: tuple(x)).applymap(list).reset_index()\n",
    "# spikes_df['type'] = spikes_df['type'].map(lambda x: x[0])\n",
    "# spikes_df['detected'] = spikes_df['detected'].map(lambda x: x[0])\n",
    "\n",
    "# #Format timestamps columns into list of size num_trials, empty lists inserted on trials where no spikes fired\n",
    "# grouped_ts = []\n",
    "# trial_sim = []\n",
    "# for gid in np.unique(spikes_df['gid'].values):\n",
    "#     gid_ts = []\n",
    "#     gid_trial_idx = 0 \n",
    "#     for trial_idx in range(num_samples):\n",
    "#         if trial_idx in spikes_df['trial'][spikes_df['gid'] == gid].values[0]:\n",
    "#             gid_ts.append(np.array(spikes_df['timestamps'][spikes_df['gid'] == gid].values[0][gid_trial_idx]))\n",
    "#             gid_trial_idx = gid_trial_idx + 1\n",
    "#         else:\n",
    "#             gid_ts.append(np.array([]))\n",
    "#     grouped_ts.append(np.array(gid_ts))\n",
    "#     trial_sim.append(spike_train_functions.vpTrialSimilarityMatrix(gid_ts,0.1).reshape((1,-1)).squeeze())\n",
    "# spikes_df['grouped_ts'] = grouped_ts\n",
    "# spikes_df['trial_sim'] = trial_sim\n",
    "n_neighbors=20\n",
    "min_dist=0.01\n",
    "n_components=2\n",
    "for q in np.linspace(0,1,9):\n",
    "    spikes_df = spike_train_functions.grouped_ts_spikes_df(spikes_df_all, num_samples,q=q)\n",
    "    mapper = umap.UMAP(metric='correlation',n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components).fit(np.stack(spikes_df['trial_sim'].values))\n",
    "    umap.plot.points(mapper,labels = spikes_df['type'].values, theme='fire')\n",
    "    # umap.plot.points(mapper,labels = spikes_df['detected'].values, theme='fire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors=20\n",
    "min_dist=0.01\n",
    "n_components=2\n",
    "mapper = umap.UMAP(metric='correlation',n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components).fit(np.stack(spikes_df['trial_sim'].values))\n",
    "umap.plot.points(mapper,labels = spikes_df['type'].values, theme='fire')\n",
    "umap.plot.points(mapper,labels = spikes_df['detected'].values, theme='fire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.connectivity(mapper, edge_bundling='hammer')\n",
    "umap.plot.diagnostic(mapper, diagnostic_type='local_dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(sim_times, ERPYes_dpl[::100], 'C0', LineWidth=4)\n",
    "plt.plot(sim_times, ERPNo_dpl[::100], 'C1',LineWidth=4)\n",
    "plt.legend(['Detected', 'Non-Detected'])\n",
    "x_times = np.linspace(0,170,len(res_Yes[0][0]))\n",
    "# for sim_idx in range(len(res_Yes)):\n",
    "#     plt.plot(x_times, res_Yes[sim_idx][0], 'C0', LineStyle='--', alpha=0.5)\n",
    "# for sim_idx in range(len(res_No)):\n",
    "#     plt.plot(x_times, res_No[sim_idx][0], 'C1', LineStyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "# plt.plot(x_times, res_Yes[0][0], 'C0', LineStyle='--', alpha=0.5)\n",
    "# plt.plot(x_times, res_No[0][0], 'C1', LineStyle='--', alpha=0.5)\n",
    "# plt.legend(['Detected', 'Non-Detected'])\n",
    "\n",
    "plt.plot(x_times, np.vstack(res_Yes[sim_idx][0] for sim_idx in range(len(res_Yes))).transpose(), 'C0', LineStyle='--', alpha=0.5)\n",
    "plt.plot(x_times, np.vstack(res_No[sim_idx][0] for sim_idx in range(len(res_No))).transpose(), 'C1', LineStyle='--', alpha=0.5)\n",
    "\n",
    "plt.title('Inferred ERPs (100000 training sims)')\n",
    "plt.ylabel('Current Dipole')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.xlim([0,175])\n",
    "# plt.ylim([-55,205])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sbi)",
   "language": "python",
   "name": "sbi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
